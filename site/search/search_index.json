{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"guides/","title":"Guides","text":"<p>This folder contains various guides to help those working in software development, both generally and in support of our standards.</p>"},{"location":"guides/#contents","title":"Contents","text":"<ul> <li>Choosing between a mono repo and a multi repo</li> <li>Choosing packages</li> <li>Continuous integration</li> <li>Developer workflows</li> <li>Docker guidance</li> <li>Java auto-format with Eclipse</li> <li>Kubernetes</li> <li>Mobile application guidance</li> <li>New starters</li> <li>PL/SQL auto-format with TOAD</li> <li>SQL Prompt</li> <li>Style guide for standards</li> <li>Using AWS session manager</li> <li>Using unmanaged devices</li> <li>Version control guidance</li> </ul>"},{"location":"guides/#general-guidance","title":"General guidance","text":"<p>For most developers the primary activity they will carry out is writing code.</p> <p>This is some general guidance to consider when going about it.</p>"},{"location":"guides/#logically-architect-the-layout-of-the-code","title":"Logically architect the layout of the code","text":"<p>Writing clean and maintainable code is much easier when it's clear which components belong where. The architecture should take into account potential expansion. The SOLID principles provide an example of this approach.</p>"},{"location":"guides/#coding-standards","title":"Coding standards","text":"<p>Follow secure coding standards where available to avoid potential security vulnerabilities.</p>"},{"location":"guides/#use-self-explanatory-naming-conventions","title":"Use self-explanatory naming conventions","text":"<p>To help the reader understand what is going on, classes, methods, functions, file names and folder names should all be self-explanatory</p>"},{"location":"guides/#provide-good-supportive-tooling-to-developers","title":"Provide good supportive tooling to developers","text":"<p>This may include modern IDEs with plugins that support the developer with capabilities such as pre-defined code snippets, debugging, unit testing, security hints, 'linting', specification checking and annotations.</p>"},{"location":"guides/#maintain-a-consistent-coding-style","title":"Maintain a consistent coding style","text":"<p>Understanding code becomes more difficult when different coding styles used by different developers mix and intertwine.</p>"},{"location":"guides/#clearly-outline-code-block-responsibilities","title":"Clearly outline code block responsibilities","text":"<p>Security issues can arise when one code component incorrectly assumes another has taken responsibility for an action. For example, when validating potentially malicious input at the border of your application. One way to achieve this is to have a comment block at the top of every method or function.</p>"},{"location":"guides/#separate-secret-credential","title":"Separate secret credential","text":"<p>Keep secrets such as passwords and private keys logically isolated from the core code base. This will help prevent them being checked in to public code repositories. Hard coding credentials in source code is bad practice.</p>"},{"location":"guides/#do-small-and-regular-code-commits","title":"Do small and regular code commits","text":"<p>Performing effective review becomes more difficult when large changes are made with each commit. Small and clearly labelled commits simplify the review and roll back process.</p>"},{"location":"guides/#attribute-code-changes-to-an-author","title":"Attribute code changes to an author","text":"<p>It should be clear who has authored a code change, and strong authentication controls should be used to do so. It should not be possible to falsify the code's author or its review status.</p>"},{"location":"guides/#police-and-critique-each-others-work-through-peer-review","title":"Police and critique each other's work through peer review","text":"<p>Encourage a culture that does not accept complicated, confusing or insecure coding practices. Peer review helps prevent such issues being incorporated into your code base. Feedback helps support education within your team. Using pull requests and comments is one way to achieve this.</p>"},{"location":"guides/#team-communications","title":"Team communications","text":"<p>When multiple team members are working on the same code base, there should be strong and regular communication channels between them. The aim here is to avoid the following scenario: 'I thought you were securing that component!'. Keeping teams physically close to one-another, or providing real-time chat channels are two ways to achieve this.</p>"},{"location":"guides/#document-and-comment-clearly-and-consistently","title":"Document and comment clearly and consistently","text":"<p>Clear and concise documentation should support your product. This may be as a result of self-documenting code, code comments, or supportive material. Documentation should be kept up to date, as a system evolves. Old and out-of-date documentation is difficult to trust and could be damaging for security if it's interpreted incorrectly.</p>"},{"location":"guides/#support-new-team-members","title":"Support new team members","text":"<p>Developers and other team members may come and go over the life span of a product. To ensure adequate knowledge of the product is maintained, provide good support and documentation to new team members. After all, who will fix security vulnerabilities left behind by previous developers?</p>"},{"location":"guides/#check-return-values-and-handle-errors-appropriately","title":"Check return values and handle errors appropriately","text":"<p>Checking for errors at the point of a call and handling them immediately means that your code doesn't continue running in a potentially unstable state. This will make your code more robust and secure.</p>"},{"location":"guides/aws_session_manager/","title":"Session Manager","text":"<p>The purpose of this guide is demonstrate how to use AWS Session Manager to gain access EC2 instances, instead of using SSH.</p>"},{"location":"guides/aws_session_manager/#prerequisites","title":"Prerequisites","text":"<ul> <li>You will need an AWS account and your user must be part of the relevant developer group/s.</li> <li>You must either have access to the AWS console, or have installed the AWS CLI and Session Manager plugin. You will then need to configure the AWS CLI. </li> </ul> <p>If you need any assistance, don't hesitate to contact the CCoE AWS WebOps team.</p>"},{"location":"guides/aws_session_manager/#user-permissions","title":"User permissions","text":"<p>When initiating a session, you will be logging into the server as the <code>developer</code> user. This user has limited permissions and is intended for basic investigative work only. You won't be able to: - Run <code>sudo</code>, <code>pm2</code>, <code>docker</code> among other administrative commands. - Switch to other users. - Copy files onto or off the server.</p> <p>If there is anything that you can't do, but that you need to do, please contact the CCoE AWS WebOps team to discuss further. </p>"},{"location":"guides/aws_session_manager/#using-the-aws-cli","title":"Using the AWS CLI","text":""},{"location":"guides/aws_session_manager/#starting-a-session","title":"Starting a session","text":"<p>Find which instances are available for you to connect to:</p> <pre><code>aws ec2 describe-instances --query \"Reservations[*].Instances[*].{Instance:InstanceId,Name:Tags[?Key=='Name']|[0].Value,Status:State.Name}\" --output table\n</code></pre> <p>Copy the instance ID and then start a session into the instance:</p> <pre><code>aws ssm start-session --target &lt;instance-id&gt;\n</code></pre> <p>You can find other examples of starting sessions here, which includes port forwarding.</p> <p>If you are having difficulty connecting, please see the troubleshooting guide here. If you're still having issues, please contact ddts-aws-webops@defra.gov.uk for support.</p>"},{"location":"guides/aws_session_manager/#ending-a-session","title":"Ending a session","text":"<p>Typically you will be connecting to an Ubuntu server, you can just execute the <code>exit</code> command to come out of the session.</p> <p>However, if your session is stuck, you first need to find your session ID:</p> <pre><code>aws ssm describe-sessions --state Active\n</code></pre> <p>Copy the session ID and then terminate your session with this:</p> <pre><code>aws ssm terminate-session --session-id &lt;session-id&gt;\n</code></pre>"},{"location":"guides/aws_session_manager/#using-the-aws-console","title":"Using the AWS Console","text":"<ol> <li>Login to the AWS Console</li> <li>Navigate to EC2 dashboard -&gt; instances</li> <li>Select a single instance that you want to connect to</li> <li>Select Connect and then Session Manager<ul> <li>If you are presented with <code>We weren't able to connect to your instance</code>, please contact the CCoE AWS WebOps team.</li> </ul> </li> </ol>"},{"location":"guides/choosing_packages/","title":"Choosing packages and dependencies","text":"<p>We use a lot of packages and dependencies in our services. Using an established package can be a great way to fix a problem or add a feature. It can be quicker and easier than writing the code from scratch. And it can make your application more consistent with other Defra services and industry standards.</p> <p>When you are picking a package or a dependency to include in your project, there are some things to consider:</p>"},{"location":"guides/choosing_packages/#what-have-other-defra-services-used","title":"What have other Defra services used?","text":"<p>Is there an agreed-upon standard package for this purpose? If there is, use it.</p> <p>For example, Hapi is our Node.js framework of choice. So if you're starting a new Node project and need a web framework, use Hapi over alternatives like Express.</p> <p>If there's no official standard, you should still consider what similar services use. Codebases can be more consistent and easier to maintain if they use the same packages.</p> <p>It can also be quicker to introduce an already-used package to your project. You can reuse code from our other services, or learn from developers who have worked with it.</p>"},{"location":"guides/choosing_packages/#what-are-the-most-popular-solutions","title":"What are the most popular solutions?","text":"<p>If you're looking for a package for a common function (like logging or user authentication), there may be many potential candidates.</p> <p>When in doubt, go for the most used package, especially if it's become the industry standard.</p> <p>You can compare how popular packages are by looking at the number of downloads they have on package manager sites (like NPM or RubyGems). You can also check how many times the repository has been watched or starred on GitHub.</p> <p>A well-established package will usually have a lot more guidance and support. There will be more blog posts, more questions on Stack Overflow, and other projects you can refer to.</p> <p>It also makes it easier to introduce new developers to your service, as it's more likely they'll be familiar with an industry standard.</p> <p>This isn't a hard rule. If the most popular package doesn't meet your needs, or has clear issues with security or maintainability, then it's OK to go for an alternative. But if you're choosing between one package with a thousand downloads and another with a million downloads, you should go for the second one.</p>"},{"location":"guides/choosing_packages/#is-the-package-maintained","title":"Is the package maintained?","text":"<p>Check the package history. How often is it updated? Who is updating it?</p> <p>Poorly maintained packages are more likely to have security problems. They may not be upgraded to work with newer versions of your language, framework and other dependencies.</p> <p>A package with a single maintainer is also a much riskier choice than one managed by a team.</p> <p>If the package has a GitHub issues section, or a forum, check if the maintainers respond to problems. If reported issues never get fixed, then it's time to consider alternatives.</p>"},{"location":"guides/choosing_packages/#is-the-package-secure","title":"Is the package secure?","text":"<p>Check the package history. Is there evidence that the maintainers have applied security fixes? Do they regularly update their dependencies?</p> <p>Many GitHub repositories will have READMEs with badges for their security CI tools. Check to see if there are any warnings or known security issues.</p>"},{"location":"guides/choosing_packages/#is-the-package-documented","title":"Is the package documented?","text":"<p>Is there a thorough README or a Wiki? It should be clear how you can install and configure the package, and how to use its features and API.</p>"},{"location":"guides/choosing_packages/#is-the-package-compatible-with-our-licence","title":"Is the package compatible with our licence?","text":"<p>Our open source applications should use the Open Government Licence. So any packages they include should be compatible with this licence.</p> <p>The National Archives has more guidance about open software licences.</p> <p>Also it's important to know that if the package does not have a licence you cannot use it.</p>"},{"location":"guides/continuous_integration/","title":"Continuous integration","text":"<p>NOTE: This guidance only applies to repositories and projects which are public on GitHub and can therefore take advantage of several free integrations. We intend to expand upon this guide in the future to cover the rest of our ecosystem.</p> <p>Continuous Integration (CI) is a development practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early.</p> <p>ThoughtWorks</p> <p>This automated build can check several things, including:</p> <ul> <li>the application compiles successfully (if it needs compiling)</li> <li>unit tests pass</li> <li>code style checks pass (for example, linters)</li> </ul>"},{"location":"guides/continuous_integration/#ci-with-github","title":"CI with GitHub","text":"<p>Many services can integrate with GitHub and automatically respond to new commits or pull requests. So every time you push code to GitHub, it will trigger the build and report the status.</p> <p>GitHub Actions is free to use for open source GitHub repositories.</p> <p>You can configure your repository to always require certain checks when merging to protected branches (like <code>master</code>). If a required check fails, it will block merging until the problem is fixed.</p> <p>We recommend making the build a required check. This should prevent anyone from merging breaking changes into the master branch.</p> <p>There are lots of other tools which can integrate with GitHub, especially if your repository is open source.</p>"},{"location":"guides/continuous_integration/#security","title":"Security","text":"<p>These tools check the security of your project. This can include reporting vulnerabilities in your dependencies, or doing static analysis on your code.</p> <ul> <li>Use Snyk for Node.js projects</li> <li>Use Hakiri for Ruby projects</li> </ul> <p>These tools are free to use for open source GitHub repositories.</p>"},{"location":"guides/continuous_integration/#maintainability-and-test-coverage","title":"Maintainability and test coverage","text":"<p>Defra has a SonarCloud organisation, which should be used to perform static quality analysis checks on your code. It provides a rating for the security, reliability and maintainability of your code and estimates the time it would take to deal with any technical debt.</p> <p>You should include SonarCloud in your CI so that it flags problems it spots in your code, like duplication or complexity.</p> <p>You can also configure your build tool (like GitHub Actions) to report unit test coverage to SonarCloud. SonarCloud will then include your test coverage in its assessment of your code.</p> <p>SonarCloud is free to use for open source GitHub repositories.</p>"},{"location":"guides/continuous_integration/#ci-with-jenkins","title":"CI with Jenkins","text":"<p>We use Jenkins for our internal build servers.</p>"},{"location":"guides/developer_workflows/","title":"Developer Workflows","text":"<p>When working with Git most teams use one of two different workflows depending on their preference.</p> <ul> <li>Feature branch workflow (also known as GitHub flow)</li> <li>Gitflow</li> </ul> <p>Though FBW is the simpler of the two, it should be used by teams that are confident whenever they change the master branch it still remains ready for production (Master is always shippable).</p> <p>Gitflow is useful for teams that like the space to combine multiple features, or are working on a large feature, before then shipping to production.</p>"},{"location":"guides/developer_workflows/#feature-branch-workflow-github-flow","title":"Feature branch workflow (GitHub flow)","text":"<p> <sub>https://guides.github.com/activities/hello-world/branching.png</sub></p> <p>Using this workflow we only use two branches</p> <ul> <li> <p>Master is the main branch developers work from. It represents the latest version of the code. The key principle is that master is always production ready. Anything merged in needs to have been peer reviewed, passed by CI, and ready for release.</p> </li> <li> <p>A Feature branch is started each time we want to add to, update or fix something in the code. We branch off Master when creating the feature, and once complete merge it back in</p> </li> </ul>"},{"location":"guides/developer_workflows/#all-branches","title":"All branches","text":"<p>In all cases when the branch is created it should be pushed up to the origin repo and a pull request created as per the pull request process.</p>"},{"location":"guides/developer_workflows/#gitflow","title":"Gitflow","text":"<p> <sub>https://www.atlassian.com/git/images/tutorials/collaborating/comparing-workflows/gitflow-workflow/05.svg</sub></p> <p>Using this workflow it means our branches have specific uses</p> <ul> <li> <p>Master is the version of code that is in production (see the principle Master is always shippable)</p> </li> <li> <p>We create a Hotfix branch when we need to make a change to production code because of a critical error. When finished we merge the change back into Master, but also Develop</p> </li> <li> <p>Develop is the main branch the developers work from. It represents the current version of the code, including new features we\u2019ve completed but not yet released. You should make this the default branch in GitHub</p> </li> <li> <p>A Feature branch is started each time we want to add to, update or fix something in the code. We branch off Develop when creating the feature, and once complete merge it back into Develop</p> </li> <li> <p>Once we have a set of features we want to put live, we create a Release branch. Last minute fixes and tidying up is done on this branch and then it is merged into Master (put live) and back into Develop</p> </li> </ul>"},{"location":"guides/developer_workflows/#use-of-tools","title":"Use of tools","text":"<p>There are a number of tools you can use to help you with gitflow, for example tools such as Sourcetree have support built in, and you can add support to git via extensions.</p> <p>However these all assume that merging will be done locally and then pushed to the origin repo. Because we create pull requests on all the branches we create, and merge them using the GitHub web UI it means we cannot use these tools for merging.</p> <p>So feel free to use them for creating your branches, however you may find it easier to simply manually create your branches and just ensure you stick to gitflow's naming convention.</p>"},{"location":"guides/docker_guidance/","title":"Docker guidance","text":"<p>A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably across multiple environments.  Docker is a tool to build and run these containers.</p>"},{"location":"guides/docker_guidance/#more-information","title":"More information","text":"<p>Docker introduction on docker.com</p>"},{"location":"guides/docker_guidance/#terminology","title":"Terminology","text":"<p><code>Dockerfile</code> - set of instructions for building a docker image <code>Image</code> - a constructed set of layered docker instructions <code>Container</code> - a running instance of an image</p>"},{"location":"guides/docker_guidance/#multi-stage-builds","title":"Multi stage builds","text":"<p>Dockerfiles should implement multi stage builds to allow different build stages to be targeted for specific purposes.  For example, a final production image does not need all the unit test files and a unit test running image would use a different running command than the application.</p> <p>Below is an example multi stage build which is intended to use the Defra Node.js base image.</p> <pre><code>ARG PARENT_VERSION=1.0.0-node12.16.0\nARG PORT=3000\nARG PORT_DEBUG=9229\n\n# Development\nFROM defradigital/node-development:${PARENT_VERSION} AS development\nARG PARENT_VERSION\nARG REGISTRY\nLABEL uk.gov.defra.parent-image=defradigital/node-development:${PARENT_VERSION}\nARG PORT\nENV PORT ${PORT}\nARG PORT_DEBUG\nEXPOSE ${PORT} ${PORT_DEBUG}\nCOPY --chown=node:node package*.json ./\nRUN npm install\nCOPY --chown=node:node app/ ./app/\nRUN npm run build\nCMD [ \"npm\", \"run\", \"start:watch\" ]\n\n# Production\nFROM defradigital/node:${PARENT_VERSION} AS production\nARG PARENT_VERSION\nARG REGISTRY\nLABEL uk.gov.defra.parent-image=defradigital/node:${PARENT_VERSION}\nARG PORT\nENV PORT ${PORT}\nEXPOSE ${PORT}\nCOPY --from=development /home/node/app/ ./app/\nCOPY --from=development /home/node/package*.json ./\nRUN npm ci\nCMD [ \"node\", \"app\" ]\n</code></pre>"},{"location":"guides/docker_guidance/#docker-compose-guidance","title":"Docker Compose guidance","text":""},{"location":"guides/docker_guidance/#use-override-files-to-reduce-duplication","title":"Use override files to reduce duplication","text":"<p>Additional settings can be applied to a docker compose file by using override files.</p> <p>Override files can be applied by listing the files after the <code>docker-compose</code> command with the <code>-f</code> parameter, i.e.</p> <p><code>docker-compose -f docker-compose.yaml -f docker-compose.override.yaml up</code></p> <p>Note that the above is equivalent to running the command:</p> <p><code>docker-compose up</code></p> <p>as calling <code>docker-compose</code> without specifying any files will run <code>docker-compose</code> with any available <code>docker-compose.yaml</code> and <code>docker-compose.override.yaml</code> files in the executing directory. </p> <p>Note however that:</p> <p><code>docker-compose up -f docker-compose.yaml</code></p> <p>will not apply the docker <code>docker-compose.override.yaml</code> file, only the file specified.</p> <p>One use case is for running tests in CI - common settings can be put into the base <code>docker-compose.yaml</code> file, while changes to the command and containers needed in local development can be placed in override files.</p> <p>The below example demonstrates changing the command and container name for testing:</p> <p><code>docker-compose.yaml</code></p> <pre><code>version: '3.4'\nservices:\n  ffc-demo-service:\n    build: .\n    image: ffc-demo-service\n    container_name: ffc-demo-service\n    environment:\n      DEMO_API: http://demo-api\n\nvolumes:\n  node_modules: {}\n\n</code></pre> <p><code>docker-compose.test.yaml</code></p> <pre><code>version: '3.4'\nservices:\n  ffc-demo-service:\n    command: npm run test\n    container_name: ffc-demo-service-test\n</code></pre> <p>The tests can be run by providing the <code>docker-compose.test.yaml</code> file with a <code>-f</code> parameter:</p> <p><code>docker-compose up -f docker-compose.yaml -f docker-compose.test.yaml</code> </p> <p>It is also recommended not to expose any ports through Docker Compose used in CI as they may conflict with other ports already in use in the build agent.  </p> <p>Further documentation on docker-compose can be found at https://docs.docker.com/compose/reference/overview/#specifying-multiple-compose-files.</p>"},{"location":"guides/docker_guidance/#use-projects-to-provide-unique-volumes-and-networks","title":"Use projects to provide unique volumes and networks","text":"<p>To avoid conflicts when running different permutations of docker files, projects should be specified to segregate the volumes and networks.</p> <p>This can be achieved with the <code>-p</code> switch when calling docker compose on the command line.</p> <p>i.e. to start the service</p> <p><code>docker-compose -p ffc-demo-service -f docker-compose.yaml up</code></p> <p>and to run the tests</p> <p><code>docker-compose -p ffc-demo-service-test -f docker-compose.yaml -f docker-compose.test.yaml up</code></p>"},{"location":"guides/docker_guidance/#use-environment-variables-to-guarantee-unique-projects-and-containers","title":"Use environment variables to guarantee unique projects and containers","text":"<p>When running through CI, a combination of the <code>-p</code> switch and environment variables can be used to ensure each build and test has unique project and container names.  This will prevent conflicts with other build pipelines when using tools such as a single node Jenkins.</p> <p>For example using Jenkins, the following compose files can be started via:</p> <p><code>docker-compose -p ffc-demo-service-$PR_NUMBER-$BUILD_NUMBER -f docker-compose.yaml up</code></p> <p>and tested with</p> <p><code>docker-compose -p ffc-demo-service-test-$PR_NUMBER-$BUILD_NUMBER -f docker-compose.yaml -f docker-compose.test.yaml up</code></p> <p>using <code>PR_NUMBER</code> and <code>BUILD_NUMBER</code> environment variables to isolate build tasks.</p> <p><code>docker-compose.yaml</code></p> <pre><code>version: '3.4'\nservices:\n  ffc-demo-service:\n    build: .\n    image: ffc-demo-service\n    container_name: ffc-demo-service-${PR_NUMBER}-${BUILD_NUMBER}\nvolumes:\n  node_modules: {}\n\n</code></pre> <p><code>docker-compose.test.yaml</code></p> <pre><code>version: '3.4'\nservices:\n  ffc-demo-service:\n    command: npm run test\n    container_name: ffc-demo-service-test-${PR_NUMBER}-${BUILD_NUMBER}\n</code></pre>"},{"location":"guides/docker_guidance/#composing-multiple-repositories-for-local-development","title":"Composing multiple repositories for local development","text":"<p>For scenarios where multiple containers need to be created across multiple repositories, it might be advantageous to create a \"development\" repo.</p> <p>The development repository would: - clone all necessary repositories - builds images from Dockerfiles in each repository by referencing Docker Compose files in those repositories - run containers based on those images in a single Docker network by referencing Docker Compose files in those repositories - run single containers for any shared dependencies across repositories such as message queues or databases</p> <p>To facilitate this, each repository with a potentially shared dependency will need its Docker Compose override files to be setup in such a way that dependency containers can be isolated.  This will allow those repository services to run both in isolation and as part of wider service depending on development needs.</p> <p>For example, let's say we have two repositories, ServiceA and ServiceB.  ServiceA communicates with ServiceB via an ActiveMQ message queue. ServiceB has a PostgreSQL database.</p> <p>ServiceA's Docker Compose files could be structured as follows.</p> <p><code>docker-compose.yaml</code> - builds image and runs ServiceA <code>docker-compose.override.yaml</code> - runs Artemis ActiveMQ container <code>docker-compose.link.yaml</code> - runs ServiceA in a named Docker network</p> <p>ServiceB's Docker Compose files could be structured as follows.</p> <p><code>docker-compose.yaml</code> - builds image and runs ServiceB and PostgreSQL container <code>docker-compose.override.yaml</code> - runs Artemis ActiveMQ container <code>docker-compose.link.yaml</code> - runs ServiceB in a named Docker network</p> <p>ServiceA and ServiceB can be run in isolation by running the following commands in each repository.</p> <p><code>docker-compose build</code> <code>docker-compose up</code></p> <p>The development repository would contain the following.</p> <p><code>docker-compose.yaml</code> - runs Artemis ActiveMQ container in named Docker network</p> <p>A script which would run the following commands:</p> <pre><code>if [ -z \"$(docker network ls --filter name=^NETWORK_NAME$ --format={{.Name}})\" ]; then\n  docker network create NETWORK_NAME\nfi\ndocker-compose up\ndocker-compose -f path/to/ServiceA/docker-compose.yaml -f path/to/ServiceA/docker-compose.link.yaml up --detach\ndocker-compose -f path/to/ServiceB/docker-compose.yaml -f path/to/ServiceB/docker-compose.link.yaml up --detach\n</code></pre>"},{"location":"guides/docker_guidance/#avoiding-docker-composeyaml-in-the-development-repository","title":"Avoiding docker-compose.yaml in the development repository","text":"<p>If it is preferred to avoid the need for an additional <code>docker-compose.yaml</code> file in the development repository itself, an alternative approach would be to explicity declare the shared resources are not started in subsequent <code>override</code> files in the start up script.</p> <p>For example:</p> <pre><code>if [ -z \"$(docker network ls --filter name=^NETWORK_NAME$ --format={{.Name}})\" ]; then\n  docker network create NETWORK_NAME\nfi\ndocker-compose up\ndocker-compose -f path/to/ServiceA/docker-compose.yaml -f path/to/ServiceA/docker-compose.override.yaml -f path/to/ServiceA/docker-compose.link.yaml up --detach\ndocker-compose -f path/to/ServiceB/docker-compose.yaml -f path/to/ServiceB/docker-compose.override.yaml -f path/to/ServiceB/docker-compose.link.yaml up --detach --scale SERVICE_NAME=0\n</code></pre>"},{"location":"guides/docker_guidance/#binding-volumes-to-container","title":"Binding volumes to container","text":"<p>To aide local development, the local workspace can be bound to a Docker volume.  This allows code changes to be automatically picked up within the container without the need to rebuild the image or restart the container.  </p> <p>To best support this, workspaces should be structured so it is simple to determine which files should be bound to Docker volumes as it would not be appropriate to bind everything.  For example, it would not be beneficial to bind <code>node_modules</code> or a <code>README</code>.  </p> <p>Example of Docker compose file with volume binding.  </p> <pre><code>volumes:\n  - ./app/:/home/node/app/\n  - ./test/:/home/node/test/\n  - ./test-output/:/home/node/test-output/\n  - ./package.json:/home/node/package.json\n</code></pre> <p>Changes to any of the directories listed above would automatically be picked up in the running container.</p> <p>Binding also allows developers to take advantage of file watching in testing applications.  Changes made to code locally will automatically be reflected in the running container supporting a TDD approach.</p>"},{"location":"guides/docker_guidance/#dockerignore","title":".dockerignore","text":"<p>A <code>.dockerignore</code> file is a way of preventing local files being copied into an image during build.</p> <p>For example, if a repository contains the following files.</p> <pre><code>app/index.js\napp/config.js\nnode_modules\nindex.js\nREADME.md\nLICENCE\nDockerfile\n</code></pre> <p>The <code>Dockerfile</code> in this repository includes the following layer which would copy all local files to the container.</p> <pre><code>COPY . .\n</code></pre> <p>When the image is built then all files in the repository are copied to the image.  In this scenario, it is not ideal for performance and disk space reasons to copy the <code>node_modules</code>, <code>LICENCE</code>, <code>Dockerfile</code> or <code>README.md</code> to the image.</p> <p>To prevent this a <code>.dockerignore</code> file should be added with the following content.</p> <pre><code>node_modules\nDockerfile\nLICENCE\nREADME.md\n</code></pre>"},{"location":"guides/docker_guidance/#container-and-image-names-using-docker-compose","title":"Container and image names using Docker Compose","text":"<p>If an image name or container name is not specified in a Docker Compose file, then Docker Compose will determine it's own based on the service name.  This can result in duplication in the name and unpredictabilty in futher container interaction.</p>"},{"location":"guides/docker_guidance/#set-image-and-container-name","title":"Set image and container name","text":"<pre><code>version: '3.7'\nservices:\n  my-service:\n    image: my-service\n    container_name: my-service\n</code></pre>"},{"location":"guides/docker_guidance/#preserving-database-volumes-during-test-runs","title":"Preserving database volumes during test runs","text":"<p>In many scenarios it is beneficial to utilise Docker to run local integration tests against a containerised dependency such as a database or message broker.</p> <p>These tests would typically write and delete data during test execution.  In order to prevent this impacting on local development data and still avoid duplication in Docker Compose definitions, volumes should be declared separate to the database definition.</p> <p>For example, if you have the following Docker Compose files</p> <ul> <li><code>docker-compose.yaml</code> - base definition used in all scenarios</li> <li><code>docker-compose.override.yaml</code> - applied when running locally only</li> <li><code>docker-compose.test.yaml</code> - applied when running tests only</li> </ul> <p>Then using a Postgres image as an example each definition should contain the following.</p>"},{"location":"guides/docker_guidance/#docker-composeyaml","title":"docker-compose.yaml","text":"<pre><code>version: '3.7'\nservices:\n  my-postgres-service:\n    image: postgres:11.4-alpine\n    environment:\n      POSTGRES_DB: my_database\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_USERNAME: postgres      \n</code></pre>"},{"location":"guides/docker_guidance/#docker-composeoverrideyaml","title":"docker-compose.override.yaml","text":"<pre><code>version: '3.7'\nservices:\n  ffc-demo-claim-postgres:\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data: {}\n</code></pre> <p>Then volume and port bindings are only used during local development and any local tests runs will not impact development data.</p>"},{"location":"guides/java_auto_format_eclipse/","title":"Java Auto-format with Eclipse","text":"<p>Eclipse is an</p> <p>integrated development environment (IDE) used in computer programming. It contains a base workspace and an extensible plug-in system for customizing the environment. Eclipse is written mostly in Java and its primary use is for developing Java applications, but it may also be used to develop applications in other programming languages via plug-ins.</p> <p>We have a standard for Java which can be applied automatically when saving a file in Eclipse.</p>"},{"location":"guides/java_auto_format_eclipse/#the-format-file","title":"The format file","text":"<p>In these guides is a 'code style' <code>.xml</code> that can be used called defra_java_eclipse_code_style.xml.</p>"},{"location":"guides/java_auto_format_eclipse/#importing-the-file","title":"Importing the file","text":"<p>In Eclipse, select <code>Windows &gt; Preferences &gt; Java &gt; Code Style &gt; Formatter &gt; Import</code>.</p> <p>Select the file and then apply the change.</p>"},{"location":"guides/java_auto_format_eclipse/#applying-the-format-on-save","title":"Applying the format on save","text":"<p>In Eclipse, select <code>Windows &gt; Preferences &gt; Java &gt; Editor &gt; Save Actions</code> and then check</p> <ul> <li><code>Perform the selected actions on save</code></li> <li><code>Format source code</code></li> </ul>"},{"location":"guides/kubernetes/","title":"Kubernetes guidance","text":"<p>This guidance is aimed at those deploying to and supporting Kubernetes clusters and includes helpful how to guides for common Kubernetes tasks and lessons learned.</p>"},{"location":"guides/kubernetes/#configure-nginx-ingress-controller","title":"Configure NGINX Ingress Controller","text":"<p>An Ingress controller is an application that runs in a Kubernetes cluster and configures an HTTP load balancer according to Ingress resources.</p> <p>In the case of NGINX, the Ingress controller is deployed in a pod along with the load balancer.</p>"},{"location":"guides/kubernetes/#installation","title":"Installation","text":"<p>The documentation for NGINX's chart includes details on how to install it.</p> <p>TL;DR:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nhelm install my-release ingress-nginx/ingress-nginx\n</code></pre>"},{"location":"guides/kubernetes/#creating-a-namespace-for-a-service","title":"Creating a namespace for a service","text":"<p>Each service in a cluster will have their own dedicated namespace in each cluster.</p> <p>This allows logical separation between services as well as the enabling simpler implementation of monitoring, RBAC and stability mechanisms.</p>"},{"location":"guides/kubernetes/#requirements","title":"Requirements","text":"<p>The following are the suggested outcomes of a namespace. - <code>ResourceQuota</code> resource to limit resource usage - <code>RoleBinding</code> resource to restrict interaction to delivery team</p> <p>Example setup instructions are included in the FFC Kubernetes Configuration repository.</p>"},{"location":"guides/kubernetes/#install-kubernetes-dashboard","title":"Install Kubernetes dashboard","text":"<p>The Kubernetes dashboard is a web-based Kubernetes user interface.</p>"},{"location":"guides/kubernetes/#installation_1","title":"Installation","text":"<p>Install the dashboard to a Kubernetes cluster using the <code>kubectl apply</code> command specified in the Deploying the dashboard UI section in the below link.</p> <p>https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</p> <p>Example: <code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml</code></p>"},{"location":"guides/kubernetes/#create-default-user-and-access-token","title":"Create default user and access token","text":"<p>Follow the guide in the below link. https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md</p>"},{"location":"guides/kubernetes/#run-dashboard","title":"Run dashboard","text":"<ol> <li>Run terminal command <code>kubectl proxy</code></li> <li>Access dashboard at http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</li> </ol>"},{"location":"guides/kubernetes/#interact-with-cluster","title":"Interact with cluster","text":"<p><code>kubectl</code> is used to interact with the cluster.  In order to use <code>kubectl</code> with an FFC cluster, a <code>kubeconfig</code> file for the cluster is required.</p>"},{"location":"guides/kubernetes/#acquiring-a-kubeconfig-file-for-a-cluster-in-aks","title":"Acquiring a Kubeconfig file for a cluster in AKS","text":"<p>To acquire a Kubeconfig, a subscription Id is needed along with the name of the cluster and the resource group in which it resides.  This information can be acquired via the Azure Portal or from CSC.</p> <p><code>az account set --subscription SUBSCRIPTION_ID</code></p> <p><code>az aks get-credentials --resource-group RESOURCE_GROUP --name CLUSTER --file WHERE_TO_SAVE_KUBECONFIG</code></p> <p>Warning if the file parameter is not passed, the Kubeconfig will be merged with the users default configuration file stored at <code>~/.kube/config</code>. This can result in the file becoming corrupted due to the nature of the merge.</p>"},{"location":"guides/kubernetes/#productivity-tools","title":"Productivity Tools","text":"<p>Developers may find it more productive to use tools such as k9s or kubectl-aliases to avoid needing to regularly type long terminal commands to interact with the cluster.</p>"},{"location":"guides/kubernetes/#authenticating-with-cloud-resources","title":"Authenticating with cloud resources","text":"<p>Both Azure and AWS' managed Kubernetes service, AKS and EKS respectively offer mechanisms to authenticate with cloud resources without the need for credential management in applications.</p>"},{"location":"guides/kubernetes/#aad-pod-identity-azure","title":"AAD Pod Identity (Azure)","text":"<p>With this approach, Pod Identity pods are deployed to a cluster. Application code then uses these resources to request tokens to use in subsequent authentication requests to Azure resources.  Identity mappings are also deployed to the cluster.</p> <p>Token lifecycle is managed within the application itself, some Azure resources have official SDKs that manage this lifecycle, but others the developer must handle the token refresh themselves.</p> <p>See full setup details in the official documentation.</p>"},{"location":"guides/kubernetes/#iam-role-for-service-accounts-aws","title":"IAM role for Service Accounts (AWS)","text":"<p>With this approach, only an identity mapping resource is required and the cluster will automatically ensure a valid token is available to the application.</p> <p>As token lifecycle is managed outside of the application, this approach results in less complex application code than the Azure implementation.</p> <p>See full setup details in the official documentation</p>"},{"location":"guides/kubernetes/#azure-aad-pod-identity","title":"Azure - AAD Pod Identity","text":"<p>AAD Pod Identity enables Kubernetes applications to access cloud resources securely with Azure Active Directory (AAD).</p> <p>Using Kubernetes primitives, administrators configure identities and bindings to match pods. Then your containerized applications can leverage any resource in the cloud that depends on AAD as an identity provider.</p> <p>See Kubernetes Secret standards for standards for secret management when Pod Identity is not suitable.</p>"},{"location":"guides/kubernetes/#probes","title":"Probes","text":"<p>Kubernetes has two types of probes, readiness and liveness.</p> <p>Kubernetes uses readiness probes to know when a container is ready to start accepting traffic.</p> <p>Kubernetes uses liveness probes to know when to restart a container.</p>"},{"location":"guides/kubernetes/#configuring-probes","title":"Configuring probes","text":"<p>Probes can be configured in the Helm chart on a <code>Deployment</code> resource, under the container node.</p> <p>The below is a simple example of an HTTP readiness and liveness probes.</p> <pre><code>readinessProbe:\n  path: /healthy\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n\nlivenessProbe:\n  path: /healthz\n  port: 3000\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  failureThreshold: 3\n</code></pre> <p>In this example, the cluster will wait for 10 seconds after the pod is deployed.  It will then poll both the liveness and readiness endpoints on port 3000 every 10 seconds.  </p> <p>If it receives three successive status codes other than 200 for the readiness probe it will stop routing traffic to that pod.</p> <p>If it receives three successive status codes other than 200 for the liveness probe it will assume the pod is unresponsive and kill it.</p> <p>Note that a liveness probe works in conjunction with the restartPolicy value. In order to restart the restartPolicy must be set to Always or OnFailure.</p>"},{"location":"guides/kubernetes/#values","title":"Values","text":"<p><code>path</code>: the URL route the liveness probe should sent a response to</p> <p><code>port</code>: the port on which the service is exposing</p> <p><code>initialDelaySeconds</code>: how long before the first probe should be sent. This should be safely longer than it takes the pod to start up, otherwise the pod could be stuck in a reboot loop</p> <p><code>periodSeconds</code>: how often the liveness probe should check the pod is responsive. Recommendation is between 10 and 20 seconds</p> <p><code>failureThreshold</code>: how many probe failures before the pod is automatically restarted</p> <p>As well as HTTP probes, there are also command and TCP based probes, full details can be found in the documentation https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</p>"},{"location":"guides/kubernetes/#descheduler","title":"Descheduler","text":"<p>Scheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by a component of Kubernetes called kube-scheduler. </p> <p>The scheduler's decisions, whether or where a pod can or can not be scheduled, are guided by its configurable policy which comprises of set of rules, called predicates and priorities. </p> <p>The scheduler's decisions are influenced by its view of a Kubernetes cluster at that point of time when a new pod appears for scheduling. </p> <p>As Kubernetes clusters are very dynamic and their state changes over time, there may be desire to move already running pods to some other nodes for various reasons:</p> <ul> <li>some nodes are under or over utilised</li> <li>the original scheduling decision does not hold true any more, as taints or labels are added to or removed from nodes, pod/node affinity requirements are not satisfied any more</li> <li>some nodes failed and their pods moved to other nodes</li> <li>new nodes are added to clusters</li> <li>consequently, there might be several pods scheduled on less desired nodes in a cluster</li> </ul> <p>Descheduler, based on its policy, finds pods that can be moved and evicts them</p> <p>Descheduler can be installed using Helm as described here</p>"},{"location":"guides/kubernetes/#autoscaling","title":"Autoscaling","text":"<p>Kubernetes pods and nodes can be automatically scaled on demand.  See below resources for usage.</p> <p>Horizontal Pod Autoscaler</p> <p>Vertical Pod Autoscaler</p> <p>Cluster autoscaling (AKS)</p> <p>Cluster autoscaling (EKS)</p>"},{"location":"guides/mobile_app_guidance/","title":"Mobile application guidance","text":""},{"location":"guides/mobile_app_guidance/#introduction","title":"Introduction","text":"<p>The huge increase in the use of mobile devices means that mobile application development is now of critical interest to the Defra family, even more so given the amount of field-work being carried out by workers within the Defra family.</p>"},{"location":"guides/mobile_app_guidance/#scope","title":"Scope","text":"<p>This guidance applies to mobile application in-house development and management.</p> <p>This guidance is intended to cover all forms of mobile application: from fully \"off-line capable\" progressive web applications, through cross-platform apps to fully native apps.</p> <p>This document should be read in conjunction with the Mobile Application Standards (Mobile Application Standards).</p>"},{"location":"guides/mobile_app_guidance/#caveats","title":"Caveats","text":"<p>This is a fast moving area involving multiple platforms and diverse technology stacks: this guidance is early and provisional.</p>"},{"location":"guides/mobile_app_guidance/#extra-security-guidance","title":"Extra security guidance","text":"<p>Please ensure that you have read the \"Extra Security Considerations\" of the Mobile Application Standards before reading this section.</p>"},{"location":"guides/mobile_app_guidance/#application-wrappers","title":"Application wrappers","text":"<p>The Mobile Application Standards refer to various mechanisms for encrypting traffic going to and from the app over usually public networks: specifically HTTPS/Transport Layer Security, use of Virtual Private Networks or use of an in-built application security wrapper. The potential use of in-built application security wrappers requires some further guidance. This technique, often called \"application wrapping\", is a service commonly offered by Mobile Device Management (MDM) software suites. It involves using the MDM suite in some way to allow the MDM software to add an extra layer of security to your application bundle, such as encryption for data traffic (based on standard transport layer security/secure sockets, i.e. shared and verifiable certificates) or locally stored application data (with that encryption usually based on a secret known only to the user, such as their device passcode). Another important advantage of application wrapping is to lock-down access, either to data stored on the device or remote systems, on a per app basis.</p> <p>While the use of this technique cannot be entirely ruled out, it is declining in popularity as it has many disadvantages. Application wrapping complicates deployment, creating an extra step and meaning that the wrapped app may not work properly outside the MDM eco-system. Even worse, it involves allowing the \"application wrapping service\" to make last minute changes to your application bundle, in ways that are largely outside of your control and, because they involve security, are likely to be deliberately obscure. It is even possible that the \"application wrapping service\" will itself be changing in ways unknown to us, often in response to continuously evolving security threats, so that an app wrapped at one moment may not be identical to one wrapped at the next, even if we have not ourselves made any changes to the app. Should the app wrapping itself cause any issues with the working of the app, those issues can therefore be extremely difficult to resolve.</p> <p>App wrapping should therefore only be adopted if it is deemed really necessary and otherwise unavoidable. On Apple devices, which natively run each app in its own \"sandbox\", so that apps cannot access each other's data, access to data stored locally on the device is already locked down on a per app basis. That means that, on Apple devices, the remaining security hole potentially pluggable by app wrapping is the handling of data traffic and access to remote systems. However, Mobile Device Management suites offer a simpler way to plug that hole: a technique called per app or in-app VPN, where the app is forced to use Virtual Private Network software provided by the MDM suite to both encrypt traffic and allow the app access to remote systems that would otherwise be blocked. In-app VPN is therefore the default approach for Apple devices, and it is very unlikely per app wrapping should ever be necessary in that case. (NB we are talking here about a per-app VPN app shipped with our adopted Mobile Device Management suite, and configured for internal use. These are not to be confused with third party VPN apps that are generally available, and which should normally be avoided as they often harvest your data).</p> <p>Conversely the degree of protection offered by Android app sandboxing is version dependent and regarded as less secure, so, for an app using Android devices, there is a possible case for wrapping, if, in this instance, the wrapper technology actually adds value and the data in scope requires it.</p>"},{"location":"guides/mobile_app_guidance/#protecting-data-at-rest-on-the-device","title":"Protecting data at rest on the device","text":"<p>Mobile applications and associated data held locally on devices will be subject to exactly the same heightened physical security risks as the devices on which the apps are running. Mobile devices are relatively easily lost or stolen. This means it is very important to consult the latest security standards as to how data stored on the device should be protected, and, if the application is intended for internal use on a managed device, that you consult with the Cloud Mobile Services team in Group Infrastructure and Operations to find out what the Mobile Device Management (MDM) software used to manage the device may mean for how you should implement your app. For example, you may discover \"out of the box\" Mobile Device Management features that can be utilised to better protect your app's data, such as the ability to force users of managed devices to set more secure passcodes before they can log into the device and decrypt its data.</p>"},{"location":"guides/mobile_app_guidance/#authenticating-external-users","title":"Authenticating external users","text":"<p>Mobile apps are increasingly being used by organisations as a way to provide publicly accessible services. Sometimes, such publicly accessible apps also need to support a form of authentication. This is likely to be more challenging than adopting authentication for systems built for internal use, where internal authentication systems and associated business processes are likely to be well established. Furthermore, any authentication mechanisms adopted by the mobile app must be capable of operating securely within one of the most insecure contexts conceivable: on an unmanaged mobile device used by any member of the general public. Developers and application architects must therefore research, consult and very carefully decide as to the authentication mechanisms the application will need to adopt.</p> <p>To that end, bear in mind the following points. Authentication that is established by linking the user to, say, a real email address, by sending the user an email to that address and making the user respond, is relatively easy to implement, but be aware that, if the user is required to prove their actual identity, that can be much more problematic and involved. Please consult the latest security standards, and bear in mind that even already available verification services such as gov.uk.Verify are not without their own very significant challenges. Implementing reliable ways of allowing someone to prove their real identity is simply a very difficult task. Given this fact it would be worth considering if, in reality, an app currently considered to be for use by the \"general public\" is really intended to be used by parties already known to the Defra family, such as contracted companies or suppliers, or regulated entities who already hold licences issued by the Defra family. In such a scenario, where the target users are already known, a \"by invitation\" pattern of establishing authentication by sending details to an already officially recorded email or physical address may provide sufficient assurance that we know the real identity of the user with whom we have engaged.</p>"},{"location":"guides/mobile_app_guidance/#extra-guidance-on-progressive-web-apps","title":"Extra guidance on Progressive Web Apps","text":"<p>Unfortunately, the Defra family mandating of Apple devices does have some negative implications for Progressive Web Apps, of which we need to be aware. Apple force the use of the Safari/WebKit browser on all iOS/iPad OS devices, and, of all the major browsers, Safari has the most limited support for Progressive Web Apps. There is no way around this as even other browser apps in the Apple app store, such as Chrome, are merely wrappers for WebKit. The most notable restrictions are likely to be around size of storage (though, in January 2020, and if using IndexedDB, which is usually the most functional local storage option in any case, over 1 GB was found to be available on a Defra iPad), persistence of stored data (as of July 2020 you can depend on IndexedDB data being retained for at least a month, and possibly much longer, even if an app is completely unused, and data has been observed to survive an iPad OS and Safari update. This therefore seems to be a diminishing issue, though one of which you should be aware.) and the fact that, in Safari on iOS or iPad OS, the way you install an app on the home screen is still currently non-standard.</p> <p>It is possible to build a perfectly useful PWA that runs on Safari, including being able to access native features such as taking photos (though, at the time of writing, only stills and not videos) and accessing geolocation. However, the situation is always changing, usually improving, though that cannot be guaranteed, so you must use a site such as https://whatwebcando.today/ to assess which features are actually available in Safari on a Defra Apple device before committing to delivering a PWA. Similarly, you should assess the current storage capabilities of Safari on a device by using a web site such as https://demo.agektmr.com/storage/.</p> <p>Of course, PWAs are storing data locally: otherwise they cannot work offline. That means that you will also need to consider the security of the data at rest on the device. These considerations are slightly different for PWAs than for other apps, as the data is being retained in the browser. If, as is likely and recommended, you are using IndexedDB to store your data, then all browsers compliant with the W3C spec, such as Safari, will only allow JavaScript from the same origin as the PWA that has created the database to access that data. Also, your data will be protected by your device's normal data encryption: for example, for an internally managed Defra iPad where the setting of a passcode is enforced, that means the data will be encrypted whenever the device is turned off, and only decrypted when the user re-enters the correct passcode. That means \"reasonably\" secure, but most likely not as secure as Apple's app sandboxing, and this should be borne in mind when assessing whether or not a PWA is appropriate, given the sensitivity, or not, of the data it will be handling, and the general security features of the devices where the app will be in use.</p>"},{"location":"guides/mobile_app_guidance/#building-cross-platform-apps","title":"Building cross-platform apps","text":"<p>It is possible that, having run through the hierarchy of Mobile Application development options as specified in the Mobile Application Standards the decision will have been reached to build a mobile app using a cross platform technology stack. These stacks are ways of using the same, or very close to the same, code-base but still build the app in ways that can allow it to run on multiple platforms and multiple devices. Otherwise a completely different code base, using a different language and development framework, is needed to build an app for each platform.</p> <p>There are many such cross platform technology stacks, but some are obscure or are nearing obsolescence. The only cross platform stacks that should be considered for new projects, from the newest to the oldest, are Flutter, React Native or Xamarin. All are backed by major vendors: Flutter by Google, React Native by Facebook and Xamarin by Microsoft, so all should have a reasonable shelf life, with Xamarin possibly the most \"at risk\" as it the oldest and has been overtaken by React Native and Flutter in terms of popularity (according to a 2018 Stack Overflow survey). Flutter requires developers to learn its own programming language, \"Dart\", though this is based on Java and C# (as this is an open eco-system it is impossible to mandate an IDE, but Android Studio or Visual Studio Code are popular choices). React Native uses JavaScript as its programming language but also the whole React framework including a presentational technology called JSX, so is a relatively easy transition for React developers, but not so much for developers who know JavaScript but are not familiar with React (here again an IDE cannot be mandated, but Atom and Visual Studio Code are popular choices). Xamarin uses C# so ought to be a reasonable match for .NET developers. Xamarin will require you to use full-blown Visual Studio for the IDE.</p> <p>A cross platform approach built using a modern framework such as Flutter, React Native or Xamarin will very usually allow for the building of a suitably reliable and functional mobile app. As a rough rule of thumb, the more mature the framework the more capable it is likely to be in this regard, as, not only will the framework have had more time to include the desired features but, if those features are not already available \"out of the box\", it is also more likely that a third party library will be available for use that does the job. However, cross platform apps may not provide the exact look and feel and performance of a native app (out of Flutter, React Native and Xamarin, \"React Native\" is likely to give the most native look and feel, as it's uses native presentational components, despite integrating with each platform in, out of the three platforms, the least \"native\" way. React Native integrates via each platform's native JavaScript engine, where-as Flutter and Xamarin both run within an engine of their own, the \"Shell plus Dart Virtual Machine\" for Flutter and \"Mono\" for Xamarin, that interact directly with native libraries). Even more importantly, it is just about possible that the framework may not be able to access some native functionality of the device, so you are advised to establish that any crucial native functionality will be available to the app before committing to a certain development option.</p>"},{"location":"guides/mono_or_multi_repo/","title":"Mono repo and a multi repo","text":"<p>A mono repo is a single repository that contains all the code for multiple projects. A multi repo is a collection of repositories, each containing the code for a single project. Both approaches have their own advantages and disadvantages. This guide will help you decide which approach is best for your project.</p> <p>Teams should proactively choose between a mono repo and a multi repo based on the project requirements. Multi repositories are the preferred approach with mono repositories only being used for small single project cases.</p> <p>The decision should be made at the beginning of the project, as it can be difficult to switch between the two later on.  The level of difficultly will typically depend on the duration of time the project has been running.</p> <p>The decision should be reviewed periodically to ensure that it still meets the project requirements. If the project requirements change, it may be necessary to switch between a mono repo and a multi repo.</p>"},{"location":"guides/mono_or_multi_repo/#considerations","title":"Considerations","text":"<p>Teams should consider the following key drivers for choosing between a mono repo and a multi repo:</p> <ul> <li>Size and complexity of the project</li> <li>Technology choices</li> <li>Application composition</li> <li>Collaboration</li> <li>CI/CD</li> <li>Versioning and release process</li> <li>Commit history</li> <li>Security</li> <li>Ownership</li> </ul>"},{"location":"guides/mono_or_multi_repo/#size-and-complexity","title":"Size and complexity","text":"<p>A mono repo is best suited for small, single language projects with a low number of dependencies. A multi repo is best suited for large, complex projects with multiple languages and dependencies.</p> <p>Larger projects can be difficult to manage in a mono repo, as the size and complexity of the codebase can make it difficult to navigate and understand. In this case, a multi repo may be a better choice, as it allows teams to work on smaller, more manageable codebases.</p> <p>Size of downloading the repository is also a consideration. A mono repo can be large and take a long time to download, particularly for new team members. A multi repo can be smaller and quicker to download, as each repository contains a smaller amount of code.</p> <p>In all cases teams should consider the content of their repositories to ensure that they are not storing unnecessary files or data.</p> <p>Performance of scanning tools such as secret detection tools is also increased with smaller repositories.</p>"},{"location":"guides/mono_or_multi_repo/#technology-choices","title":"Technology choices","text":"<p>Mono repos are best suited for projects that use a single technology stack. Multi repos are best suited for projects that use multiple technology stacks.</p> <p>If a project uses multiple technology stacks, then it is more difficult to keep a single repository structure consistent.</p> <p>Multiple technologies in a single repository can also lead to conflicts between any plugins or dependencies that are used by the different technologies within the repository.</p>"},{"location":"guides/mono_or_multi_repo/#application-composition","title":"Application composition","text":"<p>Mono repos are best suited if components are compiled into a single application.  However, all other considerations should be taken into account before making a decision as even with monolithic application, a multi repo may be the better approach.</p> <p>Multi repos are best suited if components are independently deployable applications.</p>"},{"location":"guides/mono_or_multi_repo/#collaboration","title":"Collaboration","text":"<p>Mono repos simplify collaboration as all the code is in one place.  This reduces the complexity of access control and visibility of code, pull requests, issues and release notes.</p> <p>However mono repos may also tightly coupled collaboration needs across independent teams.  This can lead to conflicts and bottlenecks in the development process.</p> <p>Multi repos are more difficult to collaborate as data is distributed across multiple repositories.  This can mean key data is missed which leads to risk.</p> <p>If multiple teams support different components then a mono repository should not be used.  Similarly if individuals within a team are responsible for different components such as frontend and backend, then a mono repository should not be used.</p>"},{"location":"guides/mono_or_multi_repo/#versioning-and-release-process","title":"Versioning and release process","text":"<p>For simple projects a single version may be sufficient within a mono repo, however for more complex projects a versioning strategy should be considered.  This may involve additional tooling to independently version and release projects.</p> <p>Tools such as Lerna can help manage dependencies and build processes but typically only support one technology stack.</p> <p>Multi repos can have independent versioning strategies for each project which can simplify the release process.</p> <p>If not composed into a single application, independent components should be released and versioned independently.</p> <p>The scale of the service should be considered as releasing or rolling back a large scale mono repo code base can add unnecessary risk and complexity.</p>"},{"location":"guides/mono_or_multi_repo/#cicd","title":"CI/CD","text":"<p>Mono repos simplify CI/CD as multiple projects can be built and tested together. This can reduce the complexity of creating and managing multiple pipelines and dependencies.</p> <p>However, mono repos can also lead to longer build times and increased complexity in managing dependencies, particularly if multiple technology choices are used.</p> <p>Additional tooling and complexity may be required to independently build and test projects within a mono repo depending on the scale.</p> <p>Multi repos require more configuration for CI/CD as each project needs its own pipeline which can lead to duplication of effort and complexity in co-ordination of pipeline updates.</p> <p>The advantage is a more optimised pipeline for each project.</p>"},{"location":"guides/mono_or_multi_repo/#commit-history","title":"Commit history","text":"<p>Mono repos have a single commit history which can make it easier to track changes and understand the history of the codebase.</p> <p>Multi repos have multiple commit histories which can make it more difficult to track changes and understand the history of the codebase, however the history is more focused.</p>"},{"location":"guides/mono_or_multi_repo/#security","title":"Security","text":"<p>Mono repos can simplify security as access control and visibility is centralised.  Dependency scanning tools are also simpler to maintain.</p> <p>Mono repos can also increase the risk of a single point of failure and increase the impact of a security incident.</p> <p>It may also be difficult to restrict access to sensitive code or data within a mono repo.</p> <p>Multi repos can increase the complexity of security as access control and visibility is distributed across multiple repositories increasing the risk stale users may have access to sensitive data.</p> <p>Multi repos allow for more granular access control.</p>"},{"location":"guides/mono_or_multi_repo/#ownership","title":"Ownership","text":"<p>Mono repos work when a single team owns all the code and is expected to maintain it.</p> <p>Multi repos support multiple teams owning different parts of the codebase.</p>"},{"location":"guides/mono_or_multi_repo/#conclusion","title":"Conclusion","text":"<p>As per Defra version control standards, teams should follow a multi repository approach as the default.  Mono repositories should only be used for small scale projects consisting of a single application or package with only one development team.</p>"},{"location":"guides/new_starters/","title":"Guide for new Defra developers","text":"<p>Welcome to Defra! This guide is an overview of tools and resources available to you as a developer, and how to get access to them.</p>"},{"location":"guides/new_starters/#developing-in-government","title":"Developing in government","text":""},{"location":"guides/new_starters/#defra-development-standards","title":"Defra development standards","text":"<p>You can find our software development standards on GitHub (in this very repository!)</p>"},{"location":"guides/new_starters/#cross-government-standards","title":"Cross-government standards","text":"<p>The Government Digital Service (GDS) sets standards for government digital services.</p> <p>You can read these standards here:</p> <ul> <li>Service standard</li> <li>Service manual</li> </ul>"},{"location":"guides/new_starters/#meet-the-community","title":"Meet the community","text":""},{"location":"guides/new_starters/#get-onto-slack","title":"Get onto Slack","text":"<p>Digital teams in Defra use Slack to keep in touch. Slack is a messaging app that lets you talk to colleagues in your team, department, or even across government.</p> <p>Join the Defra Digital Slack</p> <p>If you have a Defra email address, you should be able to sign up to this Slack workspace yourself.</p> <p>You may also want to join the UK Government Digital Slack. This has thousands of members from across government, and useful channels about software development and security.</p>"},{"location":"guides/new_starters/#come-to-a-developer-meetup","title":"Come to a developer meetup","text":"<p>Developer community of practice meetups are a chance to meet other developers, share what you\u2019re working on, and find out news and updates about best practice.</p> <p>Participating in the community of practice is considered part of your core job, so we encourage both permanent and contract devs to come along.</p> <p>Meetups usually happen in our four hub offices, but developers from any office are welcome to attend.</p>"},{"location":"guides/new_starters/#north-east-meetup","title":"North East meetup","text":"<ul> <li>Meets every other Thursday at 2pm in Foss House, York</li> <li>Organised by John Watson</li> <li>Check the #york-dev-test-cop channel in defra-digital Slack for updates</li> </ul>"},{"location":"guides/new_starters/#north-west-meetup","title":"North West meetup","text":"<ul> <li>Meets every Monday at 1pm in Richard Fairclough House, Warrington</li> <li>Organised by Paul Shaw and Lynne Farrow</li> </ul>"},{"location":"guides/new_starters/#south-west-meetup","title":"South West meetup","text":"<ul> <li>Meets every other Monday at 2pm in Temple Quay House, Bristol</li> <li>Organised by Alan Cruikshanks</li> <li>Check the #development channel in defra-digital Slack for updates</li> </ul>"},{"location":"guides/new_starters/#tech-and-tools","title":"Tech and tools","text":"<p>What technology you\u2019ll be using depends on the service you work on. Here are some common resources and tools to be aware of, and how to access them.</p>"},{"location":"guides/new_starters/#access-to-git-vpn-jenkins-and-more","title":"Access to Git, VPN, Jenkins and more","text":"<p>Access to GitHub, GitLab, Jenkins and other infrastructure services is managed by the Web Ops team.</p> <p>We aim to develop in the open on GitHub. However, some repos (especially things like environment config) are privately stored on GitLab.</p> <p>You will need VPN access to get to GitLab, Jenkins and our development environments.</p> <p>Contact the Web Ops team or another developer to request access to these services.</p>"},{"location":"guides/new_starters/#access-to-jira","title":"Access to Jira","text":"<p>Many teams use the project management tool Jira to manage their work.</p> <p>How to request Jira access</p>"},{"location":"guides/plsql_auto_format_toad/","title":"PL/SQL Auto-format with TOAD","text":"<p>Toad is a</p> <p>[..] database management toolset from Quest Software that database developers, database administrators, and data analysts use to manage both relational and non-relational databases using SQL.</p> <p>Some teams in Defra have licenses for Toad, and use it when working with our Oracle databases.</p> <p>We have a standard for PL/SQL which can be applied automatically using the auto-format tool in Toad.</p>"},{"location":"guides/plsql_auto_format_toad/#the-format-file","title":"The format file","text":"<p>In these guides is an auto-format <code>.opt</code> file that can be used called defra_plsql_toad_fmt.opt.</p>"},{"location":"guides/plsql_auto_format_toad/#importing-the-file","title":"Importing the file","text":"<p>In Toad, open an Editor window. Right click on the editor pane and select <code>Formatting Tools &gt; Formatting Options</code>.</p> <p>Select the file open icon at the top left of the screen (\"Load options from file\" is the tool tip) and then browse to where you have located a copy of the file. Select the file then hit \"Apply\". Then hit \"OK\" to close the formatter options window.</p>"},{"location":"guides/plsql_auto_format_toad/#applying-the-format","title":"Applying the format","text":"<p>To apply the formatter options to any SQL or PL/SQL code open the code in the Editor Window, right click and then select <code>Formatting Tools &gt; Format</code>.</p>"},{"location":"guides/sql_prompt_tool/","title":"SQL Prompt","text":"<p>At the moment we are trialing the use of SQL Prompt to perform code analysis and formatting. This was pretty much the only tool we could find that could format and analyse T-SQL that worked. We tried a few free ones but these were not very good and only did basic formatting, not code analysis. A 28 day trial version can be downloaded from https://www.red-gate.com/products/sql-development/sql-prompt/.</p> <p>SQL Prompt has the following funtionality</p> <ul> <li>Advanced IntelliSense-style code completion</li> <li>Code snippet library</li> <li>Refactor SQL code</li> <li>Fast and comprehensive code analysis</li> <li>SSMS tab history and coloring</li> <li>Advanced customizable and sharable code formatting</li> </ul> <p>Follow these links for SQL Prompt's Documentation and the Code Analysis Rules it uses.</p> <p>We trialed SQL Prompt over 2 weeks and have found it very good at tidying up code and making it consistent. We also found thats it's other features made writing T-SQL quicker &amp; easier. Although we were happy with its settings straight out of the box these can be changed and shared easily.</p> <p>More trialing by other T-SQL developers would be useful before any decisions are made.</p>"},{"location":"guides/style_guide_for_standards/","title":"Style guide for software development standards","text":"<p>This guide tells you how to write and format standards and other documents for this repository.</p> <p>We want our standards to be:</p> <ul> <li>clear</li> <li>consistent</li> <li>easy to read</li> </ul> <p>If a standard is easy to understand, it's also easier to follow it correctly.</p> <p>Guidelines for writing for the web</p>"},{"location":"guides/style_guide_for_standards/#be-clear-and-focused","title":"Be clear and focused","text":"<p>A standard should set clear rules. A guide should give clear advice.</p> <p>Be concise. Focus on what is most important. Avoid woolly language and excessive detail.</p> <p>It should be straightforward to answer \"Does this project meet the standard?\" with a \"yes\" or \"no\". Basically, in developer terms, a standard should be easily testable.</p>"},{"location":"guides/style_guide_for_standards/#be-easy-to-skim","title":"Be easy to skim","text":"<p>We'd love everyone to read every last word of these standards. But in reality, people often skim the content they read on the web.</p> <p>To make content easier to skim:</p> <ul> <li>use short sentences and paragraphs</li> <li>break up writing into multiple sections with clear subheadings</li> <li>display information in bulleted lists</li> <li>put the most important information first</li> </ul> <p>This helps users to quickly understand the main points of a standard. It also makes it easier to find the specific information they're looking for.</p>"},{"location":"guides/style_guide_for_standards/#use-plain-english","title":"Use plain English","text":"<p>Our guides and standards should use plain English whenever possible. What we write about can be complex, but the way we write about it doesn't have to be.</p> <p>We are writing for a specialist audience. But even specialists want content to be easy to read:</p> <p>when given a choice, 80% of people preferred sentences written in clear English ... the more specialist their knowledge, the greater their preference for plain English</p> <p>It's fine to use technical terminology, but avoid corporate jargon.</p>"},{"location":"guides/style_guide_for_standards/#check-your-writing-for-clarity","title":"Check your writing for clarity","text":"<p>Hemingway is an online tool that helps make writing easier to read.</p> <p>It makes suggestions for when to simplify vocabulary or split up sentences that are too long.</p> <p>You don't have to obey every suggestion from Hemingway, but it can be a useful guide.</p>"},{"location":"guides/style_guide_for_standards/#use-markdown-for-formatting","title":"Use Markdown for formatting","text":"<p>There are a few variations of Markdown, so we use the standard GitHub syntax.</p> <p>Read GitHub's formatting guidance</p> <p>Name each file using snake case (for example, <code>style_guide_for_standards.md</code>).</p>"},{"location":"guides/unmanaged_devices/","title":"Using Unmanaged Devices","text":"<p>A guide for everyone that needs to use unmanaged devices.</p>"},{"location":"guides/unmanaged_devices/#what-is-this-about","title":"What is this about?","text":"<p>This guide is about the appropriate steps you should take when using devices that are not managed by the normal organisational IT controls.</p> <p>That means \"off-network\" laptops used, for example, by developers, testers and designers on digital services.</p>"},{"location":"guides/unmanaged_devices/#does-it-apply-to-you","title":"Does it apply to you?","text":"<p>Most people are issued with devices have been pre-configured with the organisation's standard IT controls in place.</p> <p>For some people, however, these standard devices are too restricted and prevent us from performing our duties.</p> <p>In these cases, you will have been issued with an unmanaged device.</p>"},{"location":"guides/unmanaged_devices/#how-do-i-know-if-my-device-is-unmanaged","title":"How do I know if my device is unmanaged?","text":"<p>You will have been issued with the device by your line manager and informed that it is an unmanaged device.</p> <p>If you have a device and you do not know whether it is managed or unmanaged then ask your line manager.</p>"},{"location":"guides/unmanaged_devices/#what-if-it-is-my-own-device","title":"What if it is my own device?","text":"<p>We do not allow people to use their own devices for official work. Please don't do it.</p> <p>This includes personal smartphones chatting on Slack, personal tablets used to appear.in, etc.  Seriously, please don't.</p> <p>One day we might have thought through the policy for official data on privately-owned equipment sufficiently to make this happen, but we're not there yet.</p>"},{"location":"guides/unmanaged_devices/#how-do-i-know-if-this-guidance-applies-for-my-device","title":"How do I know if this guidance applies for my device?","text":"<p>This guidance applies for all unmanaged devices.</p>"},{"location":"guides/unmanaged_devices/#who-can-you-talk-to-about-this","title":"Who can you talk to about this?","text":"<p>If you have any questions then please contact your line manager.</p> <p>For suggestions feel free to create an Issue or raise a PR with your suggested changes.</p>"},{"location":"guides/unmanaged_devices/#what-is-different-for-unmanaged-devices","title":"What is different for unmanaged devices?","text":"<p>The main difference is that you must apply all the necessary security controls to the device yourself.</p> <p>If you are not comfortable with the full set of responsibilities outlined in this guide then speak to your line manager. It may be best to return the unmanaged device and replace it with a managed device.</p>"},{"location":"guides/unmanaged_devices/#what-are-the-security-controls-you-need-to-apply","title":"What are the security controls you need to apply?","text":"<p>The precise controls that you need to apply will vary depending on the type of device that you have and how you need to use it.</p> <p>However, there are some common controls that you must apply in all cases and these are described first.</p>"},{"location":"guides/unmanaged_devices/#common-controls","title":"Common controls","text":"<p>These controls must be applied whatever type of unmanaged device you use.</p>"},{"location":"guides/unmanaged_devices/#be-aware-of-your-responsibilities","title":"Be aware of your responsibilities","text":"<p>Familiarise yourself with your organisation's information handling responsibilities, acceptable use policies and code of conduct for the use of IT and digital resources.</p> <p>We need to be especially mindful of these as the standard IT security controls often help us to comply with them without us even realising it.</p>"},{"location":"guides/unmanaged_devices/#understand-the-security-principles","title":"Understand the security principles","text":"<p>Read the CESG End User Devices Security Guidance, in particular the Security Principles.</p> <p>This should give you a good understanding of the kind of controls that are required and why they are necessary.</p> <p>If you have any questions then speak to your line manager.</p>"},{"location":"guides/unmanaged_devices/#ensure-your-device-is-registered-on-the-appropriate-inventory","title":"Ensure your device is registered on the appropriate inventory","text":"<p>Your line manager must have a record of your device and it must be recorded in a shared inventory.  In most cases, the device itself will also have an asset identifier on it, usually in the form of a sticker.</p> <p>This ensures that we can keep track of the device and prevent it becoming lost when people move roles.  It also allows to audit equipment for security compliance.</p>"},{"location":"guides/unmanaged_devices/#keep-your-device-patched-and-up-to-date","title":"Keep your device patched and up to date","text":"<p>Whatever device you are using, you will not be receiving updates distributed by your organisation so you will need to apply these yourself.</p> <p>You should normally apply all patches and updates immediately. The only exception to this is where you need to maintain older versions of software to do compatibility testing.</p>"},{"location":"guides/unmanaged_devices/#ensure-that-you-are-using-official-online-resources","title":"Ensure that you are using official online resources","text":"<p>One of the main reasons for having unmanaged devices is to be able to make use of online resources. However, this freedom of access can also be a threat that may cause devices to be compromised or leak official information.</p> <p>For this reason, only officially approved online resources should be accessed from unmanaged devices.</p> <p>In reality, any resource that you access for legitimate work is likely to be \"officially approved\", but you are responsible for all of your online activity and will be accountable for any incidents resulting from inappropriate access.</p> <p>If you are not sure whether an online resource is considered to be officially approved then speak to your line manager.</p>"},{"location":"guides/unmanaged_devices/#use-public-networks-securely","title":"Use public networks securely","text":"<p>You should be aware whenever you are connecting to open, public networks as these can represent an information security risk. In particular, the captive portals typically found in cafes and hotels that require a username and password may not provide secure end-to-end encryption.</p> <p>Whenever you are connecting to a public network, you must ensure that you first evaluate whether it is safe to do so.</p>"},{"location":"guides/unmanaged_devices/#do-not-compromise-the-organisations-corporate-network","title":"Do not compromise the organisation's corporate network","text":"<p>Under normal circumstances you must never connect your device to the organisation's internal corporate network.</p> <p>However, for some work it may be appropriate to use the device on the corporate network and when this has been officially approved then you must ensure that the network is not compromised.</p>"},{"location":"guides/unmanaged_devices/#keep-the-device-physically-secure","title":"Keep the device physically secure","text":"<p>Unmanaged devices are generally portable and often highly valuable, so you must take appropriate steps to keep the device physically secure.</p> <p>In particular, you should:  - keep the device hidden whenever it is being transported or stored  - ensure the device is stored in a secure, locked location  - ensure the device is adequately protected from damage at all times</p> <p>Cases, bags or other containers can be provided by your line manager.</p>"},{"location":"guides/unmanaged_devices/#use-the-device-safely","title":"Use the device safely","text":"<p>Unmanaged devices often have different physical characteristics than standard-issue equipment.</p> <p>You must make sure that you complete an assessment of how to use the device in a way that does not cause strain or long-term injury.</p> <p>Some specific areas to look out for are:  - screen size, graphics resolution and text size may not be optimal in the default configuration  - on-screen or built-in keyboards, touchpads or trackpads are often not suitable for long periods of use  - default operating system or software configuration can often not be suitable for long periods of use  - devices may not have the necessary connectivity options for external equipment</p> <p>Additional equipment or guidance can be provided by your line manager.</p>"},{"location":"guides/unmanaged_devices/#fix-any-faults","title":"Fix any faults","text":"<p>If your device develops any hardware faults it could represent a safety or security issue.</p> <p>These devices are not managed by the normal corporate IT support functions so you should contact your line manager immediately to have any faults fixed.</p> <p>It may be possible to get certain repairs done under warranty, which could involve contacting the support desk or our equipment suppliers, but even so you should still speak to your line manager first.</p>"},{"location":"guides/unmanaged_devices/#backup-and-secure-data","title":"Backup and secure data","text":"<p>Using unmanaged devices often requires the creation of data outside of our secure, corporate repositories.</p> <p>You must ensure that any unique content you create is backed up to alternative persistent storage in accordance with the information management procedures that apply.</p> <p>Additional equipment such as encrypted hard drives or online services such as OneDrive can be provided by your line manager.</p>"},{"location":"guides/unmanaged_devices/#returning-devices","title":"Returning devices","text":"<p>When you return your device, you must ensure that it has been treated to remove all official information and is ready to be used by someone else.</p> <p>This usually just means performing a factory reset, although in some cases additional steps may be required. You must ensure that there are no online accounts linked to the device, such as Microsoft or Apple IDs.</p>"},{"location":"guides/unmanaged_devices/#controls-for-windows-laptops","title":"Controls for Windows laptops","text":""},{"location":"guides/unmanaged_devices/#dont-set-a-bios-password","title":"Don't set a BIOS password","text":"<p>BIOS passwords don't add any greater security to the device and primarily only help to prevent accidental mis-configuration.</p> <p>Setting such passwords can make it more difficult to reuse the device so do not set a BIOS password.</p>"},{"location":"guides/unmanaged_devices/#use-the-latest-stable-desktop-operating-system","title":"Use the latest stable desktop operating system","text":"<p>The device is likely to come with a suitable version of Windows 10 pre-installed, but if not then you should ensure that Windows 10 Professional or Enterprise is installed on it, but do not remove any existing recovery partition.</p> <p>Guidance on this can be provided by your line manager.</p> <p>You must ensure that the device has the latest stable version.</p> <p>Do not use server versions or different operating systems, e.g. any Linux distribution.</p> <p>If you need to work with a different operating system then you must use a virtualisation technology, or you could consider using the Windows Subsystem for Linux.</p>"},{"location":"guides/unmanaged_devices/#keep-the-operating-system-and-applications-patched-and-up-to-date","title":"Keep the operating system and applications patched and up to date","text":"<p>Windows 10 quality updates must be applied automatically and not paused.</p> <p>Feature updates to new versions should be applied as soon as they are available for your device, but may be deferred for a month or two if you have specific software compatibility issues.</p>"},{"location":"guides/unmanaged_devices/#user-accounts","title":"User accounts","text":"<p>Your device should be supplied relatively factory-fresh, without existing user accounts.</p> <p>You need to create a local device account for yourself, not linked to a Microsoft account, to use as a local administrator account. You should use your corporate network username for this account so that it can be unambiguously identified.</p> <p>Secure this account with a password.</p>"},{"location":"guides/unmanaged_devices/#ensure-all-the-built-in-threat-protections-are-enabled","title":"Ensure all the built-in threat protections are enabled","text":"<p>Modern Windows versions have highly capable firewall and anti-malware software, you must ensure that these remain enabled on your device.</p> <p>Very occasionally, it can be necessary to temporarily disable some of the threat protections, for example real-time file scanning can sometimes prevent certain legitimate software from installing properly. However, whenever you do this you must re-enable them once the operation has completed.</p>"},{"location":"guides/unmanaged_devices/#disc-encryption","title":"Disc encryption","text":"<p>You must activate BitLocker disc encryption on your device, with a minimum 8-digit PIN.</p> <p>You must supply the recovery key and disc identifier to your line manager so they can keep a record of them.</p>"},{"location":"guides/unmanaged_devices/#public-networks","title":"Public networks","text":"<p>When you connect to public networks, ensure that you identify them as such to Windows so that it does not share resources from your device.</p>"},{"location":"guides/unmanaged_devices/#virtualisation","title":"Virtualisation","text":"<p>Most development work should be undertaken in a dedicated virtual machine as this helps to protect the integrity of the host device.</p> <p>Commercial virtualisation software can be provided by your line manager if required.</p>"},{"location":"guides/unmanaged_devices/#corporate-resources-access","title":"Corporate resources access","text":"<p>Whilst most corporate resources sit inside entirely private networks, it is possible for some of our online services to be accessed from an unmanaged device. To do this, additional controls can be applied to the device.</p> <p>To allow your device to access the corporate Office 365 services, you will need to enrol it with InTune. This facility can be requested through your line manager.</p> <p>If you enrol your device, you will need to use your corporate account, which will become registered as a new account on the device in addition to the local account you have already created. You will need to secure this account with a PIN.</p>"},{"location":"guides/unmanaged_devices/#cloud-development-environment-access-vpn","title":"Cloud development environment access (VPN)","text":"<p>Dedicated development environments and services can be accessed via VPN. You should install the VPN client on your device according to instructions provided by the operations team.</p> <p>VPN client connections should be made from the host device, not from virtual machines.</p>"},{"location":"guides/unmanaged_devices/#controls-for-macbooks","title":"Controls for Macbooks","text":""},{"location":"guides/unmanaged_devices/#dont-set-a-firmware-password","title":"Don't set a firmware password","text":"<p>Firmware passwords offer minimal additional protection and are easily lost or forgotten, which can only be fixed by an authorised Apple representative.</p>"},{"location":"guides/unmanaged_devices/#use-the-latest-general-release-operating-system-version","title":"Use the latest general release operating system version","text":"<p>It can sometimes take a while for application vendors to support new macOS versions and there's no going back, so you can allow yourself a month or two before moving to the latest macOS operating system. But you should upgrade as soon as it is safe to do so.</p> <p>If you have a device that will not support the latest version of the operating system then you should talk to your line manager to arrange an alternative.</p> <p>You must not install any other operating system such as Windows or a Linux distribution.</p> <p>If you need to work with a different operating system then you must use a virtualisation technology.</p>"},{"location":"guides/unmanaged_devices/#keep-the-operating-system-and-applications-patched-and-up-to-date_1","title":"Keep the operating system and applications patched and up to date","text":"<p>You must ensure that your device is configured to automatically check for updates and you should regularly install macOS and app updates so that you are using current features.</p> <p>Most importantly, you must ensure that your device is configured to automatically install system data files and security updates, which only covers the built-in anti-malware software and its configuration.</p>"},{"location":"guides/unmanaged_devices/#user-accounts_1","title":"User accounts","text":"<p>Your device should be supplied relatively factory-fresh, without existing user accounts.</p> <p>You need to create a local device account for yourself to use as a local administrator account. You should use your corporate network username for this account so that it can be unambiguously identified.</p> <p>Secure this account with a password.</p> <p>You will also need to create an Apple ID registered to your corporate email address if you don't already have one. This account can be used for the App Store and can be linked to your work iPhone if you have one, which is useful for enabling two-factor authentication.</p> <p>However, you should not use this account to synchronise content across devices.</p>"},{"location":"guides/unmanaged_devices/#install-dedicated-anti-malware-software","title":"Install dedicated anti-malware software","text":"<p>Although Gatekeeper provides good protection for applications, it does not provide equivalent controls for other files.</p> <p>You must install a commercial anti-malware product, which can be obtained from your line manager.</p> <p>Very occasionally, it can be necessary to temporarily disable some of the threat protections, for example real-time file scanning can sometimes prevent certain legitimate software from installing properly. However, whenever you do this you must re-enable them once the operation has completed.</p>"},{"location":"guides/unmanaged_devices/#disc-encryption_1","title":"Disc encryption","text":"<p>You must enable FileVault on the disc of your device and create a local recovery key. Do not use your iCloud account or store the recovery key with Apple.</p> <p>You must supply the recovery key and disc identifier to your line manager so they can keep a record of them.</p>"},{"location":"guides/unmanaged_devices/#virtualisation_1","title":"Virtualisation","text":"<p>Most development work will be done in the native operating system, but you may also wish to create virtual machines.</p>"},{"location":"guides/unmanaged_devices/#corporate-resources-access_1","title":"Corporate resources access","text":"<p>Whilst most corporate resources sit inside entirely private networks, it is possible for some of our online services to be accessed from an unmanaged device. To do this, additional controls can be applied to the device.</p> <p>To allow your device to access the corporate Office 365 services, you will need to enrol it with InTune. This facility can be requested through your line manager.</p>"},{"location":"guides/unmanaged_devices/#cloud-development-environment-access-vpn_1","title":"Cloud development environment access (VPN)","text":"<p>Dedicated development environments and services can be accessed via VPN. You should install the VPN client on your device according to instructions provided by the operations team.</p>"},{"location":"guides/version_control_guidance/","title":"Version control guidance","text":"<p>This guidance helps to explain how to apply version control to our code.</p> <p>It is compliant with our version control standards, but provides some additional information on ways to implement version control practice.</p>"},{"location":"guides/version_control_guidance/#teams","title":"Teams","text":"<p>Teams should follow a multi repository approach and should avoid putting everything related to a project in a single repository.</p> <p>For example, put each component (application, package, library, etc) into its own repository, and use GitHub teams to manage access to them.</p> <p>This makes it easy to apply version control to each component individually, whilst also ensuring that access to the repositories is simple to control.</p> <p>A mono repository should only be considered for small scale projects consisting of a single application or package.</p> <p>For additional context behind this standard, see the guidance.</p>"},{"location":"principles/","title":"Principles for software development standards","text":"<p>This folder contains details on principles related to specific areas, and below general principles that underpin all our guidance and standards.</p>"},{"location":"principles/#contents","title":"Contents","text":"<ul> <li>Coding principles</li> <li>Security principles</li> </ul>"},{"location":"principles/#background","title":"Background","text":"<p>There are a number of factors that can influence the decision to adopt a particular development standard. The purpose of these principles is to condense all of those factors into a simple set of statements that can be used to help with those decisions.</p>"},{"location":"principles/#principles","title":"Principles","text":""},{"location":"principles/#dont-re-invent-the-wheel","title":"Don't re-invent the wheel","text":"<p>We adopt standards, guidance and approaches that are already in use in the industry or elsewhere in government</p> <ul> <li>This accords with the general principle of least astonishment</li> <li>It saves us effort</li> </ul>"},{"location":"principles/#do-the-minimum-necessary","title":"Do the minimum necessary","text":"<p>We only develop guidance and standards where there is a rationale for it</p> <ul> <li>This accords with the general principle of keep it simple</li> <li>We don't needlessly constrain work by creating unnecessary standards</li> <li>It saves us effort</li> </ul>"},{"location":"principles/#start-small","title":"Start small","text":"<p>We create a minimum viable product and then iteratively improve it</p> <ul> <li>We quickly see if something works</li> <li>It is easier to respond when things change</li> <li>It prevents us from trying to predict the future</li> <li>It helps us do the minimum necessary</li> </ul>"},{"location":"principles/#be-open","title":"Be open","text":"<p>We work in the open, providing the maximum amount of access possible</p> <ul> <li>More eyes makes things better</li> <li>People can buy-in more easily</li> <li>It helps us do the minimum necessary</li> </ul>"},{"location":"principles/#minimise-work-in-progress","title":"Minimise work in progress","text":"<p>We don't try to do too much at once</p> <ul> <li>It's better to have one thing that can be used than many that can't</li> </ul>"},{"location":"principles/#be-reasonable","title":"Be reasonable","text":"<p>We understand that standards represent a snapshot of our best understanding at a point in time and we explicitly articulate this in all of our standards</p> <ul> <li>This allows us to make sensible, value-based judgements about when it is appropriate to retro-fit standards</li> <li>It ensures clarity by mandating that all standards must define the period of time that they are effective and how they have changed over time</li> <li>It encourages uptake by allowing people to safely adopt the standards that are effective at the time without being committed to any and all possible future changes to those standards</li> <li>It helps people clearly describe and manage their divergence from standards over time and the work required for convergence</li> </ul>"},{"location":"principles/#be-helpful","title":"Be helpful","text":"<p>We include the use of appropriate tools and techniques when we design our standards</p> <ul> <li>Automation makes it easier for people to adopt the standards</li> <li>This makes the standards more likely to be adopted and the benefits more likely to be realised</li> </ul>"},{"location":"principles/coding_principles/","title":"Coding principles","text":"<p>These are common principles that apply to coding.</p>"},{"location":"principles/coding_principles/#rationale","title":"Rationale","text":"<ul> <li>Ensure consistency across coding standards for different languages</li> <li>Provide a reference point to justify coding standards</li> </ul>"},{"location":"principles/coding_principles/#principles","title":"Principles","text":""},{"location":"principles/coding_principles/#master-is-always-shippable","title":"Master is always shippable","text":"<p>The main branch(s) of a project is always in a state of 'shippable to production'</p> <p>We always maintain a golden copy of our code that is considered complete and production ready. It has sufficient tests, matches agreed styles, doesn't break the build, has documentation, and is our best solution at the time of committing.</p> <ul> <li>This allows us to iterate safely ('Start small')</li> <li>It minimises problems during delivery</li> <li>It prevents overload of work due to mixing of production issues and feature development ('Minimise work in progress')</li> </ul>"},{"location":"principles/coding_principles/#the-code-is-not-yours","title":"The code is not yours","text":"<p>We write and commit code thinking of someone else, not ourselves</p> <p>When writing code you should be thinking of the next developer who has to work with it, who may be your future self, another member of the team, or someone completely new.</p> <p>They will not have the understanding and context you do at the time of writing it, so when they're amending or refactoring your work later it is important they can derive the intent of what your code is doing.</p>"},{"location":"principles/coding_principles/#work-in-the-open","title":"Work in the open","text":"<p>Our code is open as early as possible. We only go private if we really, really, really have to!</p> <ul> <li>Encourages good discipline</li> <li>Increased sharing and collaboration</li> <li>It makes things better</li> <li>It improves the overall security of code</li> </ul>"},{"location":"principles/coding_principles/#code-securely","title":"Code securely","text":"<p>We incorporate good security practice in all our code</p>"},{"location":"principles/security_principles/","title":"Security principles","text":"<p>Secure development principles. These are based on ones provided by NCSC</p> <p>For more details visit https://www.ncsc.gov.uk/collection/developers-collection?curPage=/collection/developers-collection/principles</p>"},{"location":"principles/security_principles/#secure-development-is-everyones-concern","title":"Secure development is everyone's concern","text":"<p>Genuine security benefits can only be released when delivery teams weave security into their everyday working practices.</p>"},{"location":"principles/security_principles/#keep-your-security-knowledge-sharp","title":"Keep your security knowledge sharp","text":"<p>Creating code that is capable of withstanding attack requires an understanding of attack types and of defensive security practices.</p>"},{"location":"principles/security_principles/#produce-clean-maintainable-code","title":"Produce clean &amp; maintainable code","text":"<p>If your code lacks consistency, is poorly laid out and undocumented, you're adding to the overall complexity of your system.</p>"},{"location":"principles/security_principles/#secure-your-development-environment","title":"Secure your development environment","text":"<p>There is sometimes a perceived conflict between security and usability. This situation is highlighted in the case of end user devices and the environments used to support software development.</p>"},{"location":"principles/security_principles/#protect-your-code-repository","title":"Protect your code repository","text":"<p>Your code is only as secure as the systems used to create it. As the central point at which your code is stored and managed, it's crucial that the repository is sufficiently secure.</p>"},{"location":"principles/security_principles/#secure-the-build-and-deployment-pipeline","title":"Secure the build and deployment pipeline","text":"<p>Continuous integration, delivery and deployment are modern approaches to the building, testing and deployment of IT systems.</p>"},{"location":"principles/security_principles/#continually-test-your-security","title":"Continually test your security","text":"<p>Security testing can be manual, but it can also be automated.</p>"},{"location":"principles/security_principles/#plan-for-security-flaws","title":"Plan for security flaws","text":"<p>All but the very simplest software is likely to contain bugs, some of which may have a security impact.</p>"},{"location":"processes/","title":"Processes","text":"<p>This folder contains documents which cover any processes related to software development. New starters or those not familiar with our processes may find them useful.</p>"},{"location":"processes/#contents","title":"Contents","text":"<ul> <li>Credential exposure</li> <li>GitHub access</li> <li>Pull requests</li> </ul>"},{"location":"processes/credential_exposure/","title":"Credential exposure","text":"<p>Credentials are things like passwords or API keys. They are any values you wouldn't want exposed to the public but which your app or service needs to do its job.</p> <p>We haven't settled on a specific standard for how we handle them e.g. 12 factor app. But the one thing we are all agreed on is that they should not be committed with your project's source code.</p> <p>Yet mistakes happen and sometimes these secrets become exposed.</p> <p>This process will take you through what to do if that happens. The key point is</p> <p>once out always consider your secret as compromised</p> <p>No matter the speed with which you rectify the situation, or how little you think the chance is for people to see it. Treat it as compromised.</p>"},{"location":"processes/credential_exposure/#contact-operations","title":"Contact operations","text":"<p>The first step is to make contact with the team responsible for your production environment.</p> <p>For example if your service is hosted in AWS, you should call someone from the AWS web-ops team (David Blackburn or Tom Tant).</p> <p>We cannot post contact numbers here. But they are available using our internal systems.</p> <p>Follow this up with an email that contains a link to the commit, and explains what was exposed.</p>"},{"location":"processes/credential_exposure/#fix-your-git","title":"Fix your git","text":"<p>Next is fix your git tree. This involves removing the offending commit from your git history.</p>"},{"location":"processes/credential_exposure/#on-a-branch","title":"On a branch","text":"<p>If the commit was on a branch the fix involves amending or removing the commit. You can use git commands such as <code>git commit --amend</code> or <code>git rebase -i</code> to amend the current branches history.</p> <p>You will then follow this with a <code>git push -f</code>.</p>"},{"location":"processes/credential_exposure/#on-master","title":"On master","text":"<p>If it was the last commit on master, you can get away a <code>git commit --amend</code> and a <code>git push -f</code> if you have the rights.</p> <p>Else you will need to follow a much more involved strategy. GitHub provide a guide. If you are unsure contact the development community or speak to our principal developers.</p>"},{"location":"processes/credential_exposure/#remove-caches","title":"Remove caches","text":"<p>To quote GitHub's guide</p> <p>[..] it's important to note that those commits may still be accessible in any clones or forks of your repository, directly via their SHA-1 hashes in cached views on GitHub, and through any pull requests that reference them.</p> <p>If someone manages to fork or update their fork before you push your fix there is little you can do. Try to reach out to them to explain the situtation, and ask them to update their fork.</p>"},{"location":"processes/credential_exposure/#github","title":"GitHub","text":"<p>You can get the cache of your commit removed by contacting GitHub support. As long as what you need removed meets GitHub's criteria for sensitive data there will not be a problem.</p> <p>The Ruby services team report a turn around of less than an hour when they made this request.</p> <p>Inform the relevant operations team once the request is processed.</p>"},{"location":"processes/github_access/","title":"GitHub access","text":"<p>This guide covers covers all things related to accessing and using GitHub in the context of working on a Defra project.</p>"},{"location":"processes/github_access/#github-account-requirements","title":"GitHub account requirements","text":"<p>Before you can do anything on GitHub you'll need to create an account. If you already have one it is okay to use it, else create a new one specifically for your Defra work if you prefer.</p> <p>Whether you create an account or use an existing one, it must meet the following requirements</p> <ul> <li>Two factor authentication must be enabled</li> <li>Your profile must have the <code>Name</code> field populated with your full name .e.g. Alan Cruikshanks.</li> </ul> <p>You're not required to set a profile picture, but changing it from the default GitHub provides will help your team members distinguish your contributions from theirs.</p>"},{"location":"processes/github_access/#joining-a-defra-organisation","title":"Joining a Defra organisation","text":"<p>All projects at Defra must be created under one of the organisations within the Defra GitHub enterprise:</p> <ul> <li>Defra for software</li> <li>Defra Data Science Centre of Excellence for data science</li> <li>Defra design team for prototype designs</li> </ul> <p>The organisation owners can be contacted to add you to their organisation.</p> <p>Depending on your role and requirements, you may either be added as a member of the organisation or as an outside collaborator on individual repositories.</p>"},{"location":"processes/github_access/#getting-a-new-repository","title":"Getting a new repository","text":"<p>You should contact the organisation owners to create a new repository. They'll need</p> <ul> <li>A name for the repository - this should be consistent with the existing organisational naming conventions</li> <li>Whether the repository is to be private or public - and if it is private the justification for this</li> <li>The GitHub username of the person that will be its administrator</li> <li>(Optional) A description - having a short description helps other people quickly identify what is in your repository</li> </ul> <p>Do not create repositories under your own user account! Though repositories can be transferred at a later date, it is easier for everyone if they originate within our organisations.</p>"},{"location":"processes/github_access/#administering-a-repository","title":"Administering a repository","text":"<p>If you are the administrator for a repository it's your responsibility to ensure the repo has been set up and maintained in accordance with the standards of the organisation.</p> <p>Depending on the organisation settings, you may be able to create and maintain teams to help with administering who has access to your repositories. If not, you may still be able to request the creation of teams from your organisations owners.</p> <p>However you choose to manage it, you must always ensure that your repository's Collaborators and teams setting accurately reflects who should have access to the repository.</p> <p>If you will no longer be the administrator for a repository, you will need to identify a replacement and make them the administrator.</p> <p>Repositories without an administrator will be archived by the organisation owners.</p>"},{"location":"processes/github_access/#access-removal","title":"Access removal","text":"<p>The administrators of our Defra GitHub enterprise will run quarterly reports of dormant users and provide these to the owners of all the organisations in the enterprise.</p> <p>The owners will have a week to respond with the details of any dormant users that should not be removed, but after that time any other accounts will be removed from the enterprise. Organisation owners will need to remove any dormant outside collaborators.</p> <p>This will ensure that we are using our licences effectively and that we are applying appropriate security controls on our information.</p> <p>If you need access again after your account is removed, you will need to ask to join an organisation again as described above.</p> <p>If you think you may be identified as a dormant user, for example if you are taking an extended break from work, then you should let an organisation owner know and they will ensure that your account is not removed.</p>"},{"location":"processes/github_access/#getting-a-new-organisation","title":"Getting a new organisation","text":"<p>If you would like to have a new organisation added into the Defra enterprise you should contact one of the enterprise administrators to discuss your requirements.</p>"},{"location":"processes/github_access/#owning-an-organisation","title":"Owning an organisation","text":"<p>As an organisation owner you are responsible for</p> <ul> <li>setting the standards and processes for access control in the organisation</li> <li>checking whether access is required for dormant users and removing dormant outside collaborators from your organisation</li> <li>ensuring that people that will want to join your organisation know who you are and how to contact you</li> </ul>"},{"location":"processes/pull_requests/","title":"Pull requests","text":"<p>The following outlines how to make changes to a repo. The goals are</p> <ul> <li>Focused PR's related to specific changes</li> <li>Code review is simple and manageable</li> <li>A clean tree of changes in the commit history</li> <li>Consistency across the commits</li> </ul> <p>We also want to respect the time and effort of those reviewing the work. Keeping PR's small, focused and consistent makes their life easier.</p>"},{"location":"processes/pull_requests/#always-on-a-branch","title":"Always on a branch","text":"<p>No matter how small the change, all changes should be done on a branch and never directly to <code>master</code>. This is to support the principle of Master is always shippable</p> <p>Clone the repo then create your new branch. For example <code>git checkout -b add-ea-admin-area-lookup</code>.</p>"},{"location":"processes/pull_requests/#start-with-a-commit","title":"Start with a commit","text":"<p>Start your new branch with an empty commit.</p> <pre><code>git commit --allow-empty\n</code></pre> <p>The template for the commit is the following</p> <pre><code>50 character limited title\n\nLink to originating story/bug/card in relevant system (e.g. Jira or Trello)\n\nDescription covering why we're making this change, and briefly what the change is.\n</code></pre> <p>For example</p> <pre><code>Handle empty params in ValidatePreUpdate method\n\nhttps://eaflood.atlassian.net/browse/WAS-1096\n\nThis change fixes an issue found with `validate_pre_update_organisation_address` in that when passed empty parameters it would cause an `undefined method all? for nil:NilClass` error to be thrown.\n</code></pre> <p>The key point is that it should cover the actual change you are intending to make, and not just repeat what the backlog story outlined. The link to the story is added for those that need that greater context.</p>"},{"location":"processes/pull_requests/#commit-message-rules","title":"Commit message rules","text":"<ol> <li>Separate subject from body with a blank line</li> <li>Limit the subject line to 50 characters</li> <li>Capitalize the subject line</li> <li>Do not end the subject line with a period</li> <li>Use the imperative mood in the subject line (Add ability not Added ability)</li> <li>Use the body to explain what and why vs. how</li> </ol> <p>N.B. The source for these is How to write a Git commit message which you're encouraged to read.</p>"},{"location":"processes/pull_requests/#let-everyone-know","title":"Let everyone know","text":"<p>Next push your branch.</p> <pre><code>git push -u origin add-ea-admin-area-lookup\n</code></pre> <p>We create a PR right from the start so that your proposed change is visible to all. This means everyone can feedback if they see any issues, and give help and advice.</p> <p>Once pushed you'll need to go to GitHub to actually create the PR. Typically it will highlight the new branch in the UI and provide a quick option to create the PR. You'll then see GitHub automatically takes your first empty commit and uses it to populate the pull request title and description. Nice! Assign yourself to the PR so everyone knows the work is with you.</p>"},{"location":"processes/pull_requests/#now-code","title":"Now code","text":"<p>Now get on and code. You should commit frequently and push often. Don't worry too much about your commit messages here. But don't ignore them completely as they will be used as part of the PR process (so keep <code>WIP</code>, <code>Still WIP</code>, and <code>More WIP!</code> to a minimum thank you!)</p>"},{"location":"processes/pull_requests/#keep-up-to-date","title":"Keep up to date","text":"<p>It's on you to keep your branch up to date with with your main development branch e.g. master. Using <code>rebase</code> rather than <code>merge</code> will mean no merge messages appearing in your PR's commit history, but its not required. To <code>rebase</code> on master use</p> <pre><code>git rebase origin/master\n</code></pre>"},{"location":"processes/pull_requests/#get-it-looked-at","title":"Get it looked at","text":"<p>When you're finished and have pushed your last commit request someone to review it. If there are multiple members on your team and all could review, feel free to request them all. The key thing is at least one other person should review the PR before it is merged.</p> <p>You and the reviewer will then work to confirm the changes are OK. Once the reviewer is happy they need to approve it.</p>"},{"location":"processes/pull_requests/#completing-the-pr","title":"Completing the PR","text":"<p>Once approved to complete the PR you'll need to <code>squash</code> your commits down to one.</p> <p>The simplest way to do this is in the GitHub UI. Within the PR the merge button for new repo's will say <code>Squash and merge</code>. Else you may need to click the down arrow next to the button to select it.</p> <p></p> <p>When you click it GitHub will present a box which contains the combined text from all the commit messages in the PR. Use this opportunity to reword the content to a single commit message (the smaller the change the simpler this is to do!)</p> <p>When done ensure you delete the branch. Again GitHub will present this option in the UI immediately after merging so make use of it then.</p>"},{"location":"standards/","title":"Standards","text":"<p>This folder contains all current standards for software development.</p>"},{"location":"standards/#contents","title":"Contents","text":"<ul> <li>Common coding standards</li> <li>C# coding standards</li> <li>Container standards</li> <li>Deployment standards</li> <li>Development languages</li> <li>Java coding standards</li> <li>JavaScript standards</li> <li>Kubernetes standards</li> <li>Mobile application standards</li> <li>Node.js standards</li> <li>PL/SQL coding standards</li> <li>Quality assurance and test standards</li> <li>README standards</li> <li>Ruby coding standards</li> <li>Security standards</li> <li>TSQL and SQL Server database standards</li> <li>UiPath standards</li> <li>Version control standards</li> <li>Kubernetes standards</li> </ul>"},{"location":"standards/common_coding_standards/","title":"Common coding standards","text":"<p>These are common standards that apply to coding.</p> <p>Mostly these follow common programming principles</p>"},{"location":"standards/common_coding_standards/#rationale","title":"Rationale","text":"<ul> <li>Ensure consistency across coding standards for different languages</li> </ul>"},{"location":"standards/common_coding_standards/#standards","title":"Standards","text":""},{"location":"standards/common_coding_standards/#all-code-is-open","title":"All code is open","text":"<p>Whilst we acknowledge that there are cases where it may be necessary for code to not be open, this standard ensures that those cases are managed as exceptions.</p>"},{"location":"standards/common_coding_standards/#all-coding-is-done-on-a-branch","title":"All coding is done on a branch","text":"<p>No code changes are made directly to the master branch.</p> <p>This reflects the 'golden' state of the branch and mirrors the general standard that changes are never made directly to production services.</p>"},{"location":"standards/common_coding_standards/#all-code-is-checked-for-correct-coding-style","title":"All code is checked for correct coding style","text":"<p>Language-dependent</p>"},{"location":"standards/common_coding_standards/#all-branches-have-ci","title":"All branches have CI","text":"<p>Covering code style, security checking, dependency checking, build and unit tests.</p>"},{"location":"standards/common_coding_standards/#unit-test-coverage-is-at-least-90","title":"Unit test coverage is at least 90%","text":"<p>All changes must have sufficient coverage and the overall total coverage must not decrease.</p>"},{"location":"standards/common_coding_standards/#all-code-is-reviewed","title":"All code is reviewed","text":"<p>All code changes are reviewed by another developer before they are merged.</p>"},{"location":"standards/common_coding_standards/#all-code-is-checked-for-readability","title":"All code is checked for readability","text":"<p>There are only two hard things in Computer Science: cache invalidation and naming things.</p> <p>-- Phil Karlton</p> <p>Avoid basic errors such as</p> <ul> <li>Generic variable names: <code>p</code>, <code>temp</code>, <code>v</code></li> <li>Using arrays over hashes: <code>['Bob', 'Smith', 'Bristol']</code> over <code>{ name: 'Bob', surname: 'Smith', location: 'Bristol' }</code></li> <li>Use of acronyms: particularly domain specific ones. Someone new to the service may not know their GOR from their WIMS</li> </ul> <p>Other than that when considering the name for a method, class, module etc try not to approach it from what makes sense to you, but instead what best describes the intent of the thing, and what will help the person who comes after you.</p> <p>Stick to the conventions and naming standards of the language or framework you are using.</p> <p>Aim to write readable code that does not require comments.</p> <p>Use the principle of separate in order to name. Essentially break your code down into small (and if necessary one line) sub routines that have names that express what their intent is. Read Write readable code without comments for more on how to do this.</p> <p>If you have to add comments, make them useful</p> <ul> <li>Comments are code - Keep them small, simple and re-factor them as you make changes</li> <li>Don't document the how - Document why the thing is needed, or why its doing it the way it is</li> <li>Comment in the right place - Talk about the module at the module declaration, the subroutine at the method declaration etc</li> <li>Only write about the code - Don't include names, dates, commit comments or your opinions</li> </ul> <p>Don't include commented out code. It's noise that only confuses. Use the commit history if you need to see what was there before.</p> <p>And most importantly write them as you write the code. If you don't, you probably never will!</p>"},{"location":"standards/common_coding_standards/#all-code-is-appropriately-documented","title":"All code is appropriately documented","text":"<p>Document the how for the project</p> <p>When it comes to other developers or users of the project, the thing they are most interested in is the how. How can I build/deploy/run this project. How can I use it in my own work.</p>"},{"location":"standards/common_coding_standards/#all-code-is-checked-for-simplicity","title":"All code is checked for simplicity","text":"<p>We don't code for reuse until we have a confirmed need for reuse.</p> <p>We apply the rule of three to help decide when to abstract code.</p> <p>We don't break up our applications until there is a confirmed need to do so.</p> <p>The path to reuse starts with module/namespacing. When the need for reuse is confirmed we first build a package. Only if there is demand from multiple services, and the benefits for a deployed service over a shared package are clear do we build a new application.</p>"},{"location":"standards/common_coding_standards/#all-code-is-checked-for-quality","title":"All code is checked for quality","text":"<p>All repos are connected to a quality analysis tool and the tool's maximum quality rating is maintained.</p> <p>All repos on GitHub and Azure Repos are connected to our SonarCloud organisation and the Defra standard quality gate is met.</p>"},{"location":"standards/common_coding_standards/#all-code-is-checked-for-security","title":"All code is checked for security","text":"<p>All repos are connected to a security tool to monitor for vulnerabilities in the dependencies and static code vulnerabilities.</p>"},{"location":"standards/common_coding_standards/#all-code-has-dependency-checking","title":"All code has dependency checking","text":"<p>No out of date dependencies or libraries with known vulnerabilities.</p>"},{"location":"standards/common_coding_standards/#status","title":"Status","text":"<p>This standard was formally adopted on 1 July 2019.</p>"},{"location":"standards/common_coding_standards/#significant-changes","title":"Significant changes","text":"<p>SonarCloud was adopted as our standard quality analysis tool on 1 April 2020.</p>"},{"location":"standards/container_standards/","title":"Container standards","text":""},{"location":"standards/container_standards/#terminology","title":"Terminology","text":"<p><code>Dockerfile</code> - set of instructions for building a docker image <code>Image</code> - a constructed set of layered docker instructions <code>Container</code> - a running instance of an image</p>"},{"location":"standards/container_standards/#standards","title":"Standards","text":""},{"location":"standards/container_standards/#defra-base-images","title":"Defra base images","text":"<p>Images are extended from a minimal Defra created parent image.  This allow us to benefit from improved security and more efficient builds as we will not have to repeat steps that are common to all Dockerfiles.</p> <p>Different parent images should be created in line with each framework's best practice.  Eg, Node.js, .Net Core, Ruby etc.</p> <p>Defra's Dockerfiles to build parent images can be found in the below GitHub repositories: - Node.js - .Net Core</p> <p>Each image is hosted in DockerHub and has an equivalent development and production image.  </p> <ul> <li>Node.js</li> <li>Node.js development</li> <li>.Net Core</li> <li>.Net Core development</li> </ul>"},{"location":"standards/container_standards/#images-are-created-using-docker","title":"Images are created using Docker","text":"<p>Docker is the de facto industry standard in most enterprises.</p>"},{"location":"standards/container_standards/#docker-compose-is-used-for-defining-container-builds","title":"Docker Compose is used for defining container builds","text":"<p>Using a consistent containerisation tool will help enforce standards and better promote developer mobility.</p>"},{"location":"standards/container_standards/#linux-containers-are-used-if-the-service-to-be-containerised-can-run-on-linux","title":"Linux containers are used if the service to be containerised can run on Linux","text":"<p>Underlying container hosts can only host either Linux or Windows containers.  A model of consistency would reduce the number of underlying hosts.  Windows containers can only be run on Windows hosts which typically have greater licensing costs vs Linux.</p> <p>Note that services built using .Net Framework cannot be run using Linux containers, but services built using .Net Core can.  Defra's position is .Net Core should be preferred over .Net Framework but there may be some use cases if containerising legacy services</p>"},{"location":"standards/container_standards/#images-are-built-using-defra-base-images-see-above","title":"Images are built using Defra base images (see above)","text":""},{"location":"standards/container_standards/#containers-are-not-run-using-root-user","title":"Containers are not run using root user","text":"<p>Containers are not trust boundaries and therefore should never be run as root for security reasons.</p>"},{"location":"standards/container_standards/#public-images-are-signed","title":"Public images are signed","text":"<p>A digital fingerprint should be added to each image.  This enables consumers of images to verify the source and trust levels of the image they are consuming.</p>"},{"location":"standards/container_standards/#images-are-tagged-using-semantic-versioning","title":"Images are tagged using semantic versioning","text":"<p>Version control standards </p> <p>For simplicity, it is beneficial to keep the image and containerised application version in sync.</p>"},{"location":"standards/container_standards/#production-images-should-be-immutable-and-once-built-do-not-change-but-can-be-configured","title":"Production images should be immutable and once built do not change but can be configured","text":"<p>Immutable images do not change once created which gives the a versioned image predictability and portability.</p>"},{"location":"standards/container_standards/#images-are-self-contained-and-carry-all-runtime-dependencies","title":"Images are self contained and carry all runtime dependencies","text":"<p>Images should not be dependent on their host infrastructure for any application dependency.  This enables them to be freely deployed and orchestrated anywhere capable of running containers.</p>"},{"location":"standards/csharp_coding_standards/","title":"C# coding standards","text":"<p>These are common standards that apply to coding when using C# and are designed to be used in conjunction with the overall common coding standards.</p>"},{"location":"standards/csharp_coding_standards/#rationale","title":"Rationale","text":"<ul> <li>Ensure consistent C# code and styling across all projects</li> </ul>"},{"location":"standards/csharp_coding_standards/#resource","title":"Resource","text":"<p>Microsoft's standard conventions</p>"},{"location":"standards/csharp_coding_standards/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>PascalCasing for class names and method names</li> </ul> <pre><code>public class ClientActivity\n{\n    public void ClearStatistics()\n    {\n        // ...\n    }\n\n    public void CalculateStatistics()\n    {\n        // ...\n    }\n}\n</code></pre> <ul> <li>camelCasing for method arguments and local variables</li> </ul> <pre><code>public class UserLog\n{\n    public void Add(LogEvent logEvent)\n    {\n        int itemCount = logEvent.Items.Count;\n        // ...\n    }\n}\n</code></pre> <ul> <li>do not use underscores in identifiers (you can prefix private static variables with an underscore)</li> <li>use predefined type names instead of system type names for example</li> </ul> <pre><code>// Correct\nstring firstName;\nint lastIndex;\n\n// Avoid\nString firstName;\nInt32 lastIndex;\n</code></pre> <ul> <li>use noun or noun phrases</li> </ul> <pre><code>public class Employee\n{\n\n}\n\npublic class BusinessLocation\n{\n\n}\n</code></pre> <ul> <li>prefix interfaces with the letter <code>I</code>.</li> </ul> <pre><code>public interface IShape\n{\n\n}\n</code></pre>"},{"location":"standards/csharp_coding_standards/#layout-conventions","title":"Layout Conventions","text":"<p>CodeMaid is a free open source Visual Studio Extension which will cleanup and simplify your C# code. It can be downloaded as an extension to visual studio and can run automatically either on save or on demand on either a selection of code or a whole solution.</p> <ol> <li>Remove unused using statements</li> <li>Sort using statements</li> <li>Add unspecified access modifiers</li> <li>Remove empty regions</li> <li>Add blank line padding</li> <li>Remove blank lines next to braces</li> <li>Run Visual Studio formatting</li> <li>Remove consecutive blank lines</li> <li>Remove end of line whitespace</li> <li>Update endregion tags</li> </ol> <p>The default settings for CodeMaid should remain in place to adhere to DEFRA principles (details at http://www.codemaid.net/documentation/)</p>"},{"location":"standards/csharp_coding_standards/#testing","title":"Testing","text":"<p>As per the general coding standards it is expected that 90% of code is covered with unit tests however given unit test coverage in some C# projects (i.e. MVC) will include coverage of default Microsoft template code this 90% figure can be misleading. It is not expected that unit tests should be written to cover code already written by Microsoft and therefore attributes can be added to this code to exclude them from the code coverage calculation.</p> <p>For example in a typical MVC application exclusions can be applied to areas such as anything in <code>App_Start</code> folder, any migrations, <code>global.asax</code>, etc.</p> <p>To exclude test code from the code coverage results and only include application code, add the <code>ExcludeFromCodeCoverageAttribute</code> attribute to your test class.</p> <p>To customize code coverage, follow these steps:</p> <ol> <li>Add a run settings file to your solution. In Solution Explorer, on the shortcut menu of your solution, choose Add &gt; New Item, and select XML File. Save the file with a name such as <code>CodeCoverage.runsettings</code>.</li> <li>Add the content from the example file, and then customize it to your needs as described in the sections that follow.</li> <li>To select the run settings file, on the Test menu, choose Test Settings &gt; Select Test Settings File. To specify a run settings file for running tests from the command line or in a build workflow, see Configure unit tests by using a <code>.runsettings</code> file.</li> </ol> <p>When you select Analyze Code Coverage, the configuration information is read from the run settings file.</p> <p>To turn the custom settings off and on, deselect or select the file in the Test &gt; Test Settings menu.</p> <p>https://docs.microsoft.com/en-us/visualstudio/test/customizing-code-coverage-analysis?view=vs-2017</p>"},{"location":"standards/deployment_standards/","title":"Deployment standards","text":""},{"location":"standards/deployment_standards/#we-use-docker-containers-for-delivery-of-all-bespoke-software-being-deployed-to-aws-and-azure","title":"We use Docker containers for delivery of all bespoke software being deployed to AWS and Azure","text":"<p>Where we develop bespoke software, we default to using a Docker container as the deployable item.</p> <p>Unless the technology does not support containerisation, all deviations from this standard will be treated as architectural exceptions.</p>"},{"location":"standards/deployment_standards/#rationale","title":"Rationale","text":"<p>Containerisation is most applicable to certain specific scenarios, such as highly-available applications with varying demand or when using technologies that have a frequent update cycle.</p> <p>However, running and managing a delivery pipeline and operational environment that supports both containers and more traditional virtual machines results in a duplication of effort and technical solutions.</p> <p>Therefore, we will seek to minimise or eliminate this duplication by always preferring containerisation.</p> <p>Containerisation also allows us to better manage a diverse application estate as it somewhat mitigates the need for across the board standardisation and large, coordinated upgrade projects, allowing instead for more nuanced decision-making.</p>"},{"location":"standards/deployment_standards/#we-deploy-code-rather-than-containers-to-aws-lambda-and-azure-functions-where-their-use-is-formally-justified","title":"We deploy code rather than containers to AWS Lambda and Azure Functions where their use is formally justified","text":"<p>There are cases where it will make sense to use a serverless platform rather than a containerised solution, but these must be individually justified and recorded as formal architectural decisions.</p> <p>Additionally, justifications for using serverless platforms can only be considered for Javascript or C# functions.</p>"},{"location":"standards/deployment_standards/#rationale_1","title":"Rationale","text":"<p>Serverless platforms provide a greatly reduced operational burden, but are not so well represented in development and have certain limitations that can make their use inappropriate. They also represent some vendor lock-in that, whilst minimal, is still not preferred.</p>"},{"location":"standards/deployment_standards/#we-regularly-review-our-use-of-serverless-platforms","title":"We regularly review our use of serverless platforms","text":"<p>In particular, we will ensure that: - Vendor lock-in doesn't increase - We identify opportunities to rationalise these standards to use, effectively, \"serverless containers\"</p>"},{"location":"standards/deployment_standards/#rationale_2","title":"Rationale","text":"<p>As serverless platforms are still relatively immature, we will need to ensure that our standards are still appropriate.</p>"},{"location":"standards/development_language_standards/","title":"Development languages","text":"<p>This standard defines the development languages we use.</p> <p>The standard has come from delivering digital services that tend to follow this pattern: - A bespoke online digital service that is specifically built to meet well-researched user needs - One or more common Defra core IT systems that support our business - Pursuing a Cloud first agenda and giving equal consideration to open source software</p>"},{"location":"standards/development_language_standards/#we-use-more-than-one-development-language","title":"We use more than one development language","text":"<p>We recognise the value of choosing the right tool for the job so we do not specify one single development language for everything we deliver.</p> <p>The advantages of this approach are: - it increases our ability to rapidly respond to changing business needs - we can more easily adopt new developments in the IT industry - vendor and product selection is less constrained - it strengthens our supplier management capability - reduced development cost - better opportunities for re-use of external code, libraries, products and best practice - happier developers!</p> <p>There are some disadvantages, such as the following, but these are out-weighed by the advantages: - staff recruitment, retention and work allocation is more difficult - there are fewer opportunities to share internal code and best practice - it can be difficult to maintain enough capability with only a small number of developers - it adds extra complexity to technology choices</p>"},{"location":"standards/development_language_standards/#we-build-on-open-platforms-whenever-possible","title":"We build on open platforms whenever possible","text":"<p>The government Technology Code of Practice states that you must: - Be open and use open source - Make use of open standards</p> <p>The simplest way to achieve this is by using open platforms by default.</p>"},{"location":"standards/development_language_standards/#our-primary-development-platforms-are-microsofts-net-and-nodejs","title":"Our primary development platforms are Microsoft's .NET and Node.js","text":"<p>There are two key areas within Defra where we need to write custom code.</p> <p>We build bespoke online digital services using standard government libraries and frameworks. For these we have identified Node.js as the most suitable platform.</p> <p>We also build solutions on top of commercial software products, where we have to use the platform that the vendor provides. Many of those products are from Microsoft, which means that we use .NET for this.</p> <p>These two platforms provide a wide range of capabilities so we can use them for most of the applications we develop.  We benefit from the flexibility of having two distinct platforms to choose from but also limit the diversity to a level that is appropriate for a medium-sized team of developers</p> <p>However, these two technologies alone will not always be the solution to every problem and we also have a range of existing applications within Defra that are built using other platforms. So, we still need to maintain some limited capability in many other technologies such as Java, Python, R, Ruby and VBA.</p>"},{"location":"standards/development_language_standards/#we-develop-online-digital-services-in-nodejs","title":"We develop online digital services in Node.js","text":"<p>The world of online digital development is dominated by open source technologies so there are many freely available frameworks, libraries and tools to support it.</p> <p>These kind of services change rapidly, which has led to the rise of rapid development frameworks and a preference for dynamic languages.</p> <p>Building for the Web means that developers already need a knowledge of Javascript, so using Node.js eases the burden on developers to maintain capability in multiple languages. Also, Node.js has proven to deliver good application performance in most Web-facing scenarios.</p> <p>This can be seen across government as well, in particular the GDS front end community tend to publish their products so that they can be easily used in Ruby and Node.js, in particular.</p>"},{"location":"standards/development_language_standards/#we-use-the-hapi-framework-to-develop-in-nodejs","title":"We use the Hapi framework to develop in Node.js","text":"<p>Hapi is already in use in Defra digital services and has provided a productive level of standardisation across development teams and has proven to be robust and reliable.</p>"},{"location":"standards/development_language_standards/#we-minimise-the-customisation-inside-commodity-platforms","title":"We minimise the customisation inside commodity platforms","text":"<p>Our enterprise architecture is based around commodity platforms, but these often require customisation to meet our needs.</p> <p>This customisation leads to a maintainability burden, so we prefer to deliver custom capability outside of the platform itself. To do this, we build bespoke functionality but provide a mechanism to include that functionality in the user interface of the product. Most of the products we use have some way of doing this built in.</p> <p>Taking this approach means that we decouple the bespoke capability from the core product, making it easier to perform changes and upgrades, but we also provide a coherent user interface.</p>"},{"location":"standards/development_language_standards/#we-use-net-to-customise-and-extend-commodity-products","title":"We use .NET to customise and extend commodity products","text":"<p>Many of our commodity products come from Microsoft and so provide a .NET customisation platform.</p> <p>In addition, they will often provide highly capable .NET libraries and SDKs that enable integration with the products, including add-in functionality such as SharePoint Provider Hosted Add-Ins.</p>"},{"location":"standards/development_language_standards/#we-use-c-as-our-net-development-language","title":"We use C# as our .NET development language","text":"<p>C# is the de facto standard language for .NET development and owing to its similarity to Java provides a level of familiarity and cross-fertilisation with other language communities.</p> <p>It is also the language that is best supported in open source .NET managed frameworks.</p>"},{"location":"standards/development_language_standards/#we-use-net-core-wherever-practical-in-preference-to-net-framework","title":"We use .NET Core wherever practical in preference to .NET Framework","text":"<p>.NET Core is Microsoft's strategic direction for .NET and gives all the benefits of being open source and cross platform, including growing community support.</p>"},{"location":"standards/java_coding_standards/","title":"Java Coding Standards","text":"<p>These standards are a slightly updated version of the standards used in practice for many years in the Environment Agency during the period when Java was defined as the strategic programming language. As such they are comprehensive, mature and follow standard industry practice.</p> <p>For users of Eclipse we have guidance on how to auto-format your code that applies the standards described below.</p>"},{"location":"standards/java_coding_standards/#naming-structure-and-declarations","title":"Naming, Structure and Declarations","text":"<ul> <li>English descriptors should be used for all variables and classes such that they define the purpose of the object they describe</li> </ul> <p>The only allowable exceptions to this rule are firstly where local variables are used as iteration counters where <code>i</code> is the accepted standard. Where multiple counters are required they should start alphabetically at <code>i</code> then <code>j</code> etc. Secondly exceptions may be abbreviated for example in the catch clause of a programmatic block an exception may be called <code>e</code>.</p> <ul> <li>All variables should start with a lower case letter and the initial of any non-leading word should be capitalised</li> </ul> <p>For example <code>fishingLicense</code>, <code>dataElement</code>. This rule should ensure that variable names do not differ only in case. One exception to this rule is where a name includes a word which is commonly capitalised such as an acronym. An example of this would be <code>userID</code> or <code>serialUID</code>.</p> <ul> <li> <p>Do not use leading or trailing underscores</p> </li> <li> <p>Local variables should not share the same name as a variable with greater scope</p> </li> </ul> <p>The only exception to this is in a setter, where the passed in parameter name should match the name of the instance variable.</p> <ul> <li>Class and interface names should begin with a capital letter and the initial of any non-leading word should be capitalised</li> </ul> <p>For example <code>FishingLicenceComparator</code>, <code>ValueObjectEngineer</code>.</p> <ul> <li>Static final variables should be named with capitalised names and should have words separated by an underscore</li> </ul> <pre><code>SALMON_LICENCE_TYPE\n</code></pre> <ul> <li>Arrays should be declared using the bracket notation after the datatype not the variable</li> </ul> <pre><code>byte[] fileBytes\n</code></pre> <ul> <li> <p>Where methods are overloaded they should appear next to each other in the class</p> </li> <li> <p>Objects should be initialised to <code>null</code> at declaration time</p> </li> <li> <p>Accessor and Mutator methods (gets and sets) should follow the Javabeans naming conventions</p> </li> </ul> <p>For example the variable <code>licenceCount</code> would be accessed by the method <code>getLicenceCount()</code>.</p> <ul> <li> <p>All classes and interfaces should belong to an explicit package</p> </li> <li> <p>All packages should be prefixed by <code>uk.gov.defra</code>. Package identifiers should all be singular i.e. should not contain plurals</p> </li> <li> <p>Packages should not be imported in their entirety by using the wildcard notation</p> </li> </ul> <p>If using an IDE such as Eclipse this, plus removing any unwanted imports, ought to be the default behaviour of the IDE.</p> <ul> <li>Where possible methods and variables should be referenced without using full name qualification</li> </ul> <p>For example reference <code>IOException</code> rather than <code>java.util.IOException</code>.</p> <ul> <li>Constructors should appear as the first methods in any class, followed by the finalize() method if overridden, followed by static methods and then any other methods and finally any inner classes</li> </ul>"},{"location":"standards/java_coding_standards/#statements-and-expressions","title":"Statements and Expressions","text":"<ul> <li> <p>A single line should not contain multiple statements</p> </li> <li> <p>When using unary operators no space should be left between the variable and the operator</p> </li> </ul> <pre><code>Licences++;\n</code></pre> <ul> <li>For readability, equality operator must always be wrapped in spaces</li> </ul> <pre><code>licenceType == 2\n</code></pre> <ul> <li>For readability assignment operators must be wrapped in spaces</li> </ul> <pre><code>LicenceType = 3;\n</code></pre> <ul> <li>For readability, arrow (lambda) operators must be wrapped in spaces</li> </ul> <pre><code>interface NumericTest\n{\n  boolean computeTest(int n);\n}\n\nNumericTest isNegative = (n) -&gt; (n &lt; 0);\n</code></pre> <ul> <li>Logic and binary operators should be wrapped in spaces</li> </ul> <pre><code>payAward += 4;\n\n(payAward &gt;= 1000);\n</code></pre> <ul> <li> <p>The semicolon closing a statement should immediately follow the statement and should not be preceded by a space</p> </li> <li> <p>A single blank line should appear between a method end and the next method's leading comments</p> </li> <li> <p>Indents should be 2 characters wide</p> </li> <li> <p>Opening and closing braces should be aligned and not appear on the same line</p> </li> </ul> <pre><code>try\n{\n  // some actions\n}\ncatch (Exception e)\n{\n  // handle exception\n}\n</code></pre> <ul> <li>Lambda expressions should ideally be single line, in which case the entire expression should wherever possible be kept to the same line (i.e. unless forced to wrap as the line exceeds the maximum specified line length).</li> </ul> <p>For example, assuming this functional interface:</p> <pre><code>interface NumericTest\n{\n  boolean computeTest(int n);\n}\n</code></pre> <p>A correctly formatted lambda expression could look like:</p> <pre><code>NumericTest isNegative = (n) -&gt; (n &lt; 0);\n</code></pre> <ul> <li>Where a lambda expression spans many lines, the braces surrounding those lines should follow the same rules as all other braces</li> </ul> <p>For example, assuming this functional interface:</p> <pre><code>interface NumericTest\n{\n  boolean computeTest(int n);\n}\n</code></pre> <p>A correctly formatted lambda expression could look like:</p> <pre><code>NumericTest isNotEven = (n) -&gt;\n{\n  n++;\n  return n % 2 == 0;\n};\n</code></pre>"},{"location":"standards/java_coding_standards/#loops-and-conditionals","title":"Loops and Conditionals","text":"<ul> <li>Loops and conditionals should be blocked out in the following manner unless the content is a single line statement</li> </ul> <pre><code>for (int i=0; i&lt;maxValue; i++)\n{\n  // loop body goes in here\n}\n\nif (licenceType == 1)\n{\n  // conditional body goes in here\n}\n\nswitch (licenceCode)\n{\n  case F172:\n    // do actions\n    break;\n\n  default:\n    // do actions\n    break;\n}\n</code></pre> <p>For single line statements either the above approach or the following is permissible.</p> <pre><code>for (i = 0; i &lt;= 12; i++)\n\nj = j + i;\n</code></pre> <p>The important thing is that the content of the loop is clearly distinguished, either by the braces or, in the case of a single line and only a single line, that line being separately indented.</p> <ul> <li>Indentation should be used to indicate enclosed statements in loops and conditionals</li> </ul>"},{"location":"standards/java_coding_standards/#class-interface-and-method-declaration","title":"Class, Interface and Method Declaration","text":"<ul> <li>Classes and interfaces should be declared, if possible, on a single line. Where this is not possible indentation should be used to aid the reading of the class declaration as follows</li> </ul> <pre><code>public class MySpecialisedClass extends MyGenericClass\n implements Serializable\n{\n  // class body goes in here\n}\n</code></pre> <ul> <li>Methods should be declared, if possible, on a single line</li> </ul> <p>Where this is not possible indentation should be used to aid the reading of the method declaration as follows.</p> <pre><code>public myIOMethod(String param1, Integer param2) throws IOException,\n  FileNotFoundException\n{\n  // method body goes in here\n}\n</code></pre>"},{"location":"standards/java_coding_standards/#commenting-and-documentation","title":"Commenting and Documentation","text":"<p>See the Common coding standards for the general standards and guidance on commenting code.</p>"},{"location":"standards/java_coding_standards/#javadoc","title":"JavaDoc","text":"<ul> <li>All classes and interfaces should include JavaDocs comments as a header</li> </ul> <p>The comments should explain in plain English what the purpose of the class or interface is, where that purpose is not obvious from the name. The JavaDoc header should be placed below the package name and import list and above the class definition.</p> <ul> <li>The following header format should be used</li> </ul> <pre><code>/**\n*  Original Author: @author\n*/\n\n/**\n*     Brief description of what MyClass does.\n*\n*/\n</code></pre> <p>Note that any information about the class history, beyond the identity of the original author, is not included here, as such information should be included in the version control system. Attempting to maintain such information in the class comments themselves is not only duplicate effort but is likely to result in the two sources of information becoming confusingly inconsistent.</p> <ul> <li>All methods should be preceded by JavaDoc comments</li> </ul> <p>Comments must state in plain English what the purpose of the method is, what effect it has on any referenced objects and any pre or post conditions that apply to the method's use. The comment should also indicate if the method overrides an inherited method. The javadoc should include entries for <code>@param</code>, <code>@return</code> and <code>@exception</code>, wherever those are appropriate.</p> <pre><code>/** Method To return a count of the outstations associated with the location\n*    @param locID, the locationID to search on\n*    @return Integer, number of outstations, null if none are found\n*    @exception DataAccessException, thrown when outstations can't be counted\n*/\n</code></pre> <p>Other javadocs tags can be used as required.</p>"},{"location":"standards/java_coding_standards/#other-comments","title":"Other Comments","text":"<ul> <li> <p>Both multi-line (<code>\\* */</code>) and single line (<code>//</code>) comments may be used</p> </li> <li> <p>Comments within the method body should, wherever possible, be restricted to single line comments</p> </li> </ul>"},{"location":"standards/java_coding_standards/#status","title":"Status","text":"<p>This standard was formally adopted on 20 Feb 2021.</p>"},{"location":"standards/javascript_standards/","title":"Javascript standards","text":""},{"location":"standards/javascript_standards/#javascript-standards","title":"JavaScript standards","text":""},{"location":"standards/javascript_standards/#use-standard-js-to-lint-your-code","title":"Use Standard JS to lint your code","text":"<ul> <li>A consistent approach to code layout, spacing and formatting makes it easier to switch between projects.</li> </ul> <p>By adding it as a dev dependency it can be easily used in the terminal, Webstorm, and Visual Studio.</p>"},{"location":"standards/kubernetes_standards/","title":"Kubernetes standards","text":""},{"location":"standards/kubernetes_standards/#standards","title":"Standards","text":""},{"location":"standards/kubernetes_standards/#use-a-managed-kubernetes-service","title":"Use a managed Kubernetes service","text":"<p>Managed Kubernetes services such as Azure Kubernetes Service (AKS) in Azure or Elastic Kubernetes Service (EKS) in AWS are used as opposed to any IaaS Kubernetes implementation.</p> <p>This is because managed Kubernetes services abstract the maintenence and configuration of master nodes to the cloud provider, meaning teams only need to support the worker nodes where services run.  Maintaining a full Kubernetes cluster can be very complicated and requires a high level of in depth Kubernetes and networking knowledge which can be a barrier to entry for some teams.  Using a managed service significantly reduces this complexity.</p>"},{"location":"standards/kubernetes_standards/#use-helm-for-packaging-deployments","title":"Use Helm for packaging deployments","text":"<p>Helm is a tool to bundle individual Kubernetes configuration files into single deployable packages.  This significantly reduces the complexity of configuring a cluster and deploying applications to it.</p> <p>Helm 3 must be used and teams should never use Helm 2 due to security risk introduced by running Tiller in a cluster.</p> <p>Teams are also encouraged to use a Helm Library chart to reduce duplication of Helm charts across multiple services.  Such as this one used by the Future Farming and Countryside Programme (FFC).</p>"},{"location":"standards/kubernetes_standards/#use-configmaps-for-configuration","title":"Use ConfigMaps for configuration","text":"<p>Configuration for an application running in a pod should be passed to the pod via a <code>ConfigMap</code> Kubernetes resource.  The <code>ConfigMap</code> is more flexible than just using environment variables alone, and as well as supporting file based values, allows decoupling of pod definitions from configuration definitions.</p> <p>Environment specific values should be overriden during the Helm deployment.</p> <p>Sensitive values should never be passed to a <code>ConfigMap</code>.</p>"},{"location":"standards/kubernetes_standards/#secrets","title":"Secrets","text":"<p>Where possible, secrets should not be stored within an application or Kubernetes pod.  Instead, when communicating with supported cloud infrastructure, clusters should use AAD Pod Identity in Azure or IAM role for Service Accounts in AWS.</p> <p>When secrets in a pod are unavoidable, for example when a third party API key is needed, secrets should be injected into pods during deployment.</p> <p>Note the precise mechanism for managing this is still being reviewed with in a collaboration between Cloud Technology Working Group (CTWG), Future Farming and Countryside Programme (FFC) and Europe and Trade Delivery Portfolio (EuTDP). </p> <p>This document will be updated when an agreement is reached.</p> <p>The Kubernetes <code>Secrets</code> resource type must not be used as data is only Base64 encrypted.</p>"},{"location":"standards/kubernetes_standards/#labels","title":"Labels","text":"<p>Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system. </p> <p>Labels can be used to organise and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time. Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p> <p>In order to take full advantage of using labels, they should be applied on every resource object within a Helm chart. i.e. all deployments, services, ingresses etc.</p>"},{"location":"standards/kubernetes_standards/#required-labels","title":"Required labels","text":"<p>Each Helm chart templated resource should have the below labels. Example placeholders are provided for values.</p> <pre><code>metadata:\n  labels:\n    app: {{ quote .Values.namespace }}\n    app.kubernetes.io/name: {{ quote .Values.name }}\n    app.kubernetes.io/instance: {{ .Release.Name }}\n    app.kubernetes.io/version: {{ quote .Values.labels.version }}\n    app.kubernetes.io/component: {{ quote .Values.labels.component }}\n    app.kubernetes.io/part-of: {{ quote .Values.namespace }}\n    app.kubernetes.io/managed-by: {{ .Release.Service }}\n    helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }}\n    environment: {{ quote .Values.environment }}\n</code></pre> <p>Note <code>Deployment</code> resource objects should have two sets of labels, one for the actual deployment and another for the pod template the deployment manages.</p>"},{"location":"standards/kubernetes_standards/#selectors","title":"Selectors","text":"<p>Services selectors should be matched by app and name. Selectors should be consistent otherwise updates to Helm charts will be rejected.</p> <pre><code>selector:\n  app: {{ quote .Values.name }}\n  app.kubernetes.io/name: {{ quote .Values.name }}\n</code></pre>"},{"location":"standards/kubernetes_standards/#resource-usage","title":"Resource usage","text":"<p>Predictable demands are important to help Kubernetes find the right place for a pod within a cluster. When it comes to resources such as a CPU and memory, understanding the resource needs of a pod will help Kubernetes allocate the pod to the most appropriate node and generally improve the stability of the cluster.</p>"},{"location":"standards/kubernetes_standards/#declaring-a-profile","title":"Declaring a profile","text":"<ul> <li>all pods declare both a <code>request</code> and <code>limit</code> value for CPU and memory</li> <li>production clusters do not contain any <code>best-effort</code> pods</li> <li>pods with consistent usage patterns are run as<code>guaranteed</code> pods (i.e. equal <code>request</code> and <code>limit values</code>)</li> <li>pods with spiky usage patterns can be run as <code>burstable</code> but effort should be taken to understand why performance is not consistent and whether the service is doing too much</li> </ul>"},{"location":"standards/kubernetes_standards/#resource-profiling","title":"Resource profiling","text":"<p>Performance testing a pod is the only way to understand it's resource utilisation pattern and needs. Performance testing should take place on all pods to accurately understand their usage before they can be deployed to production.</p>"},{"location":"standards/kubernetes_standards/#resource-quotas","title":"Resource quotas","text":"<p>Clusters will limit available resources within a namespace using a <code>resourceQuota</code> to improve cluster stability. </p> <p>An example <code>ResourceQuota</code> definition is included below</p> <pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: my-resource-quota\nspec:\n  hard:\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n</code></pre>"},{"location":"standards/kubernetes_standards/#pod-priority","title":"Pod priority","text":"<p>Kubernetes Pods can have priority levels. Priority indicates the importance of a pod relative to other pods. If a pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority pods to make scheduling of the pending pod possible.</p> <p>In the event of over utilisation of a cluster, Kubernetes will start to kill lower priorty pods first to maintain stability.</p> <p>Clusters should include pod priority classes that teams can consume based on their service needs.  The below gives examples of the Pod Priorty classes available in FFC clusters.</p>"},{"location":"standards/kubernetes_standards/#high-1000","title":"High (1000)","text":"<p>Reserved primarily for customer facing or critical workload pods.</p>"},{"location":"standards/kubernetes_standards/#default-600","title":"Default (600)","text":"<p>Default option suitable for most pods.</p>"},{"location":"standards/kubernetes_standards/#low-200","title":"Low (200)","text":"<p>For pods where downtime is more tolerable.</p> <p>Below is a full example of a <code>priorityClass</code> resource definition for reference.</p> <pre><code>apiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: default\nvalue: 600\nglobalDefault: true\ndescription: \"This priority class should be used for most services\"\n</code></pre>"},{"location":"standards/kubernetes_standards/#resource-profile-impact","title":"Resource profile impact","text":"<p>In the event a cluster has to make a choice between killing one of two services sharing the same priority level, the resource profile configuration will influence which is killed.</p>"},{"location":"standards/kubernetes_standards/#probes","title":"Probes","text":"<p>To increase the stability and predicatability of a Kubernetes cluster, services should make use of both readiness and liveness probes unless there is a significant reason not to.</p> <p>Probe end points should follow the convention of <code>healthy</code> for readiness probes and <code>healthz</code> for liveness probes.</p>"},{"location":"standards/mobile_app_standards/","title":"Mobile application standards","text":""},{"location":"standards/mobile_app_standards/#introduction","title":"Introduction","text":"<p>The huge increase in the use of mobile devices means that mobile application development is now of critical interest to the Defra family, even more so given the amount of field-work being carried out by workers within the Defra family.</p>"},{"location":"standards/mobile_app_standards/#scope","title":"Scope","text":"<p>These standards apply to mobile application in-house development and management.</p> <p>These standards are intended to cover all forms of mobile application.</p> <p>As implied, there are a number of ways of delivering mobile apps. These standards will cover which approach is best and under what circumstances, and aspects that are common to all or many of the approaches. As and when more detail is available on specific approaches, such as Flutter app development, or Progressive Web Apps, such approach-specific standards should form part of their own separate, detailed, documented standard, which, ideally, will simply be a reference to a recognised external standard for that specific technology.</p>"},{"location":"standards/mobile_app_standards/#caveats","title":"Caveats","text":"<p>This is a fast moving area involving multiple platforms and diverse technology stacks: these standards are therefore early and provisional. As such, these standards often require more background explanation and justification than those for other, more mature or more homogenous, technology stacks. Inevitably, therefore, these standards, for the time being, are likely to be more discursive than those for other areas, and, unavoidably, the line between standard and guidance can often, as yet, not be so sharply drawn.</p> <p>This document should therefore be read in conjunction with the Mobile Application Guidance (guidance).</p>"},{"location":"standards/mobile_app_standards/#what-is-a-mobile-app","title":"What is a mobile app?","text":""},{"location":"standards/mobile_app_standards/#assume-we-are-providing-an-offline-mobile-app-unless-we-have-clear-agreement-that-the-app-will-only-work-online","title":"Assume we are providing an offline mobile app unless we have clear agreement that the app will only work online","text":"<p>Mobile apps are commonly defined along the following lines: \"A mobile application, also referred to as a mobile app or simply an app, is a computer program or software application designed to run on a mobile device such as a phone, tablet, or watch.\"</p> <p>In practice, the user need that often drives the requirement to have a mobile app in the first place is the necessity to use the app when the user is out and about, including in situations where there is no signal and therefore no internet access. Indeed, it is difficult for an app to be truly \"mobile\" unless it is capable of still doing something useful where connectivity is lost. This is particularly the case for Defra family field workers who will often be working in remote, rural locations.</p> <p>Therefore, rather than providing purely \"mobile apps\", we will usually be providing what are commonly called offline mobile apps (a.k.a. offline apps).</p> <p>Offline apps can continue to be usable without an internet connection, including being able to work with data locally on the device if that is necessary for the app still to perform \"usefully\".</p> <p>We should assume that any mobile apps we are providing are offline mobile apps unless we have a clear understanding, and written agreement with the users, that this is not a requirement, as this can often be a cause of confusion, and retrofitting offline functionality to a purely online mobile application after it has been provided may be very difficult or even not practical at all.</p>"},{"location":"standards/mobile_app_standards/#if-only-an-online-mobile-app-is-needed-its-functionality-should-still-degrade-gracefully-when-offline","title":"If only an online mobile app is needed its functionality should still degrade gracefully when offline","text":"<p>Even if it is agreed that the app only needs to work online then it should still degrade gracefully when the connection is lost, for example by providing a clear message to the user that connection has been lost and the user should check their connection.</p>"},{"location":"standards/mobile_app_standards/#which-mobile-platforms-must-be-supported","title":"Which mobile platforms must be supported","text":""},{"location":"standards/mobile_app_standards/#mobile-applications-for-use-within-the-defra-family-must-run-on-the-apple-platform-apps-intended-for-public-use-must-run-on-at-least-apple-and-android-devices","title":"Mobile applications for use within the Defra family must run on the Apple platform; apps intended for public use must run on at least Apple and Android devices","text":"<p>Mobile applications must run on mobile devices. Currently Defra has adopted Apple technology as the standard platform for mobile devices. All mobile applications intended for internal use must therefore run on Apple devices. All mobile applications intended for by the general public, must run on at least Apple and Android devices. If a mobile application is intended for use outside Defra, but restricted to a specific and atypical (in terms of likely use of mobile technology as compared to the general public) user community, then we will need to be guided by the preferences of that user community in terms of the platform that we need to support.</p>"},{"location":"standards/mobile_app_standards/#troubleshooting","title":"Troubleshooting","text":""},{"location":"standards/mobile_app_standards/#before-starting-development-you-must-have-the-knowledge-kit-privileges-and-app-distribution-arrangements-in-place-to-be-able-to-remotely-debug-the-app-on-suitable-devices","title":"Before starting development you must have the knowledge, kit, privileges and app distribution arrangements in place to be able to remotely debug the app on suitable devices","text":"<p>Often, issues will only be encountered when running apps on actual target devices. Therefore you must have the knowledge, kit, privileges and app distribution arrangements in place to be able to remotely debug the app on suitable devices.</p>"},{"location":"standards/mobile_app_standards/#the-app-must-log-errors-and-it-must-be-possible-for-the-user-to-make-that-information-available-to-us-for-support-purposes","title":"The app must log errors and it must be possible for the user to make that information available to us for support purposes","text":"<p>Information about errors occurring on mobile devices will often be lost to us, making problem resolution much more difficult. To mitigate this issue the app must log errors, and it must be possible for the user to make that information available to us for support purposes, even if that is just via screenshot. It should also be possible to vary the level of detail logged, for example by implementing a \"debug\" mode. Early implementation of such features is very likely to also speed up error resolution during development itself.</p>"},{"location":"standards/mobile_app_standards/#extra-security-considerations-for-mobile-apps","title":"Extra security considerations for mobile apps","text":""},{"location":"standards/mobile_app_standards/#assess-and-address-the-security-challenges-to-your-app-before-finally-agreeing-app-scope-or-minimal-viable-product","title":"Assess and address the security challenges to your app before finally agreeing app scope or minimal viable product","text":"<p>Data security is always essential, and mobile apps can be especially exposed to security risks and challenges. All mobile devices suffer from poor physical security in terms of being easily lost or stolen, and all have to operate over potentially insecure public networks. Furthermore, when an app is intended to run on a public device, then the security of the device is an unknown, and, by default, we should assume the device is inherently insecure. This means that the points below must be considered and addressed very early in any project to develop such a mobile app, as they could even influence the practical viability of the app or the scope of the requirements it can securely deliver. Design the complete security profile for your app before finally agreeing the scope of the app's requirements or the minimal viable product.</p>"},{"location":"standards/mobile_app_standards/#all-traffic-to-and-from-the-app-must-be-encrypted","title":"All traffic to and from the app must be encrypted","text":"<p>As mobile applications will usually be accessing the internet over entirely public networks all traffic to and from the app must be encrypted, for example via HTTPS/Transport Layer Security and/or via the use of a Virtual Private Network (VPN) and/or in-built application security wrapper. Please consult the latest security standards as to the appropriate level of encryption any such mechanisms should adopt, given the confidentiality marking of the data in question.</p>"},{"location":"standards/mobile_app_standards/#if-the-app-needs-to-access-defra-family-systems-that-are-non-public-developers-andor-application-architects-must-design-the-security-profile-for-the-app-in-consultation-with-the-cloud-mobile-services-team-in-group-infrastructure-and-operations","title":"If the app needs to access Defra family systems that are non-public, developers and/or application architects must design the security profile for the app in consultation with the Cloud Mobile Services team in Group Infrastructure and Operations","text":"<p>If your app, which will normally be operating across the public internet, needs to access Defra family systems that are non-public, developers and/or application architects must consult with the Cloud Mobile Services team in Group Infrastructure and Operations, who will advise on what is or isn't permissible and, where permissible, what mechanisms they can provide to allow such access to be done securely. For example, Cloud Mobile Services may advise as to VPN software and VPN end-points that they can make available to mobile devices that can allow safe access into \"back-end\", non-public Defra family systems in the cloud or on premise.</p>"},{"location":"standards/mobile_app_standards/#also-consider-the-guidance-before-designing-the-apps-security-profile","title":"Also consider the guidance before designing the app's security profile","text":"<p>There are other recommendations to also bear in mind, especially around the security of data on the device and authentication strategies, so you are strongly advised to also read the accompanying guidance on mobile app development alongside these standards.</p>"},{"location":"standards/mobile_app_standards/#which-type-of-mobile-technology-to-adopt","title":"Which type of mobile technology to adopt","text":""},{"location":"standards/mobile_app_standards/#decide-which-type-of-mobile-technology-to-adopt-by-working-through-the-options-below-in-the-stated-order-of-preference-stopping-as-early-as-possible","title":"Decide which type of mobile technology to adopt by working through the options below in the stated order of preference, stopping as early as possible","text":"<p>Mobile apps will usually be required to work on multiple devices and operating systems. For example, for apps intended for general public use, we have already stated that mobile apps created by the Defra family must work on at least the Apple and Android platforms. That alone creates a considerable development challenge, and can mean continuing high costs of ownership, especially as those platforms are rapidly changing, and some suppliers, such as Apple, impose additional constraints and demands on users of their technology stack. Add in the need for the mobile app to work readily even with no internet access, and the additional concerns mobile poses to security, and the approach adopted to deliver a mobile app needs to be chosen extremely carefully, following the standard hierarchy explained below, starting from the top and only working your way further down the options where it is clearly necessary to do so.</p>"},{"location":"standards/mobile_app_standards/#option-0-pick-something-off-the-shelf","title":"Option 0: Pick something \"off-the-shelf\"","text":"<p>This means that, before developing an in-house app, it is even more important than for other bespoke developments to consider if a Commercial Off-The-Shelf (COTS) approach is preferred. For example, is a suitable app already available in the app stores? Also, if the app is intended for use with an already adopted COTS suite, for example a time recording or asset management system, then is it possible that the supplier of that suite already has or will soon have a suitable app available, removing the need for us to develop our own app and then integrate that app with third party software? Then not only the development costs of the app, but also the ongoing costs of ownership, will be shared across the supplier's many customers rather than being having to be paid by us alone.</p> <p>If using a third party app of any kind carefully assess whether use of that app will be appropriate given the sensitivity of the data it will be handling. In particular, many apps, especially free apps available in app stores, harvest user data, for example via Google Analytics. If those data harvesting features cannot be reliably disabled, and the data being processed by the app is in any way sensitive, then the use of such an app would be entirely inappropriate.</p>"},{"location":"standards/mobile_app_standards/#option-1-develop-a-microsoft-power-app","title":"Option 1: Develop a Microsoft Power App","text":"<p>Microsoft Power Apps offer a low or even no code way of developing mobile apps, which can then run within a Power Apps Mobile \"app player\" that comes in varieties downloadable to a range of mobile platforms. This option is likely to be appropriate if the app is going to have to interact with the rest of the Microsoft technology stack in the cloud, for example Dynamics or Office365, and when the app is simpler. Due to the Microsoft licensing model, it is also currently only likely to be appropriate for internal apps. Note that Microsoft frequently revises its licensing model, so you are advised to check out the current model before proceeding to assess what is possible, the costs and what licences we may already have available.</p>"},{"location":"standards/mobile_app_standards/#option-2-develop-a-progressive-web-app-pwa","title":"Option 2: Develop a Progressive Web App (PWA)","text":"<p>Where a fully bespoke app is needed the next option to consider is a Progressive Web App (PWA), as recommended by the Government Digital Service here: https://www.gov.uk/service-manual/technology/working-with-mobile-technology</p> <p>Progressive Web Apps provide cross platform and cross device support and can be delivered just like ordinary web apps to the browser, so no engagement with an app store or Mobile Device Management (MDM) software is needed, either by ourselves or the users, and the constraints of dealing with a proprietary platform, such as Apple, are avoided (for example, Apple require all apps to be re-released with a renewed provisioning profile at least once a year). Ongoing costs of ownership, as compared with other bespoke approaches, ought to be lower. Furthermore the skills involved in developing Progressive Web Apps are more transferable and much more likely to have a longer shelf life, so time and money invested in learning will have a bigger pay-off. This approach should also make it feasible to take existing web applications, and progressively enhance them until they can become PWAs.</p> <p>It is possible to build PWAs that are fully functional offline mobile apps even on less standards compliant browsers such as Apple's Safari/WebKit. For more information on this please see read the \"Extra Guidance on Progressive Web Apps\" section in the accompanying guidance.</p>"},{"location":"standards/mobile_app_standards/#option-3-build-a-cross-platform-app","title":"Option 3: Build a cross-platform app","text":"<p>If, having considered above and done the recommended research, a PWA is not acceptable, the next option to consider is building the mobile app using a cross platform technology stack. These stacks are ways of using the same, or very close to the same, code-base but still building the app in ways that can allow it to run on multiple platforms and multiple devices. Otherwise a completely different code base, using a different language and development framework, is needed to build an app for each platform.</p> <p>There are many such cross platform technology stacks, but some are obscure or are nearing obsolescence. This is still a too broad and fast moving area for us to as yet mandate solid standards, so you are therefore strongly advised to read the accompanying Mobile Application Guidance before considering a technology stack for developing cross platform apps (guidance).</p> <p>Note that cross platform app development, where in any way feasible, is always preferable to native app development, even when we are currently targeting only a single platform (say Apple) and range of associated devices. That is because such apps ought to be easily adaptable to run on a different platform, for example if our selected choice of devices changes, or if we decide to make the app, or a version of the app, available on more devices. Also, investment, in terms of time, money and \"hard lessons learned\", in cross platform app development is more likely to be applicable across the whole range of possible future apps than if we focus on a single native app technology. Furthermore, cross platform app development techniques also provide much better insulation against all the myriad changes that happen on the underlying platform itself (not completely, but better than with pure native development).</p>"},{"location":"standards/mobile_app_standards/#dont-build-hybrid-apps","title":"Don't build hybrid apps","text":"<p>In the past one common technique used to build cross platform apps was to actually run the app inside a hidden browser on the device, with such apps often being called \"hybrid\" apps. However, that is not the approach adopted by any of latest and more popular frameworks, and it is increasingly challenged by ever stricter adoption of \"single origin\" security restrictions in more modern browser engines. A mobile app rendering local content automatically uses up one \"origin\", to be precise, before it even starts to interact with any \\\"remote\\\" systems, plus single origin restrictions disallow delivering up local content via AJAX calls. Therefore, such frameworks including Apache Cordova, the closely related PhoneGap (which is Adobe's own Cordova variant) and Cordova-based Ionic (though Ionic also now has a React based version) must now be regarded as legacy and not used for new projects. It is still just about possible to work around such restrictions using the above frameworks, but the approaches then become painful and convoluted, for example Cordova-based Ionic does so by using a local web server built into the app. Avoid these \"hybrid\" approaches for building a cross-platform app within a browser.</p> <p>Of course, PWAs are also mobile apps that run within the browser, but with PWAs we are working \"with the grain\" of industry standards, that are becoming ever more capable all the time, where-as proprietary frameworks tend to eventually be allowed to fall behind browser standards, and then become gradually but inexorably less capable over time. For example, PWAs offer industry standards such as the indexedDB API to make managing and accessing local content much more straightforward than in any of the old \"hybrid\" app technologies. Also, because they are based on open W3C standards, knowledge gained building PWAs is likely to be useful for much longer and be more transferable.</p>"},{"location":"standards/mobile_app_standards/#dont-build-native-apps","title":"Don't build native apps","text":"<p>Given all the alternative approaches, and the significant disadvantages of developing purely native apps, we do not currently anticipate any need to ever build native apps. Should this ever appear to be necessary, it will need to be managed as an exception to this standard.</p>"},{"location":"standards/mobile_app_standards/#extra-considerations-for-testing-mobile-apps","title":"Extra considerations for testing mobile apps","text":"<p>This section further supports the general testing guidance on the Defra quality assurance and test wiki.</p>"},{"location":"standards/mobile_app_standards/#mobile-apps-must-be-tested-on-a-representative-range-of-platforms-and-devices","title":"Mobile apps must be tested on a representative range of platforms and devices","text":"<p>Mobile Apps must be tested on a representative range of platforms and devices. That range of platforms and devices will obviously depend on the intended use of the app: for example, it can include the Apple platform and associated devices for an internal app, but for an app intended for use beyond the Defra family will include at least both the Apple and Android platforms and associated devices.</p>"},{"location":"standards/mobile_app_standards/#clearly-state-the-specific-operating-system-versions-and-device-types-on-which-the-app-will-be-officially-supported","title":"Clearly state the specific operating system versions and device types on which the app will be \"officially supported\"","text":"<p>At all times clearly state the specific operating system versions and device types on which the app will be tested, and where it is therefore \"officially supported\". This list will change constantly as operating systems and devices are updated.</p> <p>Appropriate tools should be adopted to facilitate this. For example, if using a full functional IDE such as Xcode or Android Studio, make extensive use of the device simulators.</p> <p>However, no simulator is entirely perfect, and, once the mobile app has gone live, you will also be faced with the need to keep it tested and running despite rapid operating system updates (for example, for iPad OS these occur roughly monthly). It is therefore also strongly recommended that you acquire, and refresh as needed, a range of suitable devices for use in developing and then supporting the app.</p>"},{"location":"standards/mobile_app_standards/#for-apps-running-on-defra-managed-devices-test-the-app-on-the-currently-latest-defra-adopted-version-of-iosipad-os-and-the-currently-latest-public-version-of-iosipad-os","title":"For apps running on Defra managed devices, test the app on the currently latest Defra adopted version of iOS/iPad OS and the currently latest public version of iOS/iPad OS","text":"<p>If the app is to be deployed on Defra managed devices it must be tested on the currently latest Defra adopted version of iOS/iPad OS and the currently latest public version of iOS/iPad OS (most of the time these versions will be the same, but there is normally a slight delay between Apple releasing a new version of iOS/iPad OS and Defra adoption, to ensure that no business critical apps have been negatively affected).</p>"},{"location":"standards/mobile_app_standards/#test-public-apps-on-the-appropriate-versions-of-iosipad-os-and-android","title":"Test public apps on the appropriate versions of iOS/iPad OS and Android","text":"<p>If the app is intended for use outside Defra, then it must be tested on the versions of iOS/iPad OS stated above, plus it should also be tested on the previous public version, at least. It must also be tested on the latest version of Android and should also be tested on the previous version, at least.</p>"},{"location":"standards/mobile_app_standards/#use-beta-programmes-to-test-apps-on-emerging-versions-of-iosipad-os-or-android","title":"Use beta programmes to test apps on emerging versions of iOS/iPad OS or Android","text":"<p>Device/s must also be enrolled in any beta programmes and used to test emerging versions of iOS/iPad OS or Android, so that we have advance warning of any upcoming issues.</p>"},{"location":"standards/mobile_app_standards/#distributing-and-managing-apps","title":"Distributing and managing apps","text":""},{"location":"standards/mobile_app_standards/#choose-the-mechanisms-to-distribute-the-app-and-app-updates-to-end-user-devices-before-development-starts","title":"Choose the mechanisms to distribute the app and app updates to end user devices before development starts","text":"<p>App delivery is impossible without a mechanism for distributing the app and continuing app updates to end user devices. It is therefore best to assess the app distribution options early in the project, in order to understand any implications for the project as quickly as possible, and to best ensure that testing during development can be done on a realistic basis.</p> <p>For progressive web apps this task is straight-forward: merely requiring the end user to navigate to the app's URL in a browser.</p> <p>Distribution of Android apps is very open. Android apps can be distributed via app stores, such as Google Play or the Amazon app store, via your own website or even sent via emails. Android apps could also, of course, be distributed internally via Defra Mobile Device Management software, currently AirWatch, but, as mobile applications intended for internal use must run on Apple devices, attempting to distribute Android apps this way would currently be pointless, as such apps cannot run on the mandated Apple devices.</p> <p>Distribution of Apple apps is, however, outside of distributions for purely development and testing purposes, very closed. Apple expect such apps to be distributed via the Apple app store, with the exception of apps that as an organisation intends to be used purely internally. Such internal apps can also, or instead, be distributed internally via an organisation's Mobile Device Management software (so in our case AirWatch) provided the organisation has purchased an Enterprise licence via the Apple Developer Enterprise Programme.</p> <p>The Environment Agency is signed up to the Apple Developer Enterprise Programme, at least at the time of writing, May 2020, and can therefore distribute apps through the shared Defra family Mobile Device Management (MDM) software (i.e. AirWatch). As of the same date (May 2020) core Defra and the other parts of the Defra family are NOT signed up to the Apple Developer Enterprise Programme, meaning that they are contractually not meant to deploy their apps internally via our shared AirWatch. However, the costs of signing up are low, in the order of hundreds of pounds a year. If you want to sign up another part of Defra to the Apple Enterprise Programme, first seek the advice of the Cloud Mobile Services team.</p>"},{"location":"standards/mobile_app_standards/#internal-apps-that-can-be-managed-by-an-internal-mobile-device-management-solution-must-be-so-managed-at-least-in-production","title":"Internal apps that can be managed by an internal mobile device management solution must be so managed (at least in production)","text":"<p>Deploying an app intended for internal use via our MDM (currently AirWatch a.k.a. VMWare Workspace One) has many advantages in terms of app management and monitoring, such as, for example, controlling which user groups receive the app, allowing those groups to receive the app automatically or forcing the app to always use a provided VPN when connecting to specific URLs (this approach is called \"per app\" or \"in-app\" VPN). Production distribution of internally developed apps intended purely for internal use, meaning use within the Defra family on managed devices, that can technically be managed via our MDM solution (so, for example, this standard cannot apply to PWAs) must therefore be via the Defra Mobile Device Management system, currently AirWatch. Note the unavoidable implication of this rule: this means that internal apps developed for use by any part of Defra other than the Environment Agency will require an Apple Developer Enterprise Programme membership to be arranged for the part of Defra that needs to use the app.</p> <p>The advantages of having apps, which we are using officially internally, managed via an MDM means that it will often (but not always) be desirable to also deploy third party apps via the same mechanism, in other words use our MDM (currently AirWatch a.k.a. VMWare Workspace One) to also manage these third party apps (in addition to apps we have developed internally). Note, however, that as organisations are contractually only meant to use an MDM to distribute production apps to themselves, and then only when they have an Apple Developer Enterprise Programme licence, third party apps cannot be directly deployed to our MDM. Instead such apps must first be deployed to the app store. MDMs (including our current AirWatch set-up) then integrate via an Apple provided service called the Apple Business Manager (https://www.apple.com/uk/business/it/ ) to the Apple app store, and are then able to manage apps pulled out of the app store and include them in the MDM's own application catalogues. (In AirWatch such apps can be managed in a very similar way to our own apps, such as being rolled out automatically to certain groups of users, or being forced to use a VPN).</p> <p>We may also want a third party supplier to make, for example, an app developed or customised for our specific use available to us privately, rather than via the public part of the app store. Though, at the time of writing (May 2020), we have never used this facility, Apple have a service, called \"App Store Connect\", that is meant to allow private deployment from the Apple app store to a client's own MDM (https://developer.apple.com/business/distribute/).</p>"},{"location":"standards/mobile_app_standards/#no-standards-are-mandated-for-the-distribution-of-external-apps-but-there-are-technical-and-licensing-restrictions-to-be-aware-of-on-the-apple-platform","title":"No standards are mandated for the distribution of external apps, but there are technical and licensing restrictions to be aware of on the Apple platform","text":"<p>The above covers how we should manage apps for our own internal use (which, because of our standards about mobile devices, can currently only be Apple apps). We may also want to distribute apps externally.</p> <p>Publication of a public app to a well-known app store will often be desirable, for example to make the app more easily discoverable. However, bear in mind that distributing via an app store requires the app store providers to \"accept\" an app into their store and thus creates an extra \"hurdle\" to jump.</p> <p>Therefore, no standards are mandated about how we should externally distribute the Android versions of internally developed apps.</p> <p>Unfortunately, though, Apple versions of internally developed apps can (and must) ONLY be externally distributed via the Apple app store. It is technically possible to externally distribute an Apple app signed with an Enterprise licence by mechanisms other than the app store but, though this will work, Apple tend to regard it as a violation of licence terms and may summarily revoke the provisioning profile of the offending app, causing the app to immediately stop working.</p>"},{"location":"standards/node_standards/","title":"Node standards","text":""},{"location":"standards/node_standards/#nodejs-standards","title":"Node.js standards","text":""},{"location":"standards/node_standards/#general","title":"General","text":"<ul> <li>Node.js code is JS code and should follow the JavaScript standards</li> <li>Session state should not be stored on the node app server. Don't tie a session to a particular node server instance. Use a distributed cache or document storage database and not something like express-session. </li> <li>Avoid blocking the main event loop and the worker pool. In short \"you shouldn't do too much work for any client in any single callback or task.\" and consider passing CPU intensive tasks off to another service.</li> <li>Prefer await over callbacks and avoid nested callbacks. This is easily done in Node 8 and above.</li> </ul>"},{"location":"standards/node_standards/#versions","title":"Versions","text":"<ul> <li>Be aware of the Node.js support timeline.</li> <li>Keep on Active LTS versions.</li> <li>Don't drop behind Maintenance LTS versions. Projects older than this will be considered unmaintainable and   unsupportable until brought up to an appropriate version.</li> <li>Don't progress beyond Active LTS versions.</li> </ul>"},{"location":"standards/node_standards/#package-management","title":"Package Management","text":"<ul> <li>Use NPM.</li> <li>Use a package.json and package-lock.json for repeatable builds.</li> <li>Use an automated checker such as GreenKeeper or Dependabot to ensure that your dependencies are up to date with the   latest patches.</li> <li>Separate dependencies and dev dependencies.</li> <li>Update your version number inline with the semantic versioning standard.</li> </ul>"},{"location":"standards/node_standards/#server-framework","title":"Server framework","text":"<ul> <li>Our standard framework is Hapi.</li> <li>Be aware of the Hapi support timeline.</li> <li>Keep on the current major Hapi version.</li> <li>Don't drop behind the lowest version available through Hapi commercial support. Projects older than this will be   considered unmaintainable and unsupportable until brought up to an appropriate version.</li> </ul>"},{"location":"standards/node_standards/#commonjs-vs-es-modules","title":"CommonJS vs ES modules","text":"<ul> <li>ES modules should be used by default over CommonJS modules</li> <li>For scenarios where ES modules are not appropriate then CommonJS may be used</li> </ul> <p>Note: Some Node.js packages such as Jest are not fully compatible with ES modules and it may be more pragmatic to use CommonJS.</p>"},{"location":"standards/node_standards/#status","title":"Status","text":"<p>This standard was formally adopted on 8 January 2020.</p>"},{"location":"standards/node_standards/#significant-changes","title":"Significant changes","text":"<p>Clarification on preference between CommonJS and ESM added 29 July 2024.</p>"},{"location":"standards/plsql_coding_standards/","title":"PL/SQL Coding Standards","text":"<p>These standards have been derived from hands-on experience at the Environment Agency and Rural Payments Agency over many years. As such they are mature, comprehensive while also being reasonably lightweight and follow standard industry practice.</p>"},{"location":"standards/plsql_coding_standards/#structure","title":"Structure","text":"<p>Use <code>Block</code> and <code>Label</code> coding for loops and anonymous blocks within procedures</p> <pre><code> CREATE OR REPLACE PROCEDURE my_proc\n\n IS\n\n BEGIN\n\n    &lt;&lt;Block1&gt;&gt;\n\n    DECLARE\n\n       -- statements\n\n       &lt;&lt;yearly_analysis&gt;\n\n       FOR y_count IN yearly_analysis LOOP\n\n          &lt;&lt;monthly_analysis&gt;&gt;\n\n          FOR m_count IN monthly_analysis LOOP\n\n             -- statements\n\n          END LOOP monthly_analysis;\n\n          -- statements\n\n       END LOOP yearly_analysis;\n\n    END Block1\n\n END my_proc;\n</code></pre> <ul> <li> <p>Use a single <code>EXIT</code> point for loops</p> </li> <li> <p>Use a single <code>RETURN</code> point for Functions</p> </li> </ul>"},{"location":"standards/plsql_coding_standards/#declarations-and-types","title":"Declarations and Types","text":"<ul> <li>Anchor variables to Database data types</li> </ul> <p>That way if table/column changes are made, the code does not to be updated to reflect the changes: Variables should never be designated as a standard data type (e.g. <code>VARCHAR2 (200)</code>, <code>NUMBER (10)</code>, etc.</p> <ul> <li>Use <code>SUBTYPE</code> to standardise application-specific data types</li> </ul> <p>This allows creation of aliases for existing data types that cannot be anchored to the database. These should be declared in a separate package, for example:</p> <pre><code> CREATE OR REPLACE PACKAGE my_vars\n\n IS\n\n    SUBTYPE counter IS INTEGER (10);**\n</code></pre> <p>Then in the DECLARATION section of the code:</p> <pre><code> DECLARE\n\n    v_counter my_vars.counter;\n</code></pre> <p>This is the only place in the code where a hard-coded value is used.</p> <ul> <li>Use CONSTANT declaration for variables that do not change, and declare these in a CONSTANTS package</li> </ul> <pre><code> DECLARE OR REPLACE PACKAGE my_constants\n\n IS\n\n    C_max_hours_per_day CONSTANT NUMBER (2) := 24;\n</code></pre> <p>Then in the declaration section of a procedure</p> <pre><code> DECLARE\n\n    V_full_day my_constants.c_max_hoursper_day;\n</code></pre> <ul> <li>Global variables should never be more than \"package-wide\"</li> </ul>"},{"location":"standards/plsql_coding_standards/#error-handling","title":"Error Handling","text":"<ul> <li>All Anonymous blocks within a procedure should have their own exception handler in addition to the exception handler in the parent block</li> </ul> <pre><code> CREATE OR REPLACE PROCEDURE my_proc\n\n IS\n\n BEGIN\n\n    BEGIN\n\n       DECLARE\n\n       BEGIN\n\n       EXCEPTION\n\n       END;\n\n    EXCEPTION\n\n    END;\n\n EXCEPTION\n\n END my_proc;\n</code></pre> <ul> <li> <p>When performing bulk data operations (usually as some form of \"batch\" job) all tables should have a corresponding error table and all bulk update/insert/delete/merge statements should be suffixed with a LOG ERRORS statement referencing the errors table to prevent simple data errors terminating program execution</p> </li> <li> <p>A standard error procedure should be used that records, in addition to user-defined information such as table name, <code>DBMS_UTILITY.FORMAT_CALL_STACK</code>, <code>DBMS_UTILITY.FORMAT_ERROR_STACK</code> and <code>DBMS_UTILITY.FORMAT_ERROR_BACKTRACE</code></p> </li> </ul>"},{"location":"standards/plsql_coding_standards/#layout","title":"Layout","text":"<p>PL/SQL is often written using a tool called Toad in Defra. It includes the ability to automatically format code using imported <code>.opt</code> files.</p> <p>We have guidance on how to do this, and defra_plsql_toad_fmt.opt that applies the layout conventions described below.</p> <ul> <li> <p>Indents should be 3 characters wide. All code within loops and if statements should be so indented with one such indent for each level of nesting within if statements or loops</p> </li> <li> <p><code>THEN</code> should be positioned on the next line, in-line with the related <code>IF</code></p> </li> <li> <p><code>LOOP</code> should be positioned on the next line, in-line with the associated <code>DO</code> or <code>WHILE</code></p> </li> <li> <p><code>AS</code> or <code>IS</code> should be positioned on the next line, in-line with the associated statement (such as with an associated <code>CREATE</code>)</p> </li> </ul> <p>Layout examples:</p> <pre><code> BEGIN\n\n    a := 1;\n\n    d := 1;\n\n    b := 2;\n\n    WHILE TRUE\n\n    LOOP\n\n       a := 1;\n\n       b := 2;\n\n       IF a &gt; b\n\n       THEN\n\n          c := d;\n\n       END IF;\n\n    END LOOP;\n\n    x := 1;\n\n    c := d;\n\n END;\n\n CREATE PROCEDURE myproc\n\n AS\n\n BEGIN\n\n    NULL;\n\n END;\n</code></pre>"},{"location":"standards/plsql_coding_standards/#case-and-lists","title":"Case and Lists","text":"<ul> <li>Oracle key words should be in uppercase</li> </ul> <pre><code> SELECT aaa\n\n , bbb\n\n , cccc\n\n , SIN (x)\n\n FROM mytab;\n</code></pre> <ul> <li>Where you have lists, e.g. a list of column names or values, place each item on its own line, with commas, where needed, placed at the start of the line with a following space</li> </ul> <p>This makes it far easier to make changes, such as inserting new items into the list, without losing track of the commas.</p> <pre><code> CREATE TABLE my\\_table1\n\n (\n\n my\\_num1 NUMBER\n\n , my\\_num2 NUMBER\n\n );\n\n INSERT INTO building\\_blocks (aaaaaaaaaaaaaaaa\n\n , bbbbbbbbbbbbbb\n\n , cccccccccccccccc\n\n , ddddddddddddddddd\n\n , eeeeeeeeeeeee)\n\n SELECT aaaaaaaaaaaaaaaa\n\n , bbbbbbbbbbbbbbbbbbb\n\n , cccccccccccccccccc\n\n , dddddddddddddddd\n\n , eeeeeeeeeeeeeee\n\n , fffffffffffffffff\n\n FROM mytab, histab, hertab\n\n WHERE histab.col1 = hertab.col2;\n</code></pre>"},{"location":"standards/quality_assurance_standards/","title":"Quality assurance and test standards","text":"<p>This page covers the minimum standards that apply to quality assurance (QA) and testing on Defra's digital services.</p> <p>All digital services must meet the GOV.UK service standard.</p> <p>Government Digital Service guidance on testing services is available on the technology page of the service manual.</p> <p>Further QA &amp; Test guidance is available on the Defra QA &amp; Test wiki.</p>"},{"location":"standards/quality_assurance_standards/#acceptance-criteria","title":"Acceptance criteria","text":"<p>Wherever possible, define positive and negative acceptance criteria up front, before a story is developed.</p> <p>Default acceptance criteria for new stories:</p> <ul> <li>Screens, behaviour and content match designs from the prototype or wireframe</li> <li>Styles match the design system</li> <li>Accessible - meets Web Content Accessibility Guidelines (WCAG) version 2.1 at levels A and AA</li> <li>Works across all supported browsers including IE11 and mobile</li> <li>Server-side error validation exists for all fields</li> <li>No obvious performance issues (most transactions under 1 second, avoid transactions over 10 seconds)</li> <li>No existing functionality has regressed</li> </ul> <p>Negative scenarios are just as important as positive ones. For example, \"an admin user can export data\" implies that \"a standard user cannot export data\". Routinely check errors in manual and automated tests.</p>"},{"location":"standards/quality_assurance_standards/#accessibility","title":"Accessibility","text":"<p>All digital services, internal or external-facing, must meet Web Content Accessibility Guidelines (WCAG) version 2.1 at levels A and AA.</p> <p>Accessibility information, guides and checklist used in Defra</p> <p>GOV.UK accessibility guidance</p>"},{"location":"standards/quality_assurance_standards/#automated-testing","title":"Automated testing","text":"<p>The default test technologies for new projects are:</p> <ul> <li>Cucumber</li> <li>Webdriver.io</li> <li>Node.JS</li> <li>Selenium</li> </ul> <p>More test automation information and guides</p> <p>Use GitHub to publish test code in the open, but never publish any passwords or other sensitive data. Passwords can be stored in private files or repositories (for example hiding a file using <code>.gitignore</code>), or via local environment variables.</p>"},{"location":"standards/quality_assurance_standards/#browser-testing","title":"Browser testing","text":"<p>Test services on the GOV.UK set of required browsers.</p>"},{"location":"standards/quality_assurance_standards/#bug-defect-management","title":"Bug (defect) management","text":"<p>By default, use Jira to manage bugs as follows:</p> <ul> <li>If a bug is as a direct result of a story that's currently in development, then attach the details to that story.</li> <li>If a bug is a regression issue then raise a separate bug in Jira.</li> </ul> <p>Defra Jira guidance and list of administrators</p>"},{"location":"standards/quality_assurance_standards/#code-quality-testing","title":"Code quality testing","text":"<p>Automated test code must meet Defra common coding standards wherever applicable.</p> <p>All development and test code must undergo regular linting to check that it meets minimum standards.</p>"},{"location":"standards/quality_assurance_standards/#compliance-checking","title":"Compliance checking","text":"<p>Ensure your service's compliance with the following:</p> <ul> <li>Accessibility statements</li> <li>Assisted digital support</li> <li>Cookie policies</li> <li>General Data Protection Regulations (GDPR)</li> <li>GOV.UK content styles</li> <li>GOV.UK design system</li> <li>Privacy policies - especially when communicating with third parties such as Google for analytics</li> <li>Progressive enhancement, in particular that your service is usable without Cascading Style Sheets (CSS) and Javascript</li> </ul>"},{"location":"standards/quality_assurance_standards/#continuous-integration-pipeline","title":"Continuous integration (pipeline)","text":"<p>By default we use Jenkins for continuous integration and to manage deployments.</p> <p>Where possible, build automated tests into the team's continuous integration process, so that only working code can be deployed. These can be done for functional testing as well as performance and accessibility.</p>"},{"location":"standards/quality_assurance_standards/#documentation","title":"Documentation","text":"<p>Quality assurance must produce the following documentation as part of any major work:</p> <ul> <li>Test plan, using the headings on this page as a guide</li> <li>Test completion report before a major release, detailing the status of any testing and bugs, and any outstanding risks</li> <li>Automated test suite with features written in a format readable to non-technical colleagues</li> </ul> <p>Official templates are on the QA &amp; Test Sharepoint although these can be adapted to meet the needs of the team.</p> <p>For each user story, document its testing alongside that story - for example, as Jira comments or a linked document.</p> <p>Other documentation, produced by QA as required:</p> <ul> <li>QA information document covering practical information such as how to access test environments and data.</li> <li>Site map - a list of pages on the service and whether they are covered by automated tests</li> <li>Performance test report</li> </ul>"},{"location":"standards/quality_assurance_standards/#exploratory-testing","title":"Exploratory testing","text":"<p>All stories should undergo a level of manual, exploratory testing based on the perceived level of risk that needs mitigating. This relies on the experience of the tester. If unsure how best to approach this, the speak to a QA colleague to help put together an exploratory test charter.</p> <p>GOV.UK information on exploratory testing</p>"},{"location":"standards/quality_assurance_standards/#high-availability-testing","title":"High availability testing","text":"<p>Before releasing a new production environment, work with the WebOps team to test and monitor how the service recovers from servers being deactivated.</p>"},{"location":"standards/quality_assurance_standards/#penetration-security-testing","title":"Penetration (security) testing","text":"<p>GOV.UK advice on vulnerability and penetration testing</p>"},{"location":"standards/quality_assurance_standards/#performance-testing","title":"Performance testing","text":"<p>The default tool in Defra is JMeter.</p> <p>Any performance test should include the following by default:</p> <ul> <li>Load - can the service deal with 120% of the likely maximum load?</li> <li>Soak - can the service deal with high load for a prolonged period without memory leakage?</li> <li>Stress - what is the service's breaking point, and how does it recover?</li> </ul>"},{"location":"standards/quality_assurance_standards/#reporting","title":"Reporting","text":"<p>The minimum standards for agile test reporting are to:</p> <ul> <li>ensure the team is aware of major testing risks, for example through daily stand ups</li> <li>produce a test completion report before a major release.</li> </ul> <p>However, certain key metrics are recommended to get a picture of quality over time, such as:</p> <ul> <li>list of any gaps in testing and associated risks</li> <li>number of outstanding bugs by priority</li> <li>RAG (red/amber/green) status against each test type in the test plan</li> <li>unit test coverage % (via code coverage tools)</li> <li>user interface (UI) test coverage % (for example, by % of screens with test coverage)</li> <li>user satisfaction % (if already live)</li> </ul>"},{"location":"standards/quality_assurance_standards/#risk-management","title":"Risk management","text":"<p>Share any risks that originate from QA activity with your product manager and ensure that you have sight of the team risk register.</p>"},{"location":"standards/quality_assurance_standards/#unit-and-api-testing","title":"Unit and API testing","text":"<p>Maintain visibility of the unit and API test approach, and test coverage, such as via code quality metrics.</p> <p>As a team, adopt the testing pyramid: aim to find the most defects at the lowest levels (unit and API) and fewer defects at the levels where it's slower to test (automated and manual user interface tests).</p>"},{"location":"standards/quality_assurance_standards/#user-needs-and-usability","title":"User needs and usability","text":"<p>Ensure that all features map back to user needs by joining the team's user research and usability testing. The recommendation is to be involved for 2 hours every 6 weeks.</p> <p>Test your service with users with disabilities.</p>"},{"location":"standards/readme_standards/","title":"README standards","text":"<p>Every repo must have a README file in its root. The README is the starting point for anyone who wants to develop or test the repo. It provides an overview of what the repo is, and how to install, run and test its contents.</p> <p>The README should include the following (if they apply):</p> <ul> <li>Description of the product \u2013 what the service or product is, and what role this repo performs within it</li> <li>Prerequisites \u2013 what you need to install or configure before you can set up the repo</li> <li>Setup process - how to set up your local environment to work on the repo, including:</li> <li>development tools</li> <li>test tools</li> <li>How to run in development \u2013 how to locally run the application in development mode after setup</li> <li>How to run tests \u2013 how to run the test suite, broken into different categories if relevant (unit, integration, acceptance)</li> <li>Contributing to the project - what to know before you submit your first pull request (this could also be in the form of a <code>CONTRIBUTING.md</code> file)</li> <li>Licence information \u2013 what licence the repo uses (in addition to your <code>LICENSE</code> file)</li> </ul> <p>The file should be in Markdown format (.md).</p>"},{"location":"standards/readme_standards/#additional-detail","title":"Additional detail","text":"<p>You may also want to include:</p> <ul> <li>How the product fits into wider architecture</li> <li>Internal product architecture</li> <li>Data structure</li> <li>API end points</li> <li>Monitoring</li> <li>Error handling</li> <li>Audit</li> <li>User access</li> <li>Security</li> <li>Complexity worth documenting</li> <li>Pipelines</li> </ul> <p>If this documentation is too lengthy or complex, it doesn't have to be in the README. Just make sure your README tells users where to find these additional docs.</p>"},{"location":"standards/readme_standards/#a-basic-readmemd-template","title":"A basic README.md template","text":"<pre><code># name-of-repo\n\nThe \"Register your dinosaur\" service allows customers to apply online for a dinosaur licence.\n\nThis application handles the backend dinosaur processing.\n\n##\u00a0Prerequisites\n\n## Setup\n\n### Development\n\n### Test\n\n## Running in development\n\n## Running tests\n\n## Contributing to this project\n\nPlease read the [contribution guidelines](/CONTRIBUTING.md) before submitting a pull request.\n\n## Licence\n\nTHIS INFORMATION IS LICENSED UNDER THE CONDITIONS OF THE OPEN GOVERNMENT LICENCE found at:\n\n&lt;http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3&gt;\n\nThe following attribution statement MUST be cited in your products and applications when using this information.\n\n&gt;Contains public sector information licensed under the Open Government licence v3\n\n### About the licence\n\nThe Open Government Licence (OGL) was developed by the Controller of Her Majesty's Stationery Office (HMSO) to enable information providers in the public sector to license the use and re-use of their information under a common open licence.\n\nIt is designed to encourage use and re-use of information freely and flexibly, with only a few conditions.\n</code></pre>"},{"location":"standards/ruby_coding_standards/","title":"Ruby coding standards","text":"<p>Use the latest version of Ruby 2.4 when starting a new project.</p> <p>We follow the Ruby style guide.</p>"},{"location":"standards/ruby_coding_standards/#checking-your-code-style","title":"Checking your code style","text":"<p>Include defra-ruby-style in your Ruby projects. We built this gem to check and enforce code style.</p> <p>Once you have the gem installed, you can check your code for issues by running <code>bundle exec rubocop</code>. If you have a linter in your text editor, it will also use the gem.</p> <p>Include <code>bundle exec rubocop</code> in your CI, and the build will fail if there are any style violations.</p>"},{"location":"standards/ruby_coding_standards/#tools","title":"Tools","text":"<p>Use Hakiri to check for vulnerabilities in your project and dependencies. Integrate it through GitHub and it will check new commits and PRs.</p>"},{"location":"standards/ruby_coding_standards/#rails","title":"Rails","text":"<p>Use Rails 4.2 when starting a new project.</p> <p>We follow Rails best practice.</p>"},{"location":"standards/security_standards/","title":"Security Standard","text":"<p>Having a secure approach to development has never been so important.</p> <p>The way we build software and systems is rapidly evolving, becoming more and more automated and integrated. This results in a need to have some standards and guidance around security. Rather than maintain our own, we follow the standards of OWASP</p>"},{"location":"standards/security_standards/#standards","title":"Standards","text":"<p>Use the OWASP Secure coding practices - quick reference guide for details of the standards to apply.</p> <p>Important note. We are using version 2</p>"},{"location":"standards/tsql_and_sqldb_standards/","title":"TSQL and SQL Server database standards","text":""},{"location":"standards/tsql_and_sqldb_standards/#naming","title":"Naming","text":""},{"location":"standards/tsql_and_sqldb_standards/#tables","title":"Tables","text":"<p>Rules: Pascal notation; end with an <code>s</code></p> <p>Examples: <code>Products</code>, <code>Customers</code>. Group related table names<sup>1</sup></p>"},{"location":"standards/tsql_and_sqldb_standards/#stored-procs","title":"Stored Procs","text":"<p>Rules: sp[App Name]_[Group Name]_[Action][table/logical instance]</p> <p>Examples: <code>spOrders_GetNewOrders</code>, <code>spProducts_UpdateProduct</code></p>"},{"location":"standards/tsql_and_sqldb_standards/#triggers","title":"Triggers","text":"<p>Rules: TR_[TableName]_[action]</p> <p>Examples: <code>TR_Orders_UpdateProducts</code></p> <p>Note. The use of triggers is discouraged</p>"},{"location":"standards/tsql_and_sqldb_standards/#indexes","title":"Indexes","text":"<p>Rules: IX_[TableName]_[column]ID</p> <p>Examples: <code>IX_Products_ProductID</code></p>"},{"location":"standards/tsql_and_sqldb_standards/#primary-keys","title":"Primary Keys","text":"<p>Rules: PK_[TableName]</p> <p>Examples: <code>PK_Products</code></p>"},{"location":"standards/tsql_and_sqldb_standards/#foreign-keys","title":"Foreign Keys","text":"<p>Rules: FK_[TableName1]_[TableName2]</p> <p>Example: <code>FK_Products_Orders</code></p>"},{"location":"standards/tsql_and_sqldb_standards/#defaults","title":"Defaults","text":"<p>Rules: DF_[TableName]_[ColumnName]</p> <p>Example: <code>DF_Products_Quantity</code></p>"},{"location":"standards/tsql_and_sqldb_standards/#columns","title":"Columns","text":"<p>If a column references another table\u2019s column, name it [table name]ID</p> <p>Example: The Customers table has an ID column. The Orders table should have a <code>CustomerID</code> column</p>"},{"location":"standards/tsql_and_sqldb_standards/#general-rules","title":"General Rules","text":"<ul> <li>Do not use spaces in the name of database objects</li> <li>Do not use SQL keywords as the name of database objects. In cases where this is necessary, surround the object name with brackets, such as <code>[Year]</code></li> <li>Do not prefix stored procedures with <code>sp_</code><sup>2</sup></li> <li>Prefix table names with the owner name<sup>3</sup></li> </ul>"},{"location":"standards/tsql_and_sqldb_standards/#structure","title":"Structure","text":"<ul> <li>Each table must have a primary key<ul> <li>In most cases it should be an <code>IDENTITY</code> column named ID</li> </ul> </li> <li>Normalize data to third normal form<ul> <li>Do not compromise on performance to reach third normal form. Sometimes, a little denormalization results in better performance.</li> </ul> </li> <li>Do not use <code>TEXT</code> as a data type; use the maximum allowed characters of <code>VARCHAR</code> instead</li> <li>In <code>VARCHAR</code> data columns, do not default to <code>NULL</code>; use an empty string instead</li> <li>Columns with default values should not allow NULLs</li> <li>As much as possible, create stored procedures on the same database as the main tables they will be accessing</li> </ul>"},{"location":"standards/tsql_and_sqldb_standards/#formatting","title":"Formatting","text":"<ul> <li>Use upper case for all SQL keywords<ul> <li><code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>WHERE</code>, <code>AND</code>, <code>OR</code>, <code>LIKE</code>, etc.</li> </ul> </li> <li>Indent code to improve readability</li> <li>Comment code blocks that are not easily understandable<ul> <li>Use single-line comment markers(<code>--</code>)</li> <li>Reserve multi-line comments (<code>/*.. ..*/</code>) for blocking out sections of code</li> </ul> </li> <li>Use single quote characters to delimit strings.<ul> <li>Nest single quotes to express a single quote or apostrophe within a string<ul> <li>For example, <code>SET @sExample = 'SQL''s Authority'</code></li> </ul> </li> </ul> </li> <li>Use parentheses to increase readability<ul> <li><code>WHERE (color=\u2019red\u2019 AND (size = 1 OR size = 2))</code></li> </ul> </li> <li>Use <code>BEGIN..END</code> blocks only when multiple statements are present within a conditional code segment.</li> <li>Use one blank line to separate code sections.</li> <li>Use spaces so that expressions read like sentences.<ul> <li><code>fillfactor = 25</code>, not <code>fillfactor=25</code></li> </ul> </li> <li>Format <code>JOIN</code> operations using indents<ul> <li>Also, use ANSI Joins instead of old style joins<sup>4</sup></li> </ul> </li> <li>Place <code>SET</code> statements before any executing code in the procedure.</li> </ul>"},{"location":"standards/tsql_and_sqldb_standards/#coding","title":"Coding","text":"<ul> <li>Optimize queries using the tools provided by SQL Server<sup>5</sup></li> <li>Do not use <code>SELECT *</code></li> <li>Return multiple result sets from one stored procedure to avoid trips from the application server to SQL server</li> <li>Avoid unnecessary use of temporary tables<ul> <li>Use 'Derived tables' or CTE (Common Table Expressions) wherever possible, as they perform better<sup>6</sup></li> </ul> </li> <li>Avoid using <code>&lt;&gt;</code> as a comparison operator where possible<ul> <li>Consider using <code>ID IN(1,3,4,5)</code> instead of <code>ID &lt;&gt; 2</code></li> </ul> </li> <li>Use <code>SET NOCOUNT ON</code> at the beginning of stored procedures<sup>7</sup></li> <li>Do not use cursors or application loops to do inserts<sup>8</sup><ul> <li>Instead, use <code>INSERT INTO</code></li> </ul> </li> <li>Fully qualify tables and column names in JOINs</li> <li>Fully qualify all stored procedure and table references in stored procedures.</li> <li>Consider not defining default values for parameters.<ul> <li>If a default is needed, the front end should supply the value where possible.</li> </ul> </li> <li>Place all <code>DECLARE</code> statements before any other code in the procedure.</li> <li>Do not use column numbers in the <code>ORDER BY</code> clause.</li> <li>Do not use <code>GOTO</code>.</li> <li>Check the global variable <code>@@ERROR</code> immediately after executing a data manipulation statement (like <code>INSERT</code>/<code>UPDATE</code>/<code>DELETE</code>), so that you can rollback the transaction if an error occurs<ul> <li>Or use <code>TRY</code>/<code>CATCH</code></li> </ul> </li> <li>Do basic validations in the front-end itself during data entry</li> <li>Off-load tasks, like string manipulations, concatenations, row numbering, case conversions, type conversions etc., to the front-end applications if these operations are going to consume more CPU cycles on the database server</li> <li>Always use a column list in your <code>INSERT</code> statements.<ul> <li>This helps avoid problems when the table structure changes (like adding or dropping a column).</li> </ul> </li> <li>Minimize the use of NULLs, as they often confuse front-end applications, unless the applications are coded intelligently to eliminate NULLs or convert the NULLs into some other form.<ul> <li>Any expression that deals with <code>NULL</code> results in a <code>NULL</code> output.</li> <li>The <code>ISNULL</code> and <code>COALESCE</code> functions are helpful in dealing with <code>NULL</code> values.</li> </ul> </li> <li>Do not use the identitycol or rowguidcol.</li> <li>Avoid the use of cross joins, if possible.</li> <li>When executing an <code>UPDATE</code> or <code>DELETE</code> statement, use the primary key in the <code>WHERE</code> condition, if possible. This reduces error possibilities.</li> <li>Avoid using <code>TEXT</code> or <code>NTEXT</code> datatypes for storing large textual data.<sup>9</sup><ul> <li>Use the maximum allowed characters of <code>VARCHAR</code> instead</li> </ul> </li> <li>Avoid dynamic SQL statements as much as possible.<sup>10</sup></li> <li>Access tables in the same order in your stored procedures and triggers consistently.<sup>11</sup></li> <li>Do not call functions repeatedly within your stored procedures, triggers, functions and batches.<sup>12</sup></li> <li>Default constraints must be defined at the column level.</li> <li>Avoid wild-card characters at the beginning of a word while searching using the <code>LIKE</code> keyword, as these results in an index scan, which defeats the purpose of an index.</li> <li>Define all constraints, other than defaults, at the table level.</li> <li>When a result set is not needed, use syntax that does not return a result set.<sup>13</sup></li> <li>Avoid rules, database level defaults that must be bound or user-defined data types. While these are legitimate database constructs, opt for constraints and column defaults to hold the database consistent for development and conversion coding.</li> <li>Constraints that apply to more than one column must be defined at the table level.</li> <li>Where the <code>CHAR</code> data type is appropriate for a column. Only use it when the column is non-nullable.<sup>14</sup></li> <li>Do not use white space in identifiers.</li> <li>The <code>RETURN</code> statement is meant for returning the execution status only, but not data.</li> </ul>"},{"location":"standards/tsql_and_sqldb_standards/#reference","title":"Reference","text":"<ol> <li> <p>Group related table names:</p> </li> <li> <p><code>Products_UK</code></p> </li> <li><code>Products_India</code></li> <li> <p><code>Products_Mexico</code></p> </li> <li> <p>The prefix sp_ is reserved for system stored procedures that ship with SQL Server. Whenever SQL Server encounters a procedure name starting with sp_, it first tries to locate the procedure in the master database, then it looks for any qualifiers (database, owner) provided, then it tries dbo as the owner. Time spent locating the stored procedure can be saved by avoiding the <code>sp_</code> prefix.</p> </li> <li> <p>This improves readability and avoids unnecessary confusion. Microsoft SQL Server Books Online states that qualifying table names with owner names helps in execution plan reuse, further boosting performance.</p> </li> <li> <p>Example of 'False code'</p> </li> </ol> <pre><code>SELECT *\nFROM Table1, Table2\nWHERE Table1.d = Table2.c\n</code></pre> <p>Example of 'True code'</p> <pre><code>SELECT *\nFROM Table1\nINNER JOIN Table2 ON Table1.d = Table2.c\n</code></pre> <ol> <li> <p>Use the graphical execution plan in Query Analyzer or <code>SHOWPLAN_TEXT</code> or <code>SHOWPLAN_ALL</code> commands to analyze your queries. Make sure your queries do an 'Index seek' instead of an 'Index scan' or a 'Table scan.' A table scan or an index scan is a highly undesirable and should be avoided where possible.</p> </li> <li> <p>Consider the following query to find the second highest offer price from the <code>Items</code> table</p> </li> </ol> <pre><code>SELECT MIN(Price)\nFROM Products\nWHERE ID IN\n(\nSELECT TOP 2 ID\nFROM Products\nORDER BY Price Desc\n)\n</code></pre> <p>The same query can be re-written using a derived table, as shown below, and it performs generally twice as fast as the above query.</p> <pre><code>SELECT MIN(Price)\nFROM\n(\nSELECT TOP 2 Price\nFROM Products\nORDER BY Price DESC\n)\n</code></pre> <ol> <li> <p>This suppresses messages like <code>(1 row(s) affected)</code> after executing <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code> and <code>SELECT</code> statements. Performance is improved due to the reduction of network traffic.</p> </li> <li> <p>Try to avoid server side cursors as much as possible. Always stick to a 'set-based approach' instead of a 'procedural approach' for accessing and manipulating data. Cursors can often be avoided by using <code>SELECT</code> statements instead. If a cursor is unavoidable, use a <code>WHILE</code> loop instead. For a <code>WHILE</code> loop to replace a cursor, however, you need a column (primary key or unique key) to identify each row uniquely.</p> </li> <li> <p>You cannot directly write or update text data using the <code>INSERT</code> or <code>UPDATE</code> statements. Instead, you have to use special statements like <code>READTEXT</code>, <code>WRITETEXT</code> and <code>UPDATETEXT</code>. So, if you don't have to store more than 8KB of text, use the <code>CHAR(8000)</code> or <code>VARCHAR(8000)</code> datatype instead.</p> </li> <li> <p>Dynamic SQL tends to be slower than static SQL, as SQL Server must generate an execution plan at runtime. <code>IF</code> and <code>CASE</code> statements come in handy to avoid dynamic SQL.</p> </li> <li> <p>This helps to avoid deadlocks. Other things to keep in mind to avoid deadlocks are:</p> </li> <li> <p>Keep transactions as short as possible.</p> </li> <li>Touch the minimum amount of data possible during a transaction.</li> <li>Never wait for user input in the middle of a transaction.</li> <li> <p>Do not use higher level locking hints or restrictive isolation levels unless they are absolutely needed.</p> </li> <li> <p>You might need the length of a string variable in many places of your procedure, but don't call the <code>LEN</code> function whenever it's needed. Instead, call the <code>LEN</code> function once and store the result in a variable for later use.</p> </li> <li> <p>Resultset syntax</p> </li> </ol> <pre><code>IF EXISTS (SELECT 1 FROM Products WHERE ID = 50)\n</code></pre> <p>Instead of</p> <pre><code>IF EXISTS (SELECT COUNT(ID) FROM Products WHERE ID = 50)\n</code></pre> <ol> <li><code>CHAR(100)</code>, when <code>NULL</code>, will consume 100 bytes, resulting in space wastage. Preferably, use <code>VARCHAR(100)</code> in this situation. Variable-length columns have very little processing overhead compared with fixed-length columns.</li> </ol>"},{"location":"standards/uipath_standards/","title":"Uipath Best Practices","text":""},{"location":"standards/uipath_standards/#developer-best-practices","title":"Developer Best Practices","text":""},{"location":"standards/uipath_standards/#do","title":"Do","text":""},{"location":"standards/uipath_standards/#check-if-a-library-component-already-exists-for-your-need-before-you-start-development-if-you-find-a-component-that-almost-meets-the-requirement-amend-the-component-as-required","title":"Check if a library component already exists for your need before you start development. If you find a component that almost meets the requirement amend the component as required.","text":"<p>This helps save time, prevents duplication of components, and helps keep the components library up to date.</p>"},{"location":"standards/uipath_standards/#if-your-code-has-a-loop-ensure-there-is-an-infinite-loop-failsafe-within-it","title":"If your code has a loop ensure there is an infinite loop failsafe within it.","text":"<p>E.g. you can build in a counter in your loop to keep track of how many loops have taken place. You can then throw an exception or escape the loop at this point. This will prevent the automation from getting stuck.</p>"},{"location":"standards/uipath_standards/#if-users-will-have-sight-of-exception-messages-then-try-catch-specific-errors-and-reword-them-to-be-more-user-friendly","title":"If users will have sight of exception messages then try catch specific errors and reword them to be more user friendly.","text":"<p>This may reduce the number of MyIT tickets raised for our team. If possible add in a solution to the error message. E.g. \u201cThe Excel file is already open. Please close the file then restart the robot.\u201d</p>"},{"location":"standards/uipath_standards/#please-follow-the-34-letter-prefix-and-hungarian-case-naming-conventions","title":"Please follow the 3/4 letter prefix and Hungarian case naming conventions.","text":"<p>E.g. strStringVariable, boolBooleanVariable, arrArrayVariable, dictDictionaryVariable. Give variables descriptive names. This ensures the code is easier to navigate and read.</p>"},{"location":"standards/uipath_standards/#prefix-all-arguments-with-in-out-or-io","title":"Prefix all arguments with in, out, or io.","text":"<p>E.g. io_dictConfig</p>"},{"location":"standards/uipath_standards/#use-the-64-bit-template-with-modern-design-experience","title":"Use the 64-bit Template with modern design experience.","text":"<p>Some applications may not be compatible with 64-bit framework \u2013 in which case there is a 32-bit template available.</p>"},{"location":"standards/uipath_standards/#annotate-complicated-code-or-any-code-requiring-additional-information","title":"Annotate complicated code or any code requiring additional information.","text":"<p>Comment activity is also available to add text to.</p>"},{"location":"standards/uipath_standards/#ensure-activities-have-descriptive-names","title":"Ensure activities have descriptive names.","text":"<p>E.g. Assign \u2013 Initialise Dictionary</p>"},{"location":"standards/uipath_standards/#documentation","title":"Documentation","text":""},{"location":"standards/uipath_standards/#when-working-on-a-new-application-please-ensure-that-you-grant-the-same-level-of-access-to-all-team-members","title":"When working on a new application please ensure that you grant the same level of access to all team members.","text":"<p>This helps reallocate work in case additional support work is required or there is unplanned leave.</p>"},{"location":"standards/uipath_standards/#each-projectwork-package-should-have-its-own-repository-naming-convention-to-be-used-agencyworkpackage-name-ie-ea-waste-tonnage-returns","title":"Each project/work package should have its own repository. Naming convention to be used \u2013 [Agency][Workpackage Name] i.e. EA-Waste-Tonnage-Returns","text":"<p>Email Ben Sagar to create the repository.</p>"},{"location":"standards/uipath_standards/#upon-completion-of-a-project-please-book-a-technical-video-handover-with-2-developers","title":"Upon completion of a project \u2013 please book a technical video handover with 2 Developers.","text":"<p>Please upload this to our SharePoint Handover Videos. Remember to add the link URL into the Technical Developer document.</p>"},{"location":"standards/uipath_standards/#convert-useful-code-into-library-components","title":"Convert useful code into library components.","text":"<p>You may create some useful reusable code for a new project. Please try to convert it into a library and book a share and learn with the team.</p>"},{"location":"standards/uipath_standards/#create-guides-and-add-them-to-guides","title":"Create guides and add them to Guides.","text":"<p>You may have to do some configuration or set up a new application as part of a project or be given some RnD to complete. Please create a step-by-step guide with screenshots for anything that will be useful for the development team.</p>"},{"location":"standards/uipath_standards/#work-from-the-onedrive-folder-and-provide-the-development-team-with-access-to-that-folder","title":"Work from the OneDrive folder and provide the development team with access to that folder.","text":"<p>Where a developer is on leave and may not have Git pushed this helps the development team save time by retrieving code unavailable in GitHub.</p>"},{"location":"standards/uipath_standards/#use-orchestrator-assets-for-configurable-input-data","title":"Use Orchestrator Assets for configurable input data.","text":"<p>The exception being if there is a project specific reason e.g. sensitive data requiring you to use a custom approach.</p>"},{"location":"standards/uipath_standards/#complete-the-developer-technical-document-as-and-when-you-can","title":"Complete the Developer technical document as and when you can.","text":"<p>Filling in the document throughout the development process will ensure nothing is forgotten or missed.</p>"},{"location":"standards/uipath_standards/#try","title":"Try","text":""},{"location":"standards/uipath_standards/#try-using-simulate-clickbackground-activities-over-foreground-versions","title":"Try using simulate click/background activities over foreground versions.","text":"<p>For attended mode it allows users to use mouse &amp; keyboard while a robot is running. This approach is faster and more stable than foreground counterparts.</p>"},{"location":"standards/uipath_standards/#consider-when-you-should-use-a-sequence-flowchart-or-a-state-machine","title":"Consider when you should use a sequence, flowchart, or a state machine.","text":"<p>Sometimes switching your code to a different format can improve readability. E.g. the \u2018if activity\u2019 and \u2018flow decision\u2019 activity are both if statements. However swapping \u2018flow decisions\u2019 with \u2018if statements\u2019 or vice versa can make an impactful change.</p>"},{"location":"standards/uipath_standards/#using-a-template-for-your-project","title":"Using a template for your project.","text":"<p>Templates are available and kept up to date by the development team. There may be a rare scenario requiring you to create custom code. Please check with a Lead developer if the template is not fit for purpose before you create custom code.</p>"},{"location":"standards/uipath_standards/#if-you-must-use-foreground-activities-on-an-unstable-selector-ensure-there-is-confirmation-of-code-execution","title":"If you must use foreground activities on an unstable selector ensure there is confirmation of code execution.","text":"<p>Wrap the foreground activity with a retry scope and some verification. Alternatively use the modern design mode built-in verification.</p>"},{"location":"standards/uipath_standards/#for-message-boxes-addressed-to-the-user-please-make-use-of-the-popups-library-components","title":"For message boxes addressed to the user please make use of the popup\u2019s library components.","text":"<p>This is to ensure consistency in our department branding. Where you create your own try and keep the theme consistent with our branding.</p>"},{"location":"standards/uipath_standards/#use-the-latest-dependency","title":"Use the latest dependency.","text":"<p>Latest dependencies \u201cusually\u201d are faster and more stable.</p>"},{"location":"standards/uipath_standards/#git-push-at-the-end-of-the-day-or-after-a-meaningful-change","title":"Git push at the end of the day or after a meaningful change.","text":"<p>This helps to version control or checkpoint allowing you to return to a previous position. It further allows others to download and distinguish between completed code and partially built code.</p>"},{"location":"standards/uipath_standards/#maintain-consistency-in-your-codingreadability","title":"Maintain consistency in your coding/readability.","text":"<p>If you discover an improved method as you are developing a project please ensure your entire code is in line with it for a consistent approach.</p>"},{"location":"standards/uipath_standards/#asynchronous-approach-parallel-coding-style-check-outcomes-simultaneously-if-possible","title":"Asynchronous approach. Parallel coding style. Check outcomes simultaneously if possible.","text":"<p>Try using pick branches when you review multiple outcomes this is faster than having a chain of if activities. The state machine also has a built-in pick machine in the transition lines. Please note this is not truly asynchronous to achieve asynchronous code you can make use of invoke process. The pick branch executes the triggers in sequence but doesn\u2019t wait for the activities to finish before executing the next trigger (making it seem like parallel or asynchronous).</p>"},{"location":"standards/uipath_standards/#stay-up-to-date-with-uipath-software","title":"Stay up to date with UiPath software.","text":"<p>This can be done by reading the latest release notes and watching the latest UiPath training academy videos. Share findings with the development team through share and learns.</p>"},{"location":"standards/uipath_standards/#avoid","title":"Avoid","text":""},{"location":"standards/uipath_standards/#hardcoding-values-especially-usernamespasswords","title":"Hardcoding values especially usernames/passwords","text":"<p>This is required as you may upload usernames/passwords to a public GitHub repository. It will be easier to change the values of any configurable from outside the code. E.g. Assets</p>"},{"location":"standards/uipath_standards/#image-based-automation","title":"Image based automation","text":"<p>Keep this as a last resort - image based activities are slow and unstable.</p>"},{"location":"standards/uipath_standards/#nesting-activities","title":"Nesting activities","text":"<p>E.g. an if within an if within \u2026 Nesting activities within each other makes the code difficult to read. Try to change your approach to reduce the level of nesting. E.g. use a switch case instead of a nest of ifs.</p>"},{"location":"standards/uipath_standards/#spaghetti-code","title":"Spaghetti code","text":"<p>Spaghetti code refers to a difficult to follow coding style. Your code should be easy to follow in a flowchart. E.g. Having crossed lines in a flow chart will make it difficult to follow. E.g. Splitting a function written in several different invokes.</p>"},{"location":"standards/uipath_standards/#storing-passwordssensitive-information-in-orchestrator","title":"Storing passwords/sensitive information in Orchestrator","text":"<p>Some of our stakeholders do not authorise the use of cloud services. Please check with stakeholders for the preferred method of handling data.</p>"},{"location":"standards/uipath_standards/#leaving-developer-code-in-final-product-main-files","title":"Leaving developer code in final product main files","text":"<p>It is acceptable to leave separate testing xaml\u2019s however avoid leaving commented out code in the main code or temporary write-lines etc\u2026</p>"},{"location":"standards/uipath_standards/#exception-swallowing","title":"Exception swallowing","text":"<p>If you use a try catch ensure exceptions are logged as a minimum because an empty catch makes it difficult to pinpoint the root cause during debugging.</p>"},{"location":"standards/uipath_standards/#complicatedcomplex-coding-techniques","title":"Complicated/complex coding techniques","text":"<p>Avoid using complex coding techniques that could be done with simpler methods. If complex code is required please include an explanation within the annotation of the code. E.g. Using invoke code with vb.net instead of using predefined activities.</p>"},{"location":"standards/uipath_standards/#complicated-variables","title":"Complicated variables","text":"<p>E.g. Use a datatable/json object instead of dictionary within a dictionary variable.</p>"},{"location":"standards/uipath_standards/#lasagne-code","title":"Lasagne code","text":"<p>It is good practice to break up a flow into smaller flows however if this is done frequently the code becomes too layered. Try to keep how deep invokes within invoke layers reach. In addition don\u2019t use one xaml file for everything! Avoid the use of single activity invokes or single activity components.</p>"},{"location":"standards/uipath_standards/#high-number-of-variables","title":"High number of variables","text":"<p>Try to reduce the number of variables used. It is difficult to track information being transferred during debugging if it\u2019s passed from variable to variable. Try to keep arguments and variables that connect to each other in different xamls similarly named. You could make use of global variables if possible. Add into config variable if you need to use variable everywhere.</p>"},{"location":"standards/uipath_standards/#using-delay-activity","title":"Using delay activity","text":"<p>Try to keep the delay activity as a last resort. You should be able to use find element as a wait which will stop waiting as soon as confirmation is received e.g. click followed by a find element. Consider also reducing the delays in the activities to speed up your code.</p>"},{"location":"standards/uipath_standards/#status","title":"Status","text":"<p>This revised standard was formally adopted on 23 July 2024</p>"},{"location":"standards/uipath_standards/#significant-changes","title":"Significant changes","text":"<p>Original version adopted October 2020.\\ Re-write July 2024 based on the revised ways of working implemented from 2021 onwards.</p>"},{"location":"standards/version_control_standards/","title":"Version control standards","text":"<p>These standards define how version control is applied to our code.</p> <p>Wherever possible these standards follow built-in versioning facilities for the languages, tools or frameworks we are using.</p>"},{"location":"standards/version_control_standards/#rationale","title":"Rationale","text":"<ul> <li>Underpins a consistent, robust and repeatable release strategy</li> <li>Ensures that we can have reliable dependency management between components even if they use different technologies</li> </ul>"},{"location":"standards/version_control_standards/#standards","title":"Standards","text":""},{"location":"standards/version_control_standards/#all-code-is-held-in-a-centrally-managed-git-repository","title":"All code is held in a centrally managed Git repository","text":"<p>The primary repository is in the Defra GitHub organisation.</p> <p>Azure DevOps Git or hosted GitLab may be used to provide mirrors of GitHub repositories.</p>"},{"location":"standards/version_control_standards/#every-repository-has-a-protected-master-branch-with-required-status-checks-and-approving-reviews","title":"Every repository has a protected master branch with required status checks and approving reviews","text":"<p>This ensures that no change can be directly applied to production without appropriate test and review.</p>"},{"location":"standards/version_control_standards/#all-releases-are-tagged-in-the-version-control-system-before-deployment","title":"All releases are tagged in the version control system before deployment","text":"<p>This enables easy identification of the specific commits that form a release and a simple way for developers to checkout the code for a specific release for fault analysis purposes.</p>"},{"location":"standards/version_control_standards/#all-releases-have-their-version-number-included-in-the-source-code","title":"All releases have their version number included in the source code","text":"<p>All releases must include a commit that updates the version number (language dependent) in the source code. This ensures that the correct version can be identified in any published code.</p>"},{"location":"standards/version_control_standards/#releases-use-semantic-versioning","title":"Releases use semantic versioning","text":"<p>Apply semantic versioning when choosing a tag</p>"}]}